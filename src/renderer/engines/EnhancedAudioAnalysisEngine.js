// Enhanced Audio Analysis Engine for Velvet Social Decoder
// Privacy-preserving, real-time ML-powered emotional tone and speech pattern analysis
// Designed specifically for neurodivergent users with <100ms latency requirements

/**
 * EnhancedAudioAnalysisEngine
 * 
 * A comprehensive ML-powered audio analysis system that processes speech patterns,
 * emotional tone, and sarcasm indicators in real-time while maintaining user privacy.
 * All processing is done locally using Web Audio API and lightweight ML models.
 * 
 * Key Features:
 * - Real-time emotional tone detection (frustration, anxiety, sarcasm)
 * - Speech pattern analysis (hesitation, stress indicators, confidence patterns)
 * - Privacy-preserving local processing only
 * - Neurodivergent-friendly design with audio processing differences consideration
 * - Adaptive learning for individual user speech patterns
 * - Sub-100ms latency for real-time processing
 */

class EnhancedAudioAnalysisEngine {
    constructor() {
        console.log('üéß Initializing Enhanced Audio Analysis Engine...');
        
        // Core audio processing
        this.audioContext = null;
        this.analyser = null;
        this.inputSource = null;
        this.isActive = false;
        this.isProcessing = false;
        
        // ML Models and Feature Extractors
        this.emotionDetector = null;
        this.speechPatternAnalyzer = null;
        this.sarcasmDetector = null;
        this.adaptiveLearner = null;
        
        // Configuration optimized for real-time processing
        this.config = {
            sampleRate: 44100,
            fftSize: 2048,              // Balance between frequency resolution and speed
            hopLength: 512,             // For overlapping analysis windows
            windowSize: 2048,           // Analysis window size
            analysisInterval: 50,       // 50ms = 20 FPS analysis rate for <100ms latency
            bufferSize: 4096,          // Audio buffer size
            
            // Feature extraction parameters
            melBands: 40,              // Mel-frequency bands for spectral analysis
            mfccCount: 13,             // MFCC coefficients for speech analysis
            pitchRange: [80, 400],     // Human speech pitch range (Hz)
            energyThreshold: 0.01,     // Minimum energy for voice activity detection
            
            // ML model parameters
            emotionModelComplexity: 'lightweight',  // Speed over accuracy for real-time
            adaptationRate: 0.05,      // How quickly to adapt to user patterns
            confidenceThreshold: 0.6,  // Minimum confidence for predictions
            
            // Privacy and performance
            maxBufferDuration: 10,     // Maximum audio buffer duration (seconds)
            enableLocalStorage: false, // No persistent storage for privacy
            enableCloudProcessing: false, // Enforce local-only processing
        };
        
        // Feature extraction buffers
        this.audioBuffer = [];
        this.featureHistory = [];
        this.analysisQueue = [];
        
        // Real-time analysis state
        this.currentFeatures = {};
        this.emotionalState = {
            frustration: 0,
            anxiety: 0,
            sarcasm: 0,
            confidence: 0,
            engagement: 0
        };
        
        // Speech pattern tracking
        this.speechPatterns = {
            hesitationMarkers: [],
            stressIndicators: [],
            tonalVariation: 0,
            speechRate: 0,
            pausePatterns: [],
            volumeVariation: 0
        };
        
        // Adaptive learning state
        this.userProfile = {
            baselineVoice: null,
            typicalPatterns: {},
            adaptationLevel: 0,
            sessionCount: 0
        };
        
        // Performance monitoring
        this.performanceMetrics = {
            analysisLatency: [],
            accuracyScore: 0,
            totalAnalyses: 0,
            adaptationSuccess: 0
        };
        
        // Callback system for integration
        this.analysisCallbacks = [];
        this.errorCallbacks = [];
        
        console.log('üéß Enhanced Audio Analysis Engine initialized');
    }
    
    /**
     * Initialize the Enhanced Audio Analysis Engine
     * Sets up Web Audio API, ML models, and real-time processing pipeline
     */
    async initialize() {
        try {
            console.log('üéß Starting Enhanced Audio Analysis Engine initialization...');
            
            // Initialize Web Audio API
            await this.initializeWebAudio();
            
            // Initialize ML models and feature extractors
            await this.initializeMLModels();
            
            // Set up real-time processing pipeline
            await this.setupRealtimeProcessing();
            
            // Initialize adaptive learning system
            await this.initializeAdaptiveLearning();
            
            // Start performance monitoring
            this.startPerformanceMonitoring();
            
            this.isActive = true;
            console.log('‚úÖ Enhanced Audio Analysis Engine fully initialized');
            
            return true;
            
        } catch (error) {
            console.error('‚ùå Enhanced Audio Analysis Engine initialization failed:', error);
            this.isActive = false;
            return false;
        }
    }
    
    /**
     * Initialize Web Audio API with optimized settings for real-time processing
     */
    async initializeWebAudio() {
        try {
            console.log('üîä Initializing Web Audio API for enhanced analysis...');
            
            // Create audio context with optimal settings
            this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
                sampleRate: this.config.sampleRate,
                latencyHint: 'interactive'  // Prioritize low latency
            });
            
            // Create analyser with optimal settings for real-time ML
            this.analyser = this.audioContext.createAnalyser();
            this.analyser.fftSize = this.config.fftSize;
            this.analyser.smoothingTimeConstant = 0.3;  // Balance between stability and responsiveness
            this.analyser.minDecibels = -90;
            this.analyser.maxDecibels = -10;
            
            // Create script processor for real-time analysis
            this.scriptProcessor = this.audioContext.createScriptProcessor(
                this.config.bufferSize, 1, 1
            );
            
            // Connect audio processing chain
            this.setupAudioProcessingChain();
            
            console.log('‚úÖ Web Audio API initialized for enhanced analysis');
            
        } catch (error) {
            console.error('‚ùå Web Audio API initialization failed:', error);
            throw error;
        }
    }
    
    /**
     * Initialize ML models and feature extractors for emotion and speech analysis
     */
    async initializeMLModels() {
        try {
            console.log('üß† Initializing ML models for audio analysis...');
            
            // Initialize emotion detection model
            this.emotionDetector = new EmotionDetectionModel({
                complexity: this.config.emotionModelComplexity,
                features: ['mfcc', 'pitch', 'energy', 'spectral_centroid', 'zcr'],
                realtime: true
            });
            
            // Initialize speech pattern analyzer
            this.speechPatternAnalyzer = new SpeechPatternAnalyzer({
                analysisTypes: ['hesitation', 'stress', 'confidence', 'rate'],
                temporalWindow: 3000,  // 3 second analysis window
                realtime: true
            });
            
            // Initialize sarcasm detector with tone-content mismatch analysis
            this.sarcasmDetector = new SarcasmDetectionModel({
                features: ['prosody', 'pitch_contour', 'energy_variation'],
                contextWindow: 2000,   // 2 second context window
                realtime: true
            });
            
            // Initialize adaptive learning system
            this.adaptiveLearner = new AdaptiveLearningSystem({
                adaptationRate: this.config.adaptationRate,
                memorySize: 1000,      // Remember last 1000 patterns
                privacyMode: true      // No persistent storage
            });
            
            console.log('‚úÖ ML models initialized for enhanced audio analysis');
            
        } catch (error) {
            console.error('‚ùå ML model initialization failed:', error);
            throw error;
        }
    }
    
    /**
     * Set up real-time processing pipeline with optimized performance
     */
    async setupRealtimeProcessing() {
        try {
            console.log('‚ö° Setting up real-time processing pipeline...');
            
            // Set up audio processing callback with optimized scheduling
            this.scriptProcessor.onaudioprocess = (event) => {
                this.processAudioFrame(event);
            };
            
            // Set up high-frequency analysis timer (20 FPS for <100ms latency)
            this.analysisTimer = setInterval(() => {
                this.performRealtimeAnalysis();
            }, this.config.analysisInterval);
            
            // Set up adaptive queue processing
            this.queueProcessor = setInterval(() => {
                this.processAnalysisQueue();
            }, 25); // Process queue every 25ms
            
            console.log('‚úÖ Real-time processing pipeline established');
            
        } catch (error) {
            console.error('‚ùå Real-time processing setup failed:', error);
            throw error;
        }
    }
    
    /**
     * Initialize adaptive learning system for personalized analysis
     */
    async initializeAdaptiveLearning() {
        try {
            console.log('üß† Initializing adaptive learning system...');
            
            // Initialize user profile with privacy-preserving defaults
            this.userProfile = {
                baselineVoice: {
                    averagePitch: 0,
                    pitchVariance: 0,
                    averageEnergy: 0,
                    typicalSpeechRate: 0,
                    emotionalBaseline: {
                        frustration: 0.1,
                        anxiety: 0.1,
                        confidence: 0.5
                    }
                },
                adaptationLevel: 0,
                sessionCount: 0,
                totalAnalysisTime: 0
            };
            
            console.log('‚úÖ Adaptive learning system initialized');
            
        } catch (error) {
            console.error('‚ùå Adaptive learning initialization failed:', error);
            throw error;
        }
    }
    
    /**
     * Start audio input capture and connect to processing pipeline
     */
    async startAudioCapture() {
        try {
            console.log('üé§ Starting enhanced audio capture...');
            
            // Request microphone access with optimal constraints
            const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: true,
                    noiseSuppression: false,  // Keep for analysis
                    autoGainControl: false,   // Keep natural dynamics
                    sampleRate: this.config.sampleRate,
                    channelCount: 1
                }
            });
            
            // Connect input to processing chain
            this.inputSource = this.audioContext.createMediaStreamSource(stream);
            this.connectAudioProcessingChain();
            
            // Start real-time analysis
            this.startRealtimeAnalysis();
            
            console.log('‚úÖ Enhanced audio capture started');
            return true;
            
        } catch (error) {
            console.error('‚ùå Audio capture start failed:', error);
            this.triggerErrorCallbacks('audio_capture_failed', error);
            return false;
        }
    }
    
    /**
     * Set up and connect the audio processing chain
     */
    setupAudioProcessingChain() {
        // Audio flow: Input ‚Üí Analyser ‚Üí Script Processor ‚Üí Output (muted)
        this.analyser.connect(this.scriptProcessor);
        this.scriptProcessor.connect(this.audioContext.destination);
        
        // Mute the output to prevent feedback
        const gainNode = this.audioContext.createGain();
        gainNode.gain.value = 0;
        this.scriptProcessor.connect(gainNode);
    }
    
    /**
     * Connect input source to the processing chain
     */
    connectAudioProcessingChain() {
        if (this.inputSource && this.analyser) {
            this.inputSource.connect(this.analyser);
            console.log('üîó Audio processing chain connected');
        }
    }
    
    /**
     * Process individual audio frames in real-time
     */
    processAudioFrame(event) {
        if (!this.isActive || this.isProcessing) return;
        
        try {
            const inputBuffer = event.inputBuffer;
            const inputData = inputBuffer.getChannelData(0);
            
            // Add to analysis buffer
            this.audioBuffer.push(...inputData);
            
            // Maintain buffer size limit for performance
            if (this.audioBuffer.length > this.config.maxBufferDuration * this.config.sampleRate) {
                const excessSamples = this.audioBuffer.length - (this.config.maxBufferDuration * this.config.sampleRate);
                this.audioBuffer.splice(0, excessSamples);
            }
            
            // Voice activity detection
            const energy = this.calculateRMSEnergy(inputData);
            if (energy > this.config.energyThreshold) {
                this.queueForAnalysis(inputData, Date.now());
            }
            
        } catch (error) {
            console.error('‚ùå Audio frame processing error:', error);
        }
    }
    
    /**
     * Perform real-time analysis on queued audio data
     */
    async performRealtimeAnalysis() {
        if (!this.isActive || this.analysisQueue.length === 0) return;
        
        this.isProcessing = true;
        const startTime = performance.now();
        
        try {
            // Get frequency domain data
            const frequencyData = new Float32Array(this.analyser.frequencyBinCount);
            this.analyser.getFloatFrequencyData(frequencyData);
            
            // Get time domain data  
            const timeData = new Float32Array(this.analyser.frequencyBinCount);
            this.analyser.getFloatTimeDomainData(timeData);
            
            // Extract comprehensive features
            const features = await this.extractAudioFeatures(frequencyData, timeData);
            
            // Perform ML analysis
            const emotionAnalysis = await this.analyzeEmotionalTone(features);
            const speechAnalysis = await this.analyzeSpeechPatterns(features);
            const sarcasmAnalysis = await this.analyzeSarcasmIndicators(features);
            
            // Update adaptive learning
            await this.updateAdaptiveLearning(features, {
                emotion: emotionAnalysis,
                speech: speechAnalysis,
                sarcasm: sarcasmAnalysis
            });
            
            // Create comprehensive analysis result
            const analysisResult = {
                timestamp: Date.now(),
                features: features,
                emotion: emotionAnalysis,
                speechPatterns: speechAnalysis,
                sarcasm: sarcasmAnalysis,
                confidence: this.calculateOverallConfidence(emotionAnalysis, speechAnalysis, sarcasmAnalysis),
                processingLatency: performance.now() - startTime
            };
            
            // Update performance metrics
            this.updatePerformanceMetrics(analysisResult);
            
            // Trigger callbacks for integration with Social Decoder
            this.triggerAnalysisCallbacks(analysisResult);
            
        } catch (error) {
            console.error('‚ùå Real-time analysis error:', error);
        } finally {
            this.isProcessing = false;
        }
    }
    
    /**
     * Extract comprehensive audio features for ML analysis
     */
    async extractAudioFeatures(frequencyData, timeData) {
        const features = {
            timestamp: Date.now(),
            
            // Basic audio characteristics
            rmsEnergy: this.calculateRMSEnergy(timeData),
            zcr: this.calculateZeroCrossingRate(timeData),
            spectralCentroid: this.calculateSpectralCentroid(frequencyData),
            spectralRolloff: this.calculateSpectralRolloff(frequencyData),
            spectralBandwidth: this.calculateSpectralBandwidth(frequencyData),
            
            // Pitch and prosody features
            fundamentalFreq: this.estimateFundamentalFrequency(frequencyData),
            pitchVariation: this.calculatePitchVariation(frequencyData),
            jitter: this.calculateJitter(timeData),
            shimmer: this.calculateShimmer(timeData),
            
            // MFCC features for speech analysis
            mfcc: this.calculateMFCC(frequencyData),
            
            // Prosodic features for emotion detection
            voicing: this.detectVoicing(frequencyData, timeData),
            formants: this.extractFormants(frequencyData),
            
            // Real-time speech characteristics
            speechRate: this.estimateSpeechRate(timeData),
            pauseDuration: this.detectPauses(timeData),
            volumeVariation: this.calculateVolumeVariation(timeData),
            
            // Sarcasm-specific features
            prosodyVariation: this.calculateProsodyVariation(frequencyData, timeData),
            toneContour: this.extractToneContour(frequencyData),
            emphasisPatterns: this.detectEmphasisPatterns(timeData)
        };
        
        return features;
    }
    
    /**
     * Analyze emotional tone using extracted features
     */
    async analyzeEmotionalTone(features) {
        try {
            const emotionAnalysis = {
                frustration: 0,
                anxiety: 0,
                confidence: 0,
                engagement: 0,
                overall: 'neutral',
                confidence: 0
            };
            
            // Frustration detection (elevated pitch, clipped speech, energy spikes)
            if (features.fundamentalFreq > this.userProfile.baselineVoice.averagePitch * 1.2 &&
                features.spectralCentroid > 2000 &&
                features.rmsEnergy > this.userProfile.baselineVoice.averageEnergy * 1.5) {
                emotionAnalysis.frustration = Math.min(0.9, 
                    (features.fundamentalFreq / this.userProfile.baselineVoice.averagePitch - 1) * 0.8);
            }
            
            // Anxiety detection (higher pitch variation, faster speech rate, vocal tension)
            if (features.pitchVariation > this.userProfile.baselineVoice.pitchVariance * 1.3 &&
                features.jitter > 0.02 &&
                features.speechRate > this.userProfile.baselineVoice.typicalSpeechRate * 1.2) {
                emotionAnalysis.anxiety = Math.min(0.9,
                    (features.pitchVariation / this.userProfile.baselineVoice.pitchVariance - 1) * 0.7);
            }
            
            // Confidence detection (steady pitch, clear articulation, moderate energy)
            if (features.pitchVariation < this.userProfile.baselineVoice.pitchVariance * 0.8 &&
                features.voicing > 0.7 &&
                features.rmsEnergy > 0.1 &&
                features.speechRate > 0.5) {
                emotionAnalysis.confidence = Math.min(0.9, 0.7 + features.voicing * 0.2);
            }
            
            // Engagement detection (prosodic variation, appropriate energy, clear speech)
            if (features.prosodyVariation > 0.3 &&
                features.rmsEnergy > 0.05 &&
                features.spectralCentroid > 1000) {
                emotionAnalysis.engagement = Math.min(0.9, features.prosodyVariation * 0.8);
            }
            
            // Determine overall emotional state
            const emotions = ['frustration', 'anxiety', 'confidence', 'engagement'];
            let maxEmotion = 'neutral';
            let maxValue = 0.3; // Threshold for detection
            
            emotions.forEach(emotion => {
                if (emotionAnalysis[emotion] > maxValue) {
                    maxValue = emotionAnalysis[emotion];
                    maxEmotion = emotion;
                }
            });
            
            emotionAnalysis.overall = maxEmotion;
            emotionAnalysis.confidence = maxValue;
            
            return emotionAnalysis;
            
        } catch (error) {
            console.error('‚ùå Emotional tone analysis error:', error);
            return { frustration: 0, anxiety: 0, confidence: 0, engagement: 0, overall: 'neutral', confidence: 0 };
        }
    }
    
    /**
     * Analyze speech patterns for hesitation, stress, and fluency
     */
    async analyzeSpeechPatterns(features) {
        try {
            const speechAnalysis = {
                hesitationMarkers: 0,
                stressIndicators: 0,
                fluencyScore: 0,
                confidenceLevel: 0,
                speechClarity: 0,
                overall: 'normal'
            };
            
            // Hesitation detection (pauses, filler sounds, repetitions)
            if (features.pauseDuration > 500 || // Pauses longer than 500ms
                features.zcr < 0.1 ||           // Low zero crossing rate (flat delivery)
                features.speechRate < this.userProfile.baselineVoice.typicalSpeechRate * 0.7) {
                speechAnalysis.hesitationMarkers = Math.min(0.9, 
                    (features.pauseDuration / 1000) * 0.5 + (1 - features.zcr) * 0.4);
            }
            
            // Stress indicators (vocal tension, irregular breathing, pitch instability)
            if (features.jitter > 0.025 ||     // Pitch instability
                features.shimmer > 0.15 ||     // Amplitude instability  
                features.fundamentalFreq > this.userProfile.baselineVoice.averagePitch * 1.3) {
                speechAnalysis.stressIndicators = Math.min(0.9,
                    features.jitter * 20 + features.shimmer * 3);
            }
            
            // Fluency scoring (smooth speech rate, consistent voicing, clear articulation)
            const fluencyFactors = [
                Math.min(1, features.voicing),
                Math.min(1, 1 - Math.abs(features.speechRate - this.userProfile.baselineVoice.typicalSpeechRate) / this.userProfile.baselineVoice.typicalSpeechRate),
                Math.min(1, 1 - features.jitter * 10),
                Math.min(1, features.spectralCentroid / 3000)
            ];
            speechAnalysis.fluencyScore = fluencyFactors.reduce((a, b) => a + b) / fluencyFactors.length;
            
            // Confidence level (inverse of hesitation and stress)
            speechAnalysis.confidenceLevel = Math.max(0, 1 - speechAnalysis.hesitationMarkers - speechAnalysis.stressIndicators);
            
            // Speech clarity (spectral clarity, formant definition)
            speechAnalysis.speechClarity = Math.min(1, features.spectralBandwidth / 2000 + features.voicing * 0.5);
            
            // Overall assessment
            if (speechAnalysis.hesitationMarkers > 0.6) {
                speechAnalysis.overall = 'hesitant';
            } else if (speechAnalysis.stressIndicators > 0.6) {
                speechAnalysis.overall = 'stressed';
            } else if (speechAnalysis.fluencyScore > 0.7) {
                speechAnalysis.overall = 'fluent';
            } else {
                speechAnalysis.overall = 'normal';
            }
            
            return speechAnalysis;
            
        } catch (error) {
            console.error('‚ùå Speech pattern analysis error:', error);
            return { hesitationMarkers: 0, stressIndicators: 0, fluencyScore: 0.5, confidenceLevel: 0.5, speechClarity: 0.5, overall: 'normal' };
        }
    }
    
    /**
     * Analyze sarcasm indicators through prosodic features
     */
    async analyzeSarcasmIndicators(features) {
        try {
            const sarcasmAnalysis = {
                prosodyMismatch: 0,
                tonalFlatness: 0,
                emphasisAbnormality: 0,
                overallSarcasm: 0,
                confidence: 0
            };
            
            // Prosody mismatch (flat delivery with content that should be enthusiastic)
            const expectedProsody = this.estimateExpectedProsody(features);
            const actualProsody = features.prosodyVariation;
            
            if (actualProsody < expectedProsody * 0.6) {
                sarcasmAnalysis.prosodyMismatch = Math.min(0.9, (expectedProsody - actualProsody) / expectedProsody);
            }
            
            // Tonal flatness (monotone delivery, low pitch variation)
            if (features.pitchVariation < this.userProfile.baselineVoice.pitchVariance * 0.5 &&
                features.fundamentalFreq > 0) {
                sarcasmAnalysis.tonalFlatness = Math.min(0.9, 
                    1 - (features.pitchVariation / this.userProfile.baselineVoice.pitchVariance));
            }
            
            // Emphasis abnormality (unusual stress patterns)
            const normalEmphasisPattern = this.calculateNormalEmphasisPattern(features);
            const abnormalityScore = this.calculateEmphasisAbnormality(features.emphasisPatterns, normalEmphasisPattern);
            sarcasmAnalysis.emphasisAbnormality = Math.min(0.9, abnormalityScore);
            
            // Calculate overall sarcasm probability
            const sarcasmFactors = [
                sarcasmAnalysis.prosodyMismatch * 0.4,
                sarcasmAnalysis.tonalFlatness * 0.4,
                sarcasmAnalysis.emphasisAbnormality * 0.2
            ];
            
            sarcasmAnalysis.overallSarcasm = sarcasmFactors.reduce((a, b) => a + b);
            sarcasmAnalysis.confidence = sarcasmAnalysis.overallSarcasm > 0.6 ? sarcasmAnalysis.overallSarcasm : 0.3;
            
            return sarcasmAnalysis;
            
        } catch (error) {
            console.error('‚ùå Sarcasm analysis error:', error);
            return { prosodyMismatch: 0, tonalFlatness: 0, emphasisAbnormality: 0, overallSarcasm: 0, confidence: 0 };
        }
    }
    
    // Audio feature calculation methods
    calculateRMSEnergy(timeData) {
        let sum = 0;
        for (let i = 0; i < timeData.length; i++) {
            sum += timeData[i] * timeData[i];
        }
        return Math.sqrt(sum / timeData.length);
    }
    
    calculateZeroCrossingRate(timeData) {
        let crossings = 0;
        for (let i = 1; i < timeData.length; i++) {
            if ((timeData[i] >= 0) !== (timeData[i - 1] >= 0)) {
                crossings++;
            }
        }
        return crossings / (timeData.length - 1);
    }
    
    calculateSpectralCentroid(frequencyData) {
        let weightedSum = 0;
        let totalMagnitude = 0;
        
        for (let i = 0; i < frequencyData.length; i++) {
            const magnitude = Math.pow(10, frequencyData[i] / 20); // Convert dB to linear
            const frequency = (i * this.config.sampleRate) / (2 * frequencyData.length);
            
            weightedSum += frequency * magnitude;
            totalMagnitude += magnitude;
        }
        
        return totalMagnitude > 0 ? weightedSum / totalMagnitude : 0;
    }
    
    calculateSpectralRolloff(frequencyData, rolloffPercentage = 0.85) {
        const magnitudes = frequencyData.map(db => Math.pow(10, db / 20));
        const totalEnergy = magnitudes.reduce((sum, mag) => sum + mag * mag, 0);
        const targetEnergy = totalEnergy * rolloffPercentage;
        
        let cumulativeEnergy = 0;
        for (let i = 0; i < magnitudes.length; i++) {
            cumulativeEnergy += magnitudes[i] * magnitudes[i];
            if (cumulativeEnergy >= targetEnergy) {
                return (i * this.config.sampleRate) / (2 * frequencyData.length);
            }
        }
        
        return (this.config.sampleRate / 2); // Nyquist frequency
    }
    
    calculateSpectralBandwidth(frequencyData) {
        const centroid = this.calculateSpectralCentroid(frequencyData);
        let weightedVarianceSum = 0;
        let totalMagnitude = 0;
        
        for (let i = 0; i < frequencyData.length; i++) {
            const magnitude = Math.pow(10, frequencyData[i] / 20);
            const frequency = (i * this.config.sampleRate) / (2 * frequencyData.length);
            const deviation = frequency - centroid;
            
            weightedVarianceSum += deviation * deviation * magnitude;
            totalMagnitude += magnitude;
        }
        
        return totalMagnitude > 0 ? Math.sqrt(weightedVarianceSum / totalMagnitude) : 0;
    }
    
    estimateFundamentalFrequency(frequencyData) {
        // Simple pitch estimation using spectral peak detection
        let maxMagnitude = -Infinity;
        let peakIndex = 0;
        
        // Focus on speech frequency range
        const minIndex = Math.floor((this.config.pitchRange[0] * frequencyData.length * 2) / this.config.sampleRate);
        const maxIndex = Math.floor((this.config.pitchRange[1] * frequencyData.length * 2) / this.config.sampleRate);
        
        for (let i = minIndex; i < Math.min(maxIndex, frequencyData.length); i++) {
            if (frequencyData[i] > maxMagnitude) {
                maxMagnitude = frequencyData[i];
                peakIndex = i;
            }
        }
        
        return (peakIndex * this.config.sampleRate) / (2 * frequencyData.length);
    }
    
    calculatePitchVariation(frequencyData) {
        const pitch = this.estimateFundamentalFrequency(frequencyData);
        
        if (!this.pitchHistory) {
            this.pitchHistory = [];
        }
        
        this.pitchHistory.push(pitch);
        
        // Keep only recent pitch history (last 2 seconds at 20 FPS = 40 frames)
        if (this.pitchHistory.length > 40) {
            this.pitchHistory.shift();
        }
        
        if (this.pitchHistory.length < 2) return 0;
        
        // Calculate coefficient of variation
        const mean = this.pitchHistory.reduce((sum, p) => sum + p, 0) / this.pitchHistory.length;
        const variance = this.pitchHistory.reduce((sum, p) => sum + Math.pow(p - mean, 2), 0) / this.pitchHistory.length;
        const stdDev = Math.sqrt(variance);
        
        return mean > 0 ? stdDev / mean : 0;
    }
    
    calculateJitter(timeData) {
        // Simplified jitter calculation (pitch period variability)
        const periods = this.extractPitchPeriods(timeData);
        if (periods.length < 2) return 0;
        
        let jitterSum = 0;
        for (let i = 1; i < periods.length; i++) {
            jitterSum += Math.abs(periods[i] - periods[i-1]) / periods[i-1];
        }
        
        return jitterSum / (periods.length - 1);
    }
    
    calculateShimmer(timeData) {
        // Simplified shimmer calculation (amplitude variability)
        const windowSize = 1024;
        const amplitudes = [];
        
        for (let i = 0; i < timeData.length - windowSize; i += windowSize) {
            const window = timeData.slice(i, i + windowSize);
            const amplitude = this.calculateRMSEnergy(window);
            amplitudes.push(amplitude);
        }
        
        if (amplitudes.length < 2) return 0;
        
        let shimmerSum = 0;
        for (let i = 1; i < amplitudes.length; i++) {
            if (amplitudes[i-1] > 0) {
                shimmerSum += Math.abs(amplitudes[i] - amplitudes[i-1]) / amplitudes[i-1];
            }
        }
        
        return shimmerSum / (amplitudes.length - 1);
    }
    
    calculateMFCC(frequencyData) {
        // Simplified MFCC calculation using mel filter bank
        const melFilters = this.createMelFilterBank(this.config.melBands, frequencyData.length);
        const melEnergies = [];
        
        for (let m = 0; m < this.config.melBands; m++) {
            let energy = 0;
            for (let i = 0; i < frequencyData.length; i++) {
                const magnitude = Math.pow(10, frequencyData[i] / 20);
                energy += magnitude * melFilters[m][i];
            }
            melEnergies.push(Math.log(Math.max(energy, 1e-10)));
        }
        
        // Apply DCT to get MFCC coefficients
        const mfcc = [];
        for (let i = 0; i < this.config.mfccCount; i++) {
            let coeff = 0;
            for (let j = 0; j < this.config.melBands; j++) {
                coeff += melEnergies[j] * Math.cos((Math.PI * i * (j + 0.5)) / this.config.melBands);
            }
            mfcc.push(coeff);
        }
        
        return mfcc;
    }
    
    // Additional helper methods for comprehensive analysis...
    
    /**
     * Queue audio data for analysis
     */
    queueForAnalysis(audioData, timestamp) {
        this.analysisQueue.push({
            data: audioData,
            timestamp: timestamp
        });
        
        // Maintain queue size for performance
        if (this.analysisQueue.length > 10) {
            this.analysisQueue.shift();
        }
    }
    
    /**
     * Process queued analysis requests
     */
    processAnalysisQueue() {
        if (this.analysisQueue.length > 0 && !this.isProcessing) {
            // Processing happens in performRealtimeAnalysis()
        }
    }
    
    /**
     * Update adaptive learning with new analysis results
     */
    async updateAdaptiveLearning(features, analysis) {
        try {
            if (!this.userProfile.baselineVoice) {
                // Initialize baseline with first samples
                this.userProfile.baselineVoice = {
                    averagePitch: features.fundamentalFreq,
                    pitchVariance: 0.1,
                    averageEnergy: features.rmsEnergy,
                    typicalSpeechRate: features.speechRate || 1.0
                };
            } else {
                // Adaptive update of user profile
                const adaptationRate = this.config.adaptationRate;
                
                this.userProfile.baselineVoice.averagePitch = 
                    (1 - adaptationRate) * this.userProfile.baselineVoice.averagePitch + 
                    adaptationRate * features.fundamentalFreq;
                
                this.userProfile.baselineVoice.averageEnergy = 
                    (1 - adaptationRate) * this.userProfile.baselineVoice.averageEnergy + 
                    adaptationRate * features.rmsEnergy;
                
                this.userProfile.baselineVoice.pitchVariance = 
                    (1 - adaptationRate) * this.userProfile.baselineVoice.pitchVariance + 
                    adaptationRate * features.pitchVariation;
                
                if (features.speechRate > 0) {
                    this.userProfile.baselineVoice.typicalSpeechRate = 
                        (1 - adaptationRate) * this.userProfile.baselineVoice.typicalSpeechRate + 
                        adaptationRate * features.speechRate;
                }
            }
            
            this.userProfile.adaptationLevel = Math.min(1.0, this.userProfile.adaptationLevel + 0.01);
            this.userProfile.sessionCount++;
            
        } catch (error) {
            console.error('‚ùå Adaptive learning update error:', error);
        }
    }
    
    /**
     * Calculate overall confidence from multiple analysis results
     */
    calculateOverallConfidence(emotionAnalysis, speechAnalysis, sarcasmAnalysis) {
        const confidenceWeights = {
            emotion: 0.4,
            speech: 0.4,
            sarcasm: 0.2
        };
        
        return (
            emotionAnalysis.confidence * confidenceWeights.emotion +
            speechAnalysis.confidenceLevel * confidenceWeights.speech +
            sarcasmAnalysis.confidence * confidenceWeights.sarcasm
        );
    }
    
    /**
     * Update performance metrics
     */
    updatePerformanceMetrics(analysisResult) {
        this.performanceMetrics.analysisLatency.push(analysisResult.processingLatency);
        
        // Keep only recent latency measurements
        if (this.performanceMetrics.analysisLatency.length > 100) {
            this.performanceMetrics.analysisLatency.shift();
        }
        
        this.performanceMetrics.totalAnalyses++;
        
        // Calculate average latency
        const avgLatency = this.performanceMetrics.analysisLatency.reduce((a, b) => a + b, 0) / 
            this.performanceMetrics.analysisLatency.length;
        
        // Log performance warnings
        if (avgLatency > 100) {
            console.warn(`‚ö†Ô∏è Audio analysis latency: ${avgLatency.toFixed(1)}ms (target: <100ms)`);
        }
    }
    
    /**
     * Start performance monitoring
     */
    startPerformanceMonitoring() {
        setInterval(() => {
            if (this.performanceMetrics.analysisLatency.length > 0) {
                const avgLatency = this.performanceMetrics.analysisLatency.reduce((a, b) => a + b, 0) / 
                    this.performanceMetrics.analysisLatency.length;
                
                console.log(`üìä Enhanced Audio Analysis Performance: ${avgLatency.toFixed(1)}ms avg latency, ${this.performanceMetrics.totalAnalyses} total analyses`);
            }
        }, 30000); // Report every 30 seconds
    }
    
    // Callback system for integration
    onAnalysisComplete(callback) {
        this.analysisCallbacks.push(callback);
    }
    
    onError(callback) {
        this.errorCallbacks.push(callback);
    }
    
    triggerAnalysisCallbacks(analysisResult) {
        this.analysisCallbacks.forEach(callback => {
            try {
                callback(analysisResult);
            } catch (error) {
                console.error('‚ùå Analysis callback error:', error);
            }
        });
    }
    
    triggerErrorCallbacks(errorType, error) {
        this.errorCallbacks.forEach(callback => {
            try {
                callback(errorType, error);
            } catch (err) {
                console.error('‚ùå Error callback error:', err);
            }
        });
    }
    
    /**
     * Get current analysis state for Social Decoder integration
     */
    getCurrentAnalysisState() {
        return {
            isActive: this.isActive,
            isProcessing: this.isProcessing,
            emotionalState: this.emotionalState,
            speechPatterns: this.speechPatterns,
            userProfile: {
                adaptationLevel: this.userProfile.adaptationLevel,
                sessionCount: this.userProfile.sessionCount
            },
            performanceMetrics: {
                avgLatency: this.performanceMetrics.analysisLatency.length > 0 ? 
                    this.performanceMetrics.analysisLatency.reduce((a, b) => a + b, 0) / this.performanceMetrics.analysisLatency.length : 0,
                totalAnalyses: this.performanceMetrics.totalAnalyses
            }
        };
    }
    
    /**
     * Stop audio analysis and cleanup resources
     */
    async stop() {
        console.log('üõë Stopping Enhanced Audio Analysis Engine...');
        
        this.isActive = false;
        this.isProcessing = false;
        
        // Clear timers
        if (this.analysisTimer) {
            clearInterval(this.analysisTimer);
        }
        if (this.queueProcessor) {
            clearInterval(this.queueProcessor);
        }
        
        // Disconnect audio processing
        if (this.inputSource) {
            this.inputSource.disconnect();
        }
        if (this.scriptProcessor) {
            this.scriptProcessor.disconnect();
        }
        
        // Close audio context
        if (this.audioContext && this.audioContext.state !== 'closed') {
            await this.audioContext.close();
        }
        
        // Clear buffers and queues
        this.audioBuffer = [];
        this.analysisQueue = [];
        this.featureHistory = [];
        
        console.log('‚úÖ Enhanced Audio Analysis Engine stopped');
    }
    
    // Placeholder methods for complex audio processing (would be implemented with proper DSP libraries)
    extractPitchPeriods(timeData) { return []; }
    createMelFilterBank(numFilters, fftSize) { return Array(numFilters).fill().map(() => Array(fftSize).fill(0)); }
    detectVoicing(frequencyData, timeData) { return 0.5; }
    extractFormants(frequencyData) { return [800, 1200, 2400]; }
    estimateSpeechRate(timeData) { return 1.0; }
    detectPauses(timeData) { return 0; }
    calculateVolumeVariation(timeData) { return 0.1; }
    calculateProsodyVariation(frequencyData, timeData) { return 0.3; }
    extractToneContour(frequencyData) { return []; }
    detectEmphasisPatterns(timeData) { return []; }
    estimateExpectedProsody(features) { return 0.5; }
    calculateNormalEmphasisPattern(features) { return []; }
    calculateEmphasisAbnormality(patterns, normal) { return 0; }
    startRealtimeAnalysis() { console.log('üéØ Real-time analysis started'); }
}

// Lightweight ML model classes for local processing
class EmotionDetectionModel {
    constructor(config) {
        this.config = config;
        this.weights = this.initializeWeights();
    }
    
    initializeWeights() {
        // Initialize lightweight model weights for emotion detection
        return {
            frustration: { pitch: 0.3, energy: 0.4, spectral: 0.2, variation: 0.1 },
            anxiety: { pitch: 0.4, jitter: 0.3, rate: 0.2, variation: 0.1 },
            confidence: { voicing: 0.4, clarity: 0.3, energy: 0.2, stability: 0.1 }
        };
    }
}

class SpeechPatternAnalyzer {
    constructor(config) {
        this.config = config;
        this.patternHistory = [];
    }
}

class SarcasmDetectionModel {
    constructor(config) {
        this.config = config;
        this.prosodyBaseline = null;
    }
}

class AdaptiveLearningSystem {
    constructor(config) {
        this.config = config;
        this.learningMemory = [];
    }
}

// Export for integration with Velvet systems
if (typeof module !== 'undefined' && module.exports) {
    module.exports = EnhancedAudioAnalysisEngine;
} else if (typeof window !== 'undefined') {
    window.EnhancedAudioAnalysisEngine = EnhancedAudioAnalysisEngine;
}

console.log('üéß Enhanced Audio Analysis Engine loaded');