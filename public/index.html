<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Velvet</title>
    <style>
        :root {
            --primary-blue: #2563eb;
            --secondary-blue: #1d4ed8;
            --accent-cyan: #06b6d4;
            --text-primary: #ffffff;
            --text-secondary: #94a3b8;
            --radius: 20px;
            --ease: cubic-bezier(0.4, 0, 0.2, 1);
        }

        body {
            margin: 0;
            padding: 0;
            background: transparent; /* Pure transparency for Electron */
            font-family: -apple-system, BlinkMacSystemFont, "SF Pro Display", "SF Pro Text", "Helvetica Neue", Helvetica, Arial, sans-serif;
            overflow: hidden;
            color: var(--text-primary);
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
            font-feature-settings: "kern" 1, "liga" 1, "calt" 1;
            pointer-events: none; /* Smart click-through - only when containers closed */
        }
        
        /* SMART CONTAINER SYSTEM */
        body.interface-open {
            pointer-events: auto; /* Enable interactions when interface is open */
        }
        
        /* Only interactive elements should receive pointer events */
        .velvet-orb,
        .velvet-interface.open,
        .velvet-interface.open * {
            pointer-events: auto;
        }

        /* Glass Orb Button */
        .velvet-orb {
            width: 70px;
            height: 70px;
            position: fixed !important;
            top: auto !important;
            left: auto !important;
            bottom: 25px !important;
            right: 25px !important;
            border: none;
            border-radius: 50%;
            cursor: pointer;
            z-index: 1000;
            transition: all 0.4s var(--ease);
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 32px;
            font-weight: 600;
            font-family: -apple-system, BlinkMacSystemFont, "SF Pro Display", system-ui, sans-serif;
            color: rgba(255, 255, 255, 0.95);
            
            /* Glassmorphism effect - matching control panel opacity */
            background: 
                linear-gradient(135deg, 
                    rgba(15, 23, 42, 0.98) 0%,
                    rgba(30, 41, 59, 0.96) 50%,
                    rgba(15, 23, 42, 0.98) 100%
                );
            backdrop-filter: blur(20px);
            -webkit-backdrop-filter: blur(20px);
            border: 1px solid rgba(59, 130, 246, 0.3);
            
            box-shadow: 
                0 8px 32px rgba(37, 99, 235, 0.3),
                inset 0 1px 0 rgba(255, 255, 255, 0.1),
                inset 0 -1px 0 rgba(0, 0, 0, 0.1);
            
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 28px;
        }

        .velvet-orb::before {
            content: 'V';
            font-family: -apple-system, BlinkMacSystemFont, "SF Pro Display", system-ui, sans-serif;
            font-weight: 600;
            font-size: 32px;
            letter-spacing: -0.5px;
            filter: drop-shadow(0 0 10px rgba(37, 99, 235, 0.5));
            color: rgba(255, 255, 255, 0.95);
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            line-height: 1;
        }

        .velvet-orb:hover {
            transform: scale(1.05) translateY(-2px);
            box-shadow: 
                0 12px 40px rgba(37, 99, 235, 0.4),
                inset 0 1px 0 rgba(255, 255, 255, 0.15),
                inset 0 -1px 0 rgba(0, 0, 0, 0.1);
            border-color: rgba(59, 130, 246, 0.4);
        }

        .velvet-orb:active {
            transform: scale(0.98) translateY(1px);
        }

        .velvet-orb.listening {
            background: 
                linear-gradient(135deg, 
                    rgba(239, 68, 68, 0.95) 0%, 
                    rgba(249, 115, 22, 0.90) 100%
                );
            border-color: rgba(249, 115, 22, 0.4);
            animation: listeningGlow 1.5s infinite;
        }

        .velvet-orb.listening::before {
            content: '‚óè';
            font-size: 24px;
            color: #ff453a;
            filter: drop-shadow(0 0 10px rgba(249, 115, 22, 0.8));
        }

        .velvet-orb.speaking {
            background: 
                linear-gradient(135deg, 
                    rgba(6, 182, 212, 0.95) 0%, 
                    rgba(14, 165, 233, 0.90) 100%
                );
            border-color: rgba(6, 182, 212, 0.4);
            animation: speakingPulse 0.8s ease-in-out infinite alternate;
        }

        .velvet-orb.speaking::before {
            content: '‚óâ';
            font-size: 28px;
            color: #06b6d4;
            filter: drop-shadow(0 0 10px rgba(6, 182, 212, 0.8));
        }

        .velvet-orb.thinking {
            background: 
                linear-gradient(135deg, 
                    rgba(139, 92, 246, 0.95) 0%, 
                    rgba(124, 58, 237, 0.90) 100%
                );
            border-color: rgba(139, 92, 246, 0.4);
            animation: thinkingGlow 2s ease-in-out infinite;
        }

        .velvet-orb.thinking::before {
            content: '‚ãØ';
            font-size: 28px;
            color: #8b5cf6;
            filter: drop-shadow(0 0 10px rgba(139, 92, 246, 0.8));
        }

        /* Glass Chat Interface */
        .velvet-interface {
            position: fixed;
            bottom: 100px;
            right: 20px;
            width: 420px;
            max-height: 480px;
            min-height: 350px;
            border-radius: var(--radius);
            padding: 28px;
            z-index: -1; /* Move behind everything when closed */
            max-width: calc(100vw - 40px);
            
            /* CRITICAL FIX: Start completely hidden - no display, no pointer events */
            display: none; /* Completely remove from layout when closed */
            visibility: hidden;
            opacity: 0;
            transform: translateX(20px) scale(0.95);
            pointer-events: none;
            
            /* No background or borders when closed */
            background: none;
            backdrop-filter: none;
            -webkit-backdrop-filter: none;
            border: none;
            box-shadow: none;
            
            transition: all 0.4s var(--ease);
        }

        .velvet-interface.open {
            /* WHEN OPEN: Show with all effects */
            display: block; /* Show in layout when open */
            z-index: 999; /* Bring to front when open */
            visibility: visible;
            opacity: 1;
            transform: translateX(0) scale(1);
            pointer-events: auto;
            
            /* Add all the beautiful glass effects only when open */
            background: 
                linear-gradient(135deg, 
                    rgba(15, 23, 42, 0.95) 0%,
                    rgba(30, 41, 59, 0.92) 50%,
                    rgba(15, 23, 42, 0.97) 100%
                );
            backdrop-filter: blur(24px);
            -webkit-backdrop-filter: blur(24px);
            border: 1px solid rgba(59, 130, 246, 0.3);
            box-shadow: 
                0 25px 50px rgba(0, 0, 0, 0.4),
                inset 0 1px 0 rgba(255, 255, 255, 0.1),
                inset 0 -1px 0 rgba(0, 0, 0, 0.2);
        }

        /* Glass shine effects - only show when interface is open */
        .velvet-interface::before,
        .velvet-interface::after {
            content: '';
            position: absolute;
            pointer-events: none;
            opacity: 0;
            visibility: hidden;
            transition: all 0.4s var(--ease);
        }

        .velvet-interface::before {
            top: 0;
            right: 0;
            width: 100px;
            height: 100px;
            background: 
                radial-gradient(circle at center, 
                    rgba(59, 130, 246, 0.1) 0%, 
                    transparent 70%
                );
            border-radius: 0 var(--radius) 0 100%;
        }

        .velvet-interface::after {
            bottom: 0;
            left: 0;
            width: 80px;
            height: 80px;
            background: 
                radial-gradient(circle at center, 
                    rgba(6, 182, 212, 0.08) 0%, 
                    transparent 70%
                );
            border-radius: 100% 0 var(--radius) 0;
        }

        /* Show shine effects only when interface is open */
        .velvet-interface.open::before,
        .velvet-interface.open::after {
            opacity: 1;
            visibility: visible;
        }

        /* Header */
        .velvet-header {
            display: flex;
            align-items: center;
            gap: 12px;
            margin-bottom: 20px;
            padding-bottom: 16px;
            border-bottom: 1px solid rgba(59, 130, 246, 0.15);
            position: relative;
            z-index: 1;
            transition: all 0.3s ease;
        }

        /* Header - NO DRAGGING */
        .velvet-header {
            display: flex;
            align-items: center;
            gap: 12px;
            margin-bottom: 20px;
            padding-bottom: 16px;
            border-bottom: 1px solid rgba(59, 130, 246, 0.15);
            position: relative;
            z-index: 1;
            transition: all 0.3s ease;
            -webkit-app-region: no-drag;
        }

        .velvet-avatar {
            width: 44px;
            height: 44px;
            border-radius: 50%;
            background: 
                linear-gradient(135deg, 
                    rgba(37, 99, 235, 0.3) 0%, 
                    rgba(6, 182, 212, 0.2) 100%
                );
            backdrop-filter: blur(10px);
            border: 1px solid rgba(59, 130, 246, 0.3);
            display: flex;
            align-items: center;
            justify-content: center;
            font-family: -apple-system, BlinkMacSystemFont, 'SF Pro Display', system-ui, sans-serif;
            font-weight: 600;
            font-size: 18px;
            color: rgba(255, 255, 255, 0.95);
        }

        .velvet-avatar::before {
            content: 'V';
            font-weight: 600;
        }

        .velvet-title h3 {
            margin: 0;
            font-size: 18px;
            font-weight: 600;
            color: var(--text-primary);
            text-shadow: 0 1px 2px rgba(0, 0, 0, 0.5);
            user-select: none;
        }

        .velvet-title p {
            margin: 0;
            font-size: 13px;
            color: var(--text-secondary);
            user-select: none;
        }

        .velvet-title p::after {
            content: "";
        }

        .voice-output-toggle {
            width: 36px;
            height: 36px;
            border: none;
            border-radius: 50%;
            background: 
                linear-gradient(135deg, 
                    rgba(255, 255, 255, 0.08) 0%, 
                    rgba(255, 255, 255, 0.04) 100%
                );
            backdrop-filter: blur(10px);
            border: 1px solid rgba(59, 130, 246, 0.2);
            color: rgba(255, 255, 255, 0.9);
            cursor: pointer;
            transition: all 0.3s var(--ease);
            display: flex;
            align-items: center;
            justify-content: center;
            margin-left: auto;
            box-shadow: 
                0 4px 16px rgba(37, 99, 235, 0.15),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
        }

        .voice-output-toggle svg {
            width: 18px;
            height: 18px;
            stroke-width: 1.5;
        }

        .voice-output-toggle:hover {
            background: 
                linear-gradient(135deg, 
                    rgba(255, 255, 255, 0.12) 0%, 
                    rgba(255, 255, 255, 0.06) 100%
                );
            border-color: rgba(59, 130, 246, 0.3);
            color: rgba(255, 255, 255, 0.9);
            transform: scale(1.05);
        }

        .voice-output-toggle.active {
            background: 
                linear-gradient(135deg, 
                    rgba(37, 99, 235, 0.3) 0%, 
                    rgba(6, 182, 212, 0.25) 100%
                );
            border-color: rgba(59, 130, 246, 0.4);
            color: rgba(255, 255, 255, 1);
            box-shadow: 
                0 4px 16px rgba(37, 99, 235, 0.2),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
        }

        .voice-output-toggle.muted {
            background: 
                linear-gradient(135deg, 
                    rgba(156, 163, 175, 0.2) 0%, 
                    rgba(107, 114, 128, 0.15) 100%
                );
            border-color: rgba(156, 163, 175, 0.3);
            color: rgba(156, 163, 175, 0.8);
        }

        /* Control Panel Button */
        .control-panel-button {
            width: 36px;
            height: 36px;
            border: none;
            border-radius: 50%;
            background: 
                linear-gradient(135deg, 
                    rgba(255, 255, 255, 0.08) 0%, 
                    rgba(255, 255, 255, 0.04) 100%
                );
            backdrop-filter: blur(10px);
            border: 1px solid rgba(34, 197, 94, 0.2);
            color: rgba(34, 197, 94, 0.9);
            cursor: pointer;
            transition: all 0.3s var(--ease);
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 16px;
            margin-left: 8px;
            box-shadow: 
                0 4px 16px rgba(34, 197, 94, 0.15),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
        }

        .control-panel-button svg {
            width: 18px;
            height: 18px;
            stroke-width: 1.5;
        }

        .control-panel-button:hover {
            background: 
                linear-gradient(135deg, 
                    rgba(34, 197, 94, 0.15) 0%, 
                    rgba(34, 197, 94, 0.08) 100%
                );
            border-color: rgba(34, 197, 94, 0.4);
            color: rgba(34, 197, 94, 1);
            transform: scale(1.05);
            box-shadow: 
                0 6px 20px rgba(34, 197, 94, 0.25),
                inset 0 1px 0 rgba(255, 255, 255, 0.15);
        }

        /* Apple-style tooltips */
        .tooltip {
            position: relative;
        }

        .tooltip::after {
            content: attr(data-tooltip);
            position: absolute;
            bottom: calc(100% + 8px);
            left: 50%;
            transform: translateX(-50%);
            background: rgba(0, 0, 0, 0.85);
            color: white;
            padding: 6px 12px;
            border-radius: 8px;
            font-size: 12px;
            font-weight: 500;
            font-family: -apple-system, BlinkMacSystemFont, "SF Pro Text", sans-serif;
            white-space: nowrap;
            opacity: 0;
            visibility: hidden;
            transition: all 0.2s cubic-bezier(0.25, 0.46, 0.45, 0.94);
            pointer-events: none;
            z-index: 1000;
            backdrop-filter: blur(20px);
            -webkit-backdrop-filter: blur(20px);
            box-shadow: 
                0 4px 16px rgba(0, 0, 0, 0.15),
                0 1px 4px rgba(0, 0, 0, 0.1);
        }

        .tooltip::before {
            content: '';
            position: absolute;
            bottom: calc(100% + 2px);
            left: 50%;
            transform: translateX(-50%);
            width: 0;
            height: 0;
            border-left: 6px solid transparent;
            border-right: 6px solid transparent;
            border-top: 6px solid rgba(0, 0, 0, 0.85);
            opacity: 0;
            visibility: hidden;
            transition: all 0.2s cubic-bezier(0.25, 0.46, 0.45, 0.94);
            z-index: 1000;
        }

        .tooltip:hover::after {
            opacity: 1;
            visibility: visible;
            transform: translateX(-50%) translateY(-2px);
        }
        
        .tooltip:hover::before {
            opacity: 0;
            visibility: hidden;
        }

        /* Apple-style button focus states */
        button:focus-visible {
            outline: 2px solid rgba(0, 125, 255, 0.6);
            outline-offset: 2px;
        }

        /* Messages */
        .chat-messages {
            max-height: 280px;
            overflow-y: auto;
            margin-bottom: 20px;
            padding-right: 8px;
            position: relative;
            z-index: 1;
            -webkit-app-region: no-drag;
        }

        .chat-messages::-webkit-scrollbar {
            width: 4px;
        }

        .chat-messages::-webkit-scrollbar-track {
            background: rgba(255, 255, 255, 0.05);
            border-radius: 2px;
        }

        .chat-messages::-webkit-scrollbar-thumb {
            background: linear-gradient(135deg, var(--primary-blue), var(--accent-cyan));
            border-radius: 2px;
        }

        .message {
            margin-bottom: 16px;
            animation: messageSlide 0.4s var(--ease);
        }

        .message-content {
            max-width: 85%;
            padding: 12px 16px;
            border-radius: 16px;
            font-size: 14px;
            line-height: 1.5;
            position: relative;
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
        }

        .user-message {
            text-align: right;
        }

        .user-message .message-content {
            background: 
                linear-gradient(135deg, 
                    rgba(37, 99, 235, 0.3) 0%, 
                    rgba(29, 78, 216, 0.2) 100%
                );
            border: 1px solid rgba(59, 130, 246, 0.3);
            color: var(--text-primary);
            margin-left: auto;
            border-bottom-right-radius: 4px;
            display: inline-block;
            box-shadow: 
                0 4px 16px rgba(37, 99, 235, 0.2),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
        }

        .velvet-message .message-content {
            background: 
                linear-gradient(135deg, 
                    rgba(255, 255, 255, 0.08) 0%, 
                    rgba(255, 255, 255, 0.04) 100%
                );
            border: 1px solid rgba(255, 255, 255, 0.12);
            color: var(--text-primary);
            border-bottom-left-radius: 4px;
            display: inline-block;
            box-shadow: 
                0 4px 16px rgba(0, 0, 0, 0.1),
                inset 0 1px 0 rgba(255, 255, 255, 0.05);
        }

        .message-time {
            font-size: 11px;
            color: var(--text-secondary);
            margin-top: 6px;
            opacity: 0.7;
        }

        /* Input Area */
        .chat-input-area {
            display: flex;
            gap: 12px;
            align-items: center;
            position: relative;
            z-index: 1;
            -webkit-app-region: no-drag;
        }

        .chat-input {
            flex: 1;
            background: 
                linear-gradient(135deg, 
                    rgba(255, 255, 255, 0.08) 0%, 
                    rgba(255, 255, 255, 0.04) 100%
                );
            backdrop-filter: blur(10px);
            border: 1px solid rgba(59, 130, 246, 0.2);
            border-radius: 12px;
            padding: 12px 16px;
            color: var(--text-primary);
            font-size: 14px;
            outline: none;
            transition: all 0.3s var(--ease);
        }

        .chat-input::placeholder {
            color: var(--text-secondary);
        }

        .chat-input:focus {
            background: 
                linear-gradient(135deg, 
                    rgba(255, 255, 255, 0.12) 0%, 
                    rgba(255, 255, 255, 0.06) 100%
                );
            border-color: rgba(59, 130, 246, 0.4);
            box-shadow: 
                0 0 0 3px rgba(37, 99, 235, 0.1),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
        }

        .voice-button {
            width: 44px;
            height: 44px;
            border: none;
            border-radius: 50%;
            background: 
                linear-gradient(135deg, 
                    rgba(37, 99, 235, 0.3) 0%, 
                    rgba(6, 182, 212, 0.25) 100%
                );
            backdrop-filter: blur(10px);
            border: 1px solid rgba(59, 130, 246, 0.3);
            color: rgba(255, 255, 255, 0.9);
            cursor: pointer;
            transition: all 0.3s var(--ease);
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
            box-shadow: 
                0 4px 16px rgba(37, 99, 235, 0.2),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
        }

        .voice-button svg {
            width: 18px;
            height: 18px;
            stroke-width: 1.5;
            transition: all 0.3s ease;
        }

        .voice-button:hover {
            transform: scale(1.05);
            box-shadow: 
                0 6px 20px rgba(37, 99, 235, 0.3),
                inset 0 1px 0 rgba(255, 255, 255, 0.15);
        }

        .voice-button.listening {
            background: 
                linear-gradient(135deg, 
                    rgba(239, 68, 68, 0.4) 0%, 
                    rgba(249, 115, 22, 0.3) 100%
                );
            border-color: rgba(249, 115, 22, 0.5);
            color: #ff453a;
        }

        .voice-button.listening svg {
            animation: micPulse 1.2s infinite;
        }

        /* Welcome Message */
        .welcome-message {
            text-align: center;
            padding: 40px 20px;
            color: var(--text-secondary);
        }

        .welcome-message h4 {
            margin: 0 0 12px 0;
            color: var(--text-primary);
            font-size: 18px;
            font-weight: 600;
            text-shadow: 0 1px 2px rgba(0, 0, 0, 0.5);
        }

        .welcome-message p {
            margin: 0;
            font-size: 14px;
            line-height: 1.5;
            opacity: 0.9;
        }

        /* Animations */
        @keyframes listeningGlow {
            0%, 100% { 
                box-shadow: 
                    0 8px 32px rgba(249, 115, 22, 0.3),
                    inset 0 1px 0 rgba(255, 255, 255, 0.1);
            }
            50% { 
                box-shadow: 
                    0 8px 32px rgba(249, 115, 22, 0.6),
                    0 0 40px rgba(249, 115, 22, 0.4),
                    inset 0 1px 0 rgba(255, 255, 255, 0.15);
            }
        }

        @keyframes speakingPulse {
            0% { transform: scale(1); }
            100% { transform: scale(1.02); }
        }

        @keyframes thinkingGlow {
            0%, 100% { 
                box-shadow: 
                    0 8px 32px rgba(139, 92, 246, 0.3),
                    inset 0 1px 0 rgba(255, 255, 255, 0.1);
            }
            50% { 
                box-shadow: 
                    0 8px 32px rgba(139, 92, 246, 0.6),
                    0 0 40px rgba(139, 92, 246, 0.3),
                    inset 0 1px 0 rgba(255, 255, 255, 0.15);
            }
        }

        @keyframes micPulse {
            0%, 100% { 
                transform: scale(1);
                opacity: 1;
            }
            50% { 
                transform: scale(1.1);
                opacity: 0.8;
            }
        }

        @keyframes messageSlide {
            from {
                opacity: 0;
                transform: translateY(15px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        /* Gentle Intervention Nudges */
        .breathing-guide {
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 100px;
            height: 100px;
            border-radius: 50%;
            background: radial-gradient(circle, rgba(59, 130, 246, 0.1) 0%, transparent 70%);
            border: 2px solid rgba(59, 130, 246, 0.3);
            opacity: 0;
            pointer-events: none;
            z-index: 10000;
            transition: all 0.5s ease;
        }


        .breathing-guide.active {
            opacity: 1;
            animation: breathingPulse 4s infinite ease-in-out;
        }

        @keyframes breathingPulse {
            0%, 100% { 
                transform: translate(-50%, -50%) scale(1);
                border-color: rgba(59, 130, 246, 0.3);
            }
            50% { 
                transform: translate(-50%, -50%) scale(1.5);
                border-color: rgba(59, 130, 246, 0.6);
            }
        }

        /* Corner Widget */
        .corner-widget {
            position: fixed;
            bottom: 120px;
            right: 20px;
            max-width: 280px;
            padding: 12px 16px;
            background: 
                linear-gradient(135deg, 
                    rgba(15, 23, 42, 0.95) 0%,
                    rgba(30, 41, 59, 0.92) 100%
                );
            backdrop-filter: blur(16px);
            border: 1px solid rgba(59, 130, 246, 0.3);
            border-radius: 12px;
            color: var(--text-primary);
            font-size: 13px;
            line-height: 1.4;
            opacity: 0;
            transform: translateX(20px) scale(0.95);
            pointer-events: none;
            z-index: 998;
            transition: all 0.4s var(--ease);
            box-shadow: 
                0 10px 25px rgba(0, 0, 0, 0.3),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
        }

        /* Task Checklist Widget */
        .task-checklist {
            position: fixed;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%) scale(0.9);
            width: 380px;
            max-height: 600px;
            background: 
                linear-gradient(135deg, 
                    rgba(15, 23, 42, 0.98) 0%,
                    rgba(30, 41, 59, 0.96) 50%,
                    rgba(15, 23, 42, 0.98) 100%
                );
            backdrop-filter: blur(24px);
            -webkit-backdrop-filter: blur(24px);
            border: 2px solid rgba(59, 130, 246, 0.4);
            border-radius: 20px;
            padding: 24px;
            color: var(--text-primary);
            opacity: 0;
            pointer-events: none;
            z-index: 999;
            transition: all 0.8s cubic-bezier(0.34, 1.56, 0.64, 1);
            box-shadow: 
                0 25px 50px rgba(0, 0, 0, 0.5),
                0 10px 30px rgba(37, 99, 235, 0.2),
                inset 0 1px 0 rgba(255, 255, 255, 0.15),
                inset 0 -1px 0 rgba(0, 0, 0, 0.2);
            cursor: move;
        }

        .task-checklist.show {
            opacity: 1;
            transform: translate(-50%, -50%) scale(1);
            pointer-events: auto;
        }

        .task-checklist.minimized {
            opacity: 1;
            transform: translate(-50%, -50%) scale(0.3);
            pointer-events: auto;
            top: 85%;
            left: 85%;
        }

        .task-checklist.minimized .task-list,
        .task-checklist.minimized .task-goal,
        .task-checklist.minimized .task-progress {
            display: none;
        }

        .task-checklist.minimized .task-checklist-header {
            margin: -24px -24px -24px -24px;
            border-radius: 20px;
            border-bottom: none;
        }

        .task-checklist-header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            margin: -24px -24px 20px -24px;
            padding: 16px 24px;
            background: 
                linear-gradient(135deg, 
                    rgba(59, 130, 246, 0.15) 0%,
                    rgba(37, 99, 235, 0.1) 100%
                );
            border-radius: 20px 20px 0 0;
            border-bottom: 1px solid rgba(59, 130, 246, 0.3);
            cursor: move;
            user-select: none;
        }

        .task-checklist-title {
            font-size: 16px;
            font-weight: 600;
            color: var(--text-primary);
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .task-checklist-controls {
            display: flex;
            gap: 8px;
            align-items: center;
        }

        .task-checklist-minimize,
        .task-checklist-close {
            width: 28px;
            height: 28px;
            border: none;
            border-radius: 50%;
            color: rgba(255, 255, 255, 0.8);
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: center;
            transition: all 0.3s ease;
            font-size: 16px;
            font-weight: bold;
        }

        .task-checklist-minimize {
            background: 
                linear-gradient(135deg, 
                    rgba(249, 115, 22, 0.8) 0%, 
                    rgba(234, 88, 12, 0.9) 100%
                );
        }

        .task-checklist-minimize:hover {
            background: 
                linear-gradient(135deg, 
                    rgba(249, 115, 22, 1) 0%, 
                    rgba(234, 88, 12, 1) 100%
                );
            transform: scale(1.1);
        }

        .task-checklist-close {
            background: 
                linear-gradient(135deg, 
                    rgba(239, 68, 68, 0.8) 0%, 
                    rgba(220, 38, 38, 0.9) 100%
                );
        }

        .task-checklist-close:hover {
            background: 
                linear-gradient(135deg, 
                    rgba(239, 68, 68, 1) 0%, 
                    rgba(220, 38, 38, 1) 100%
                );
            transform: scale(1.1);
        }

        .task-goal {
            font-size: 14px;
            color: var(--text-secondary);
            margin-bottom: 16px;
            font-style: italic;
        }

        .task-list {
            list-style: none;
            padding: 0;
            margin: 0;
        }

        .task-item {
            display: flex;
            align-items: flex-start;
            gap: 12px;
            padding: 8px 0;
            border-bottom: 1px solid rgba(255, 255, 255, 0.05);
            transition: all 0.3s ease;
        }

        .task-item:last-child {
            border-bottom: none;
        }

        .task-checkbox {
            width: 20px;
            height: 20px;
            border: 2px solid rgba(59, 130, 246, 0.4);
            border-radius: 6px;
            background: transparent;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-top: 2px;
            transition: all 0.3s ease;
        }

        .task-checkbox.completed {
            background: linear-gradient(135deg, rgba(34, 197, 94, 0.8), rgba(22, 163, 74, 0.9));
            border-color: rgba(34, 197, 94, 0.6);
            animation: checkboxComplete 0.5s ease;
        }

        .task-checkbox.completed::after {
            content: '‚úì';
            color: white;
            font-size: 12px;
            font-weight: bold;
        }

        .task-content {
            flex: 1;
        }

        .task-text {
            font-size: 14px;
            line-height: 1.4;
            color: var(--text-primary);
            margin-bottom: 4px;
        }

        .task-item.completed .task-text {
            color: var(--text-secondary);
            text-decoration: line-through;
        }

        .task-time {
            font-size: 12px;
            color: var(--text-secondary);
            opacity: 0.8;
        }

        .task-progress {
            margin-top: 16px;
            padding-top: 16px;
            border-top: 1px solid rgba(59, 130, 246, 0.2);
        }

        .task-progress-bar {
            width: 100%;
            height: 6px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 3px;
            overflow: hidden;
            margin-bottom: 8px;
        }

        .task-progress-fill {
            height: 100%;
            background: linear-gradient(90deg, rgba(34, 197, 94, 0.8), rgba(6, 182, 212, 0.8));
            border-radius: 3px;
            transition: width 0.5s ease;
            width: 0%;
        }

        .task-progress-text {
            font-size: 12px;
            color: var(--text-secondary);
            text-align: center;
        }

        @keyframes checkboxComplete {
            0% { transform: scale(1); }
            50% { transform: scale(1.3); }
            100% { transform: scale(1); }
        }

        @keyframes taskComplete {
            0% { transform: scale(1); }
            50% { transform: scale(1.02); box-shadow: 0 0 20px rgba(34, 197, 94, 0.6); }
            100% { transform: scale(1); }
        }

        .task-checklist.task-completed {
            animation: taskComplete 0.6s ease-out;
        }

        .corner-widget.show {
            opacity: 1;
            transform: translateX(0) scale(1);
            pointer-events: auto;
        }

        .corner-widget.gentle {
            border-color: rgba(6, 182, 212, 0.4);
            background: 
                linear-gradient(135deg, 
                    rgba(6, 182, 212, 0.1) 0%,
                    rgba(15, 23, 42, 0.95) 100%
                );
        }

        .corner-widget.encouraging {
            border-color: rgba(139, 92, 246, 0.4);
            background: 
                linear-gradient(135deg, 
                    rgba(139, 92, 246, 0.1) 0%,
                    rgba(15, 23, 42, 0.95) 100%
                );
        }

        .corner-widget.focus {
            border-color: rgba(34, 197, 94, 0.4);
            background: 
                linear-gradient(135deg, 
                    rgba(34, 197, 94, 0.1) 0%,
                    rgba(15, 23, 42, 0.95) 100%
                );
        }

        .corner-widget.audio-aware-nudge {
            border-color: rgba(147, 51, 234, 0.5);
            background: 
                linear-gradient(135deg, 
                    rgba(147, 51, 234, 0.15) 0%,
                    rgba(15, 23, 42, 0.95) 100%
                );
            box-shadow: 
                0 20px 25px rgba(0, 0, 0, 0.6),
                0 8px 16px rgba(147, 51, 234, 0.3),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
            animation: audioAwarePulse 2s ease-in-out infinite;
        }

        @keyframes audioAwarePulse {
            0%, 100% { 
                box-shadow: 
                    0 20px 25px rgba(0, 0, 0, 0.6),
                    0 8px 16px rgba(147, 51, 234, 0.3),
                    inset 0 1px 0 rgba(255, 255, 255, 0.1);
            }
            50% { 
                box-shadow: 
                    0 25px 35px rgba(0, 0, 0, 0.7),
                    0 12px 24px rgba(147, 51, 234, 0.5),
                    inset 0 1px 0 rgba(255, 255, 255, 0.15);
            }
        }

        .corner-widget.audio-intelligence {
            border-color: rgba(6, 182, 212, 0.5);
            background: 
                linear-gradient(135deg, 
                    rgba(6, 182, 212, 0.15) 0%,
                    rgba(15, 23, 42, 0.95) 100%
                );
            box-shadow: 
                0 20px 25px rgba(0, 0, 0, 0.6),
                0 8px 16px rgba(6, 182, 212, 0.4),
                inset 0 1px 0 rgba(255, 255, 255, 0.1);
            animation: audioIntelligencePulse 3s ease-in-out infinite;
        }

        @keyframes audioIntelligencePulse {
            0%, 100% { 
                transform: scale(1);
                opacity: 0.95;
            }
            50% { 
                transform: scale(1.02);
                opacity: 1;
            }
        }

        /* Color shifting for orb during interventions */
        .velvet-orb.intervention-gentle {
            background: 
                linear-gradient(135deg, 
                    rgba(6, 182, 212, 0.95) 0%, 
                    rgba(14, 165, 233, 0.90) 100%
                );
            border-color: rgba(6, 182, 212, 0.4);
            animation: gentleColorShift 3s ease-in-out infinite;
        }

        .velvet-orb.intervention-encouraging {
            background: 
                linear-gradient(135deg, 
                    rgba(139, 92, 246, 0.95) 0%, 
                    rgba(124, 58, 237, 0.90) 100%
                );
            border-color: rgba(139, 92, 246, 0.4);
            animation: encouragingColorShift 3s ease-in-out infinite;
        }

        .velvet-orb.intervention-focus {
            background: 
                linear-gradient(135deg, 
                    rgba(34, 197, 94, 0.95) 0%, 
                    rgba(22, 163, 74, 0.90) 100%
                );
            border-color: rgba(34, 197, 94, 0.4);
            animation: focusColorShift 3s ease-in-out infinite;
        }

        @keyframes gentleColorShift {
            0%, 100% { 
                box-shadow: 
                    0 8px 32px rgba(6, 182, 212, 0.3),
                    inset 0 1px 0 rgba(255, 255, 255, 0.1);
            }
            50% { 
                box-shadow: 
                    0 8px 32px rgba(6, 182, 212, 0.5),
                    0 0 40px rgba(6, 182, 212, 0.3),
                    inset 0 1px 0 rgba(255, 255, 255, 0.15);
            }
        }

        @keyframes encouragingColorShift {
            0%, 100% { 
                box-shadow: 
                    0 8px 32px rgba(139, 92, 246, 0.3),
                    inset 0 1px 0 rgba(255, 255, 255, 0.1);
            }
            50% { 
                box-shadow: 
                    0 8px 32px rgba(139, 92, 246, 0.5),
                    0 0 40px rgba(139, 92, 246, 0.3),
                    inset 0 1px 0 rgba(255, 255, 255, 0.15);
            }
        }

        @keyframes focusColorShift {
            0%, 100% { 
                box-shadow: 
                    0 8px 32px rgba(34, 197, 94, 0.3),
                    inset 0 1px 0 rgba(255, 255, 255, 0.1);
            }
            50% { 
                box-shadow: 
                    0 8px 32px rgba(34, 197, 94, 0.5),
                    0 0 40px rgba(34, 197, 94, 0.3),
                    inset 0 1px 0 rgba(255, 255, 255, 0.15);
            }
        }
    </style>
</head>
<body>
    <!-- Glass Chat Interface -->
    <div class="velvet-interface" id="velvetInterface">
        <div class="velvet-header">
            <div class="velvet-avatar"></div>
            <div class="velvet-title">
                <h3>Velvet</h3>
                <p>Your AI companion</p>
            </div>
            <div style="display: flex; align-items: center;">
                <button class="voice-output-toggle tooltip" id="voiceOutputToggle" data-tooltip="Toggle voice responses" aria-label="Toggle voice output">
                    <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" d="M19.114 5.636a9 9 0 0 1 0 12.728M16.463 8.288a5.25 5.25 0 0 1 0 7.424M6.75 8.25l4.72-4.72a.75.75 0 0 1 1.28.53v15.88a.75.75 0 0 1-1.28.53l-4.72-4.72H4.51c-.88 0-1.59-.79-1.59-1.59V9.51c0-.97.71-1.76 1.59-1.76h2.24Z" />
                    </svg>
                </button>
                <button class="control-panel-button tooltip" id="controlPanelButton" data-tooltip="Open Velvet Control Panel" aria-label="Open control panel">
                    <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                        <path stroke-linecap="round" stroke-linejoin="round" d="M9.594 3.94c.09-.542.56-.94 1.11-.94h2.593c.55 0 1.02.398 1.11.94l.213 1.281c.063.374.313.686.645.87.074.04.147.083.22.127.324.196.72.257 1.075.124l1.217-.456a1.125 1.125 0 011.37.49l1.296 2.247a1.125 1.125 0 01-.26 1.431l-1.003.827c-.293.24-.438.613-.431.992a6.759 6.759 0 010 .255c-.007.378.138.75.43.99l1.005.828c.424.35.534.954.26 1.43l-1.298 2.247a1.125 1.125 0 01-1.369.491l-1.217-.456c-.355-.133-.75-.072-1.076.124a6.57 6.57 0 01-.22.128c-.331.183-.581.495-.644.869l-.213 1.28c-.09.543-.56.941-1.11.941h-2.594c-.55 0-1.02-.398-1.11-.94l-.213-1.281c-.062-.374-.312-.686-.644-.87a6.52 6.52 0 01-.22-.127c-.325-.196-.72-.257-1.076-.124l-1.217.456a1.125 1.125 0 01-1.369-.49l-1.297-2.247a1.125 1.125 0 01.26-1.431l1.004-.827c.292-.24.437-.613.43-.992a6.932 6.932 0 010-.255c.007-.378-.138-.75-.43-.99l-1.004-.828a1.125 1.125 0 01-.26-1.43l1.297-2.247a1.125 1.125 0 011.37-.491l1.216.456c.356.133.751.072 1.076-.124.072-.044.146-.087.22-.128.332-.183.582-.495.644-.869l.214-1.281z"/>
                        <circle cx="12" cy="12" r="3"/>
                    </svg>
                </button>
            </div>
        </div>
        
        <div class="chat-messages" id="chatMessages">
            <div class="welcome-message">
                <h4>Hey there! üëã</h4>
                <p>I'm Velvet, your neurodivergent-friendly AI companion. Ready to chat!</p>
            </div>
        </div>
        
        <div class="chat-input-area">
            <input type="text" class="chat-input" id="chatInput" placeholder="Type your message...">
            <button class="voice-button tooltip" id="voiceButton" data-tooltip="Press to speak" aria-label="Voice input">
                <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" d="M12 18.75a6 6 0 0 0 6-6v-1.5m-6 7.5a6 6 0 0 1-6-6v-1.5m6 7.5v3.75m-3.75 0h7.5M12 15.75a3 3 0 0 1-3-3V4.5a3 3 0 1 1 6 0v8.25a3 3 0 0 1-3 3Z" />
                </svg>
            </button>
        </div>
    </div>
    
    <!-- Glass Orb Button -->
    <button class="velvet-orb tooltip" id="velvetOrb" data-tooltip="Click to open Velvet">V</button>
    
    <!-- Gentle Intervention Elements -->
    <div class="breathing-guide" id="breathingGuide"></div>
    <div class="corner-widget" id="cornerWidget"></div>
    
    <!-- Task Checklist Widget -->
    <div class="task-checklist" id="taskChecklist">
        <div class="task-checklist-header" id="taskChecklistHeader">
            <div class="task-checklist-title">
                <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                    <path stroke-linecap="round" stroke-linejoin="round" d="M9 5H7a2 2 0 0 0-2 2v12a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V7a2 2 0 0 0-2-2h-2M9 5a2 2 0 0 0 2 2h2a2 2 0 0 0 2-2M9 5a2 2 0 0 1 2-2h2a2 2 0 0 1 2 2m-6 9l2 2 4-4"/>
                </svg>
                <span>Active Task</span>
            </div>
            <div class="task-checklist-controls">
                <button class="task-checklist-minimize" id="taskChecklistMinimize">‚àí</button>
                <button class="task-checklist-close" id="taskChecklistClose">√ó</button>
            </div>
        </div>
        <div class="task-goal" id="taskGoal">Ready to break down your task!</div>
        <ul class="task-list" id="taskList"></ul>
        <div class="task-progress">
            <div class="task-progress-bar">
                <div class="task-progress-fill" id="taskProgressFill"></div>
            </div>
            <div class="task-progress-text" id="taskProgressText">0% complete</div>
        </div>
    </div>

    <script src="../src/renderer/voice-whisper.js"></script>
    <script src="../src/renderer/ai-secure.js"></script>
    
    <!-- Enhanced OCR Processor -->
    <script src="../src/renderer/enhanced-ocr-processor.js"></script>
    
    <!-- Unified Consciousness System -->
    <script src="../src/renderer/consciousness-loader.js"></script>
    
    <script>
        const velvetOrb = document.getElementById('velvetOrb');
        const velvetInterface = document.getElementById('velvetInterface');
        const chatMessages = document.getElementById('chatMessages');
        const chatInput = document.getElementById('chatInput');
        const voiceButton = document.getElementById('voiceButton');
        const voiceOutputToggle = document.getElementById('voiceOutputToggle');
        const breathingGuide = document.getElementById('breathingGuide');
        const cornerWidget = document.getElementById('cornerWidget');
        const taskChecklist = document.getElementById('taskChecklist');
        const taskChecklistClose = document.getElementById('taskChecklistClose');
        const taskChecklistMinimize = document.getElementById('taskChecklistMinimize');
        const taskChecklistHeader = document.getElementById('taskChecklistHeader');
        const taskGoal = document.getElementById('taskGoal');
        const taskList = document.getElementById('taskList');
        const taskProgressFill = document.getElementById('taskProgressFill');
        const taskProgressText = document.getElementById('taskProgressText');
        
        let isInterfaceOpen = false;
        let voiceOutputEnabled = false; // Voice output starts disabled
        let velvetVoice = null; // Will be initialized when needed
        let velvetVoiceInput = null; // Voice input system
        let screenIntelligenceActive = true; // Screen monitoring starts enabled
        
        // Nudge system state
        let activeNudges = {
            visual: null,
            audio: null,
            haptic: null,
            text: null
        };

        // Enhanced intention-based task system with AI
        let currentTask = {
            goal: null,
            steps: [],
            currentStepIndex: 0,
            startTime: null,
            expectedApps: [],
            isActive: false,
            
            // Enhanced context tracking
            contextClues: [],
            completionConfidence: 0,
            behaviorPattern: [],
            distractionCount: 0,
            focusScore: 100,
            adaptiveHints: [],
            lastActivity: null,
            timeSpentPerStep: [],
            expectedDuration: null,
            actualProgress: 0
        };

        let intentMonitoring = {
            isActive: false,
            lastAppSwitchTime: 0,
            deviationCount: 0,
            onTrackTime: 0,
            lastWhisperTime: 0,
            
            // Enhanced monitoring
            behaviorAnalysis: {
                timePerApp: {},
                switchFrequency: 0,
                focusPeriods: [],
                distractionTriggers: [],
                productivityScore: 100,
                workingSessions: []
            },
            adaptiveNudging: {
                nudgeHistory: [],
                effectiveness: {},
                personalizedTiming: 300000, // 5 minutes default
                gentlenessLevel: 0.7, // 0-1 scale
                lastNudgeTime: 0,
                nudgeCount: 0
            },
            contextualIntelligence: {
                currentWorkflow: null,
                predictedNextAction: null,
                confidenceLevel: 0,
                assistanceMode: 'gentle' // gentle, moderate, assertive
            }
        };

        // Focus Analytics System for productivity insights
        const focusAnalytics = {
            isInitialized: false,
            sessionStartTime: null,
            currentSession: null,
            
            // Data collection
            sessions: [],
            dailyStats: {},
            weeklyPatterns: {},
            productivityMetrics: {
                averageFocusTime: 0,
                totalTasksCompleted: 0,
                averageTaskCompletionTime: 0,
                distractionFrequency: 0,
                mostProductiveHours: [],
                taskTypePerformance: {},
                weeklyProgress: 0
            },
            
            // Real-time tracking
            currentFocusSession: {
                startTime: null,
                endTime: null,
                taskType: null,
                appSwitches: 0,
                distractions: [],
                completedSteps: 0,
                totalSteps: 0,
                focusScore: 100,
                productivityEvents: [],
                audioContext: 'quiet'
            },
            
            // Behavioral insights
            behavioralPatterns: {
                optimalWorkDuration: 25, // minutes
                averageBreakTime: 5, // minutes
                peakProductivityTime: null,
                commonDistractors: [],
                focusTriggers: [],
                workingStyleProfile: 'balanced', // focused, balanced, flexible
                preferredTaskTypes: []
            }
        };

        // Checklist state
        let checklistState = {
            isMinimized: false,
            isDragging: false,
            dragOffset: { x: 0, y: 0 }
        };
        
        // SMART CONTAINER SYSTEM: Toggle interface when orb is clicked
        velvetOrb.addEventListener('click', () => {
            console.log('üîÆ Orb clicked!');
            isInterfaceOpen = !isInterfaceOpen;
            if (isInterfaceOpen) {
                velvetInterface.classList.add('open');
                document.body.classList.add('interface-open');  // Enable body interactions
                setTimeout(() => chatInput.focus(), 400);
                console.log('üîì Container open - interactions enabled');
            } else {
                velvetInterface.classList.remove('open');
                document.body.classList.remove('interface-open');  // Disable body interactions
                console.log('üîí Container closed - click-through restored');
            }
        });
        
        // SMART CONTAINER: Close interface when clicking outside - restores click-through
        document.addEventListener('click', (e) => {
            if (!velvetInterface.contains(e.target) && 
                !velvetOrb.contains(e.target) && 
                isInterfaceOpen) {
                velvetInterface.classList.remove('open');
                document.body.classList.remove('interface-open');  // Restore click-through
                isInterfaceOpen = false;
                console.log('üîí Clicked outside - container closed, click-through restored');
            }
        });
        
        // Initialize voice input system
        function initializeVoiceInput() {
            if (!velvetVoiceInput) {
                // Create audio-aware voice wrapper
                velvetVoiceInput = createAudioAwareVoice();
                
                // Override the onTranscript method to handle voice input
                velvetVoiceInput.onTranscript = async (transcript) => {
                    console.log('Voice input received:', transcript);
                    
                    // Add user message from voice
                    addMessage(transcript + ' üé§', 'user');
                    
                    // Update UI state
                    voiceButton.classList.remove('listening');
                    updateVoiceState(false);
                    
                    // Check if this is a task declaration before processing normally
                    if (isTaskDeclaration(transcript)) {
                        console.log('üéØ Voice task declaration detected:', transcript);
                        await processTaskDeclaration(transcript);
                        return;
                    }
                    
                    velvetOrb.classList.add('thinking');
                    
                    try {
                        // Get AI response
                        const response = await getVelvetResponse(transcript);
                        addMessage(response, 'velvet');
                        
                        // Speak the response if voice output is enabled
                        if (voiceOutputEnabled && velvetVoice) {
                            velvetOrb.classList.add('speaking');
                            try {
                                await velvetVoice.speak(response);
                            } catch (error) {
                                console.error('Voice output error:', error);
                            }
                            velvetOrb.classList.remove('speaking');
                        }
                        
                    } catch (error) {
                        addMessage("Sorry, I'm having trouble connecting right now. üîÆ", 'velvet');
                    }
                    
                    // Reset state
                    velvetOrb.classList.remove('thinking');
                };
            }
        }

        // Voice input button functionality
        voiceButton.addEventListener('click', async () => {
            initializeVoiceInput();
            
            if (velvetVoiceInput.isListening) {
                // Stop listening
                velvetVoiceInput.stopListening();
                voiceButton.classList.remove('listening');
                updateVoiceState(false);
                console.log('Stopped listening');
            } else {
                // Start listening
                const success = await velvetVoiceInput.startListening();
                if (success) {
                    voiceButton.classList.add('listening');
                    updateVoiceState(true);
                    console.log('Started listening');
                } else {
                    console.error('Failed to start voice input');
                    addMessage("Could not access microphone. Please check permissions.", 'velvet');
                }
            }
        });

        // Toggle voice output
        voiceOutputToggle.addEventListener('click', () => {
            voiceOutputEnabled = !voiceOutputEnabled;
            
            if (voiceOutputEnabled) {
                voiceOutputToggle.classList.add('active');
                voiceOutputToggle.classList.remove('muted');
                voiceOutputToggle.setAttribute('aria-label', 'Voice output on - click to mute');
                
                // Initialize voice system if needed
                if (!velvetVoice) {
                    velvetVoice = createAudioAwareVoice();
                }
                
                addMessage("Voice mode activated! I'll speak my responses now. üîä", 'velvet');
            } else {
                voiceOutputToggle.classList.remove('active');
                voiceOutputToggle.classList.add('muted');
                voiceOutputToggle.setAttribute('aria-label', 'Voice output off - click to enable');
                
                addMessage("Voice mode deactivated. I'll stay quiet now. ü§´", 'velvet');
            }
        });

        // Control Panel Button
        const controlPanelButton = document.getElementById('controlPanelButton');
        controlPanelButton.addEventListener('click', () => {
            if (window.electronAPI?.controlPanel) {
                window.electronAPI.controlPanel.show();
            }
        });

        // Send message on Enter
        chatInput.addEventListener('keypress', async (e) => {
            if (e.key === 'Enter' && chatInput.value.trim()) {
                const userMessage = chatInput.value.trim();
                addMessage(userMessage, 'user');
                chatInput.value = '';
                
                // Check if this is a task declaration before processing normally
                if (isTaskDeclaration(userMessage)) {
                    console.log('üéØ Task declaration detected:', userMessage);
                    await processTaskDeclaration(userMessage);
                    return;
                }
                
                // Show thinking state
                velvetOrb.classList.add('thinking');
                
                try {
                    // Debug: Check if getVelvetResponse exists
                    console.log('üö® CHAT DEBUG: getVelvetResponse exists:', typeof getVelvetResponse);
                    console.log('üö® CHAT DEBUG: window.getVelvetResponse exists:', typeof window.getVelvetResponse);
                    console.log('üö® CHAT DEBUG: About to call getVelvetResponse with:', userMessage);
                    
                    // Get AI response
                    const response = await getVelvetResponse(userMessage);
                    addMessage(response, 'velvet');
                    
                    // Speak the response if voice output is enabled
                    if (voiceOutputEnabled && velvetVoice) {
                        velvetOrb.classList.add('speaking');
                        try {
                            await velvetVoice.speak(response);
                        } catch (error) {
                            console.error('Voice output error:', error);
                        }
                        velvetOrb.classList.remove('speaking');
                    }
                    
                } catch (error) {
                    addMessage("Sorry, I'm having trouble connecting right now. üîÆ", 'velvet');
                }
                
                // Reset state
                velvetOrb.classList.remove('thinking');
            }
        });
        
        // Add message function
        function addMessage(text, sender) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${sender}-message`;
            
            const time = new Date().toLocaleTimeString([], {hour: '2-digit', minute:'2-digit'});
            
            messageDiv.innerHTML = `
                <div class="message-content">${text}</div>
                <div class="message-time">${time}</div>
            `;
            
            // Remove welcome message
            const welcomeMsg = chatMessages.querySelector('.welcome-message');
            if (welcomeMsg) {
                welcomeMsg.remove();
            }
            
            chatMessages.appendChild(messageDiv);
            chatMessages.scrollTop = chatMessages.scrollHeight;
        }
        
        // Update voice states
        function updateVoiceState(isListening) {
            if (isListening) {
                voiceButton.classList.add('listening');
                velvetOrb.classList.add('listening');
            } else {
                voiceButton.classList.remove('listening');
                velvetOrb.classList.remove('listening');
            }
        }

        // Initialize Screen Intelligence
        function initializeScreenIntelligence() {
            if (!window.electronAPI?.screenIntelligence) {
                console.warn('Screen intelligence not available');
                return;
            }

            // Listen for pattern detection
            window.electronAPI.screenIntelligence.onPatternDetected((pattern) => {
                handlePatternDetection(pattern);
            });

            // Listen for window changes
            window.electronAPI.screenIntelligence.onWindowChanged((window) => {
                console.log(`üì± Window changed: ${window.name}`);
                
                // Update intent monitoring
                intentMonitoring.lastAppSwitchTime = Date.now();
                
                // Check for step completion
                checkStepCompletion(window);
                
                // Check for intent deviation
                handleIntentDeviation(window);
            });

            // Initialize ambient audio awareness
            initializeAmbientAudioAwareness();
            
            // Initialize privacy system for neurodivergent accessibility
            initializePrivacySystem();
            
            console.log('üß† Screen intelligence initialized');
        }

        // Comprehensive Audio Intelligence System
        let audioEnvironment = {
            // Output audio monitoring
            isAudioPlaying: false,
            audioApps: [],
            environment: 'quiet',
            audioContext: 'quiet', // music, entertainment, communication, media
            audioMood: 'neutral', // focused, relaxed, social, engaged
            
            // System audio info
            systemVolume: 50,
            volumeLevel: 'medium',
            isMuted: false,
            
            // Ambient microphone listening
            isAmbientListening: false,
            ambientAudioContext: null,
            microphoneStream: null,
            audioAnalyzer: null,
            
            // Monitoring intervals
            lastCheck: 0,
            checkInterval: null,
            ambientInterval: null,
            
            // Audio intelligence
            audioHistory: [],
            contextualInsights: [],
            
            // Behavioral adjustments
            preferVisualNudges: false,
            reduceVoiceVolume: false
        };

        // Meeting Assistant System (Cluely-inspired)
        let meetingAssistant = {
            isActive: false,
            currentMeeting: null,
            transcript: '',
            keyPoints: [],
            actionItems: [],
            startTime: null,
            participants: [],
            meetingType: null, // zoom, teams, meet, discord
            
            // Real-time processing
            transcriptionBuffer: [],
            lastTranscriptTime: 0,
            keyPointsAI: null,
            
            // Meeting detection
            detectedApps: [],
            lastDetectionTime: 0
        };

        // Neurodivergent Accessibility & Privacy System
        let privacySystem = {
            isScreenSharing: false,
            isPrivacyModeActive: false,
            lastScreenShareCheck: 0,
            screenShareApps: [],
            sharingProcesses: [],
            
            // Privacy monitoring
            privacyCheckInterval: null,
            privacyEnabled: true,
            autoHideEnabled: true,
            
            // Discretion levels
            discretionLevel: 'high', // high, medium, low
            hiddenWindows: []
        };

        async function initializeAmbientAudioAwareness() {
            if (!window.electronAPI?.checkAudioPlaying) {
                console.warn('Ambient audio awareness not available');
                return;
            }

            // Check audio environment every 8 seconds (more frequent for better responsiveness)
            audioEnvironment.checkInterval = setInterval(checkComprehensiveAudio, 8000);
            
            // Initial comprehensive check
            await checkComprehensiveAudio();
            
            // Initialize ambient microphone listening
            await initializeAmbientMicrophoneListening();
            
            console.log('üéµ Comprehensive audio intelligence initialized');
        }

        async function initializeAmbientMicrophoneListening() {
            try {
                if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                    console.warn('Microphone access not available');
                    return;
                }

                // Request microphone access for ambient listening
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true,
                        sampleRate: 44100
                    } 
                });
                
                audioEnvironment.microphoneStream = stream;
                
                // Set up audio analysis
                const audioContext = new (window.AudioContext || window.webkitAudioContext)();
                audioEnvironment.ambientAudioContext = audioContext;
                
                const source = audioContext.createMediaStreamSource(stream);
                const analyzer = audioContext.createAnalyser();
                analyzer.fftSize = 2048;
                source.connect(analyzer);
                
                audioEnvironment.audioAnalyzer = analyzer;
                audioEnvironment.isAmbientListening = true;
                
                // Start ambient analysis
                startAmbientAudioAnalysis();
                
                console.log('üé§ Ambient microphone listening initialized');
                
            } catch (error) {
                console.warn('Could not initialize ambient microphone listening:', error);
                // Continue without microphone - still have output audio monitoring
            }
        }

        function startAmbientAudioAnalysis() {
            if (!audioEnvironment.audioAnalyzer) return;
            
            const analyzer = audioEnvironment.audioAnalyzer;
            const bufferLength = analyzer.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);
            
            function analyzeAmbientAudio() {
                if (!audioEnvironment.isAmbientListening) return;
                
                analyzer.getByteFrequencyData(dataArray);
                
                // Calculate audio levels
                const average = dataArray.reduce((sum, value) => sum + value, 0) / bufferLength;
                const audioLevel = Math.round((average / 255) * 100);
                
                // Detect significant audio changes
                const previousLevel = audioEnvironment.lastAmbientLevel || 0;
                audioEnvironment.lastAmbientLevel = audioLevel;
                
                if (audioLevel > 20 && Math.abs(audioLevel - previousLevel) > 15) {
                    handleAmbientAudioChange(audioLevel, dataArray);
                }
                
                // Continue analysis
                requestAnimationFrame(analyzeAmbientAudio);
            }
            
            analyzeAmbientAudio();
        }

        function handleAmbientAudioChange(audioLevel, frequencyData) {
            const now = Date.now();
            
            // Analyze frequency distribution to understand audio type
            const lowFreq = frequencyData.slice(0, 85).reduce((sum, val) => sum + val, 0) / 85;
            const midFreq = frequencyData.slice(85, 255).reduce((sum, val) => sum + val, 0) / 170;
            const highFreq = frequencyData.slice(255).reduce((sum, val) => sum + val, 0) / (frequencyData.length - 255);
            
            let ambientContext = 'unknown';
            
            // Simple audio classification
            if (midFreq > lowFreq && midFreq > highFreq) {
                ambientContext = 'speech'; // Human voice detected
            } else if (lowFreq > midFreq * 1.5) {
                ambientContext = 'music'; // Music with strong bass
            } else if (highFreq > midFreq * 1.2) {
                ambientContext = 'notification'; // System sounds, alerts
            } else if (audioLevel > 40) {
                ambientContext = 'activity'; // General activity
            }
            
            // Store in history
            audioEnvironment.audioHistory.push({
                timestamp: now,
                level: audioLevel,
                context: ambientContext,
                frequencies: { low: lowFreq, mid: midFreq, high: highFreq }
            });
            
            // Keep history manageable
            if (audioEnvironment.audioHistory.length > 50) {
                audioEnvironment.audioHistory = audioEnvironment.audioHistory.slice(-50);
            }
            
            // Generate contextual insights
            if (ambientContext === 'speech' && audioLevel > 30) {
                generateContextualInsight('conversation_detected', 'I hear conversation - would you like me to be extra quiet?');
            } else if (ambientContext === 'music' && audioLevel > 35) {
                generateContextualInsight('music_detected', 'Great music choice! I\'ll adjust my voice to not interrupt the vibe.');
            }
            
            console.log(`üé§ Ambient audio: ${ambientContext} (level: ${audioLevel})`);
        }

        async function checkComprehensiveAudio() {
            try {
                // Get output audio status
                const audioStatus = await window.electronAPI.checkAudioPlaying();
                const wasAudioPlaying = audioEnvironment.isAudioPlaying;
                
                // Get system volume
                const volumeInfo = await window.electronAPI.getSystemVolume();
                
                // Update audio environment state
                audioEnvironment.isAudioPlaying = audioStatus.isAudioPlaying;
                audioEnvironment.audioApps = audioStatus.audioApps;
                audioEnvironment.environment = audioStatus.environment;
                audioEnvironment.audioContext = audioStatus.audioContext;
                audioEnvironment.audioMood = audioStatus.audioMood;
                audioEnvironment.systemVolume = volumeInfo.volume;
                audioEnvironment.volumeLevel = volumeInfo.level;
                audioEnvironment.isMuted = volumeInfo.isMuted;
                audioEnvironment.lastCheck = Date.now();
                
                // Audio environment changed
                if (wasAudioPlaying !== audioStatus.isAudioPlaying) {
                    handleComprehensiveAudioChange(audioStatus, volumeInfo);
                }
                
                // Generate intelligent insights based on combined data
                generateAudioIntelligenceInsights(audioStatus, volumeInfo);
                
                // Check for meeting detection
                checkMeetingContext(audioStatus);
                
            } catch (error) {
                console.error('Error checking comprehensive audio:', error);
            }
        }

        function checkMeetingContext(audioStatus) {
            const now = Date.now();
            const meetingApps = ['Zoom', 'Microsoft Teams', 'Google Chrome', 'Safari', 'Discord', 'Skype', 'WebEx'];
            
            // Detect meeting applications
            const activeMeetingApps = audioStatus.audioApps.filter(app => 
                meetingApps.some(meetingApp => app.includes(meetingApp))
            );
            
            if (activeMeetingApps.length > 0 && audioStatus.audioContext === 'communication') {
                // Meeting detected
                if (!meetingAssistant.isActive) {
                    console.log('üéØ Meeting detected:', activeMeetingApps.join(', '));
                    startMeetingAssistant(activeMeetingApps);
                }
                
                meetingAssistant.detectedApps = activeMeetingApps;
                meetingAssistant.lastDetectionTime = now;
                
            } else if (meetingAssistant.isActive && (now - meetingAssistant.lastDetectionTime) > 30000) {
                // No meeting apps detected for 30 seconds - end meeting
                console.log('üéØ Meeting ended');
                endMeetingAssistant();
            }
        }

        async function startMeetingAssistant(meetingApps) {
            meetingAssistant.isActive = true;
            meetingAssistant.startTime = Date.now();
            meetingAssistant.detectedApps = meetingApps;
            meetingAssistant.meetingType = detectMeetingType(meetingApps[0]);
            
            // Initialize meeting data
            const meetingData = {
                isActive: true,
                meetingType: meetingAssistant.meetingType,
                startTime: meetingAssistant.startTime,
                transcript: 'Meeting started - listening for audio...',
                keyPoints: [`Meeting detected in ${meetingApps.join(', ')}`],
                actionItems: []
            };
            
            meetingAssistant.currentMeeting = meetingData;
            
            // Show meeting assistant window
            if (window.electronAPI?.meetingAssistant) {
                await window.electronAPI.meetingAssistant.show(meetingData);
            }
            
            // Start real-time transcription if microphone is available
            if (audioEnvironment.isAmbientListening) {
                startMeetingTranscription();
            }
            
            // Notify user
            addMessage(`üéØ Meeting Assistant activated! I'm now taking notes and tracking key points for your ${meetingAssistant.meetingType} meeting.`, 'velvet');
            
            generateContextualInsight('meeting_started', `Meeting assistant is now active - I'll help you stay organized!`);
        }

        function detectMeetingType(appName) {
            if (appName.includes('Zoom')) return 'Zoom';
            if (appName.includes('Teams')) return 'Microsoft Teams';
            if (appName.includes('Chrome') || appName.includes('Safari')) return 'Google Meet';
            if (appName.includes('Discord')) return 'Discord';
            if (appName.includes('Skype')) return 'Skype';
            if (appName.includes('WebEx')) return 'WebEx';
            return 'Unknown';
        }

        function startMeetingTranscription() {
            if (!audioEnvironment.audioAnalyzer) return;
            
            console.log('üé§ Starting meeting transcription...');
            
            // Enhanced audio processing for meetings
            meetingAssistant.transcriptionInterval = setInterval(() => {
                if (meetingAssistant.isActive && audioEnvironment.audioHistory.length > 0) {
                    const recentAudio = audioEnvironment.audioHistory.slice(-1)[0];
                    
                    if (recentAudio.context === 'speech' && recentAudio.level > 25) {
                        // Simulate live transcription (in real app, this would use Whisper API)
                        processLiveSpeech(recentAudio);
                    }
                }
            }, 2000); // Check every 2 seconds
        }

        async function processLiveSpeech(audioData) {
            // In a real implementation, this would:
            // 1. Send audio to Whisper API for transcription
            // 2. Use GPT-4 to extract key points and action items
            // 3. Update the meeting assistant in real-time
            
            const now = Date.now();
            
            // Simulate processing
            if (now - meetingAssistant.lastTranscriptTime > 10000) { // Every 10 seconds
                const simulatedTranscript = generateSimulatedTranscript();
                
                meetingAssistant.transcript += simulatedTranscript;
                meetingAssistant.lastTranscriptTime = now;
                
                // Extract key points using AI (simulated)
                const keyPoint = await extractKeyPoint(simulatedTranscript);
                if (keyPoint) {
                    meetingAssistant.keyPoints.push(keyPoint);
                }
                
                // Update meeting assistant
                updateMeetingAssistant();
            }
        }

        function generateSimulatedTranscript() {
            const transcripts = [
                "Let's discuss the project timeline and deliverables.",
                "I think we should focus on the user experience improvements.",
                "The budget allocation needs to be reviewed for Q4.",
                "Can we schedule a follow-up meeting next week?",
                "The development team needs more resources for this sprint."
            ];
            
            return transcripts[Math.floor(Math.random() * transcripts.length)] + " ";
        }

        async function extractKeyPoint(transcript) {
            // In real implementation, this would use GPT-4 to analyze transcript
            // For demo, we'll simulate key point extraction
            
            if (transcript.includes('timeline') || transcript.includes('deadline')) {
                return "Timeline and deadlines discussed";
            } else if (transcript.includes('budget') || transcript.includes('cost')) {
                return "Budget considerations mentioned";
            } else if (transcript.includes('follow-up') || transcript.includes('next')) {
                meetingAssistant.actionItems.push("Schedule follow-up meeting");
                return "Follow-up actions identified";
            } else if (transcript.includes('resources') || transcript.includes('team')) {
                return "Resource allocation discussed";
            }
            
            return null;
        }

        function updateMeetingAssistant() {
            if (!meetingAssistant.currentMeeting) return;
            
            meetingAssistant.currentMeeting.transcript = meetingAssistant.transcript;
            meetingAssistant.currentMeeting.keyPoints = meetingAssistant.keyPoints;
            meetingAssistant.currentMeeting.actionItems = meetingAssistant.actionItems;
            
            // Update the meeting assistant window
            if (window.electronAPI?.meetingAssistant) {
                window.electronAPI.meetingAssistant.update(meetingAssistant.currentMeeting);
            }
        }

        async function endMeetingAssistant() {
            if (!meetingAssistant.isActive) return;
            
            const duration = Date.now() - meetingAssistant.startTime;
            const durationMinutes = Math.round(duration / 60000);
            
            console.log(`üéØ Meeting ended after ${durationMinutes} minutes`);
            
            // Generate meeting summary
            const summary = await generateMeetingSummary();
            
            // Show completion message
            addMessage(`üéØ Meeting completed! I captured ${meetingAssistant.keyPoints.length} key points and ${meetingAssistant.actionItems.length} action items in your ${durationMinutes}-minute meeting.`, 'velvet');
            
            // Store meeting for later access
            storeMeetingRecord(summary);
            
            // Reset meeting assistant
            meetingAssistant.isActive = false;
            meetingAssistant.currentMeeting = null;
            meetingAssistant.transcript = '';
            meetingAssistant.keyPoints = [];
            meetingAssistant.actionItems = [];
            
            if (meetingAssistant.transcriptionInterval) {
                clearInterval(meetingAssistant.transcriptionInterval);
            }
            
            // Hide meeting assistant window
            if (window.electronAPI?.meetingAssistant) {
                window.electronAPI.meetingAssistant.hide();
            }
        }

        async function generateMeetingSummary() {
            // In real implementation, this would use GPT-4 to create a comprehensive summary
            return {
                date: new Date().toISOString(),
                duration: Date.now() - meetingAssistant.startTime,
                meetingType: meetingAssistant.meetingType,
                keyPoints: [...meetingAssistant.keyPoints],
                actionItems: [...meetingAssistant.actionItems],
                transcript: meetingAssistant.transcript,
                participants: meetingAssistant.participants
            };
        }

        function storeMeetingRecord(summary) {
            // Store in localStorage for now (in real app, would sync to cloud)
            const meetings = JSON.parse(localStorage.getItem('velvet_meetings') || '[]');
            meetings.push(summary);
            
            // Keep only last 10 meetings
            if (meetings.length > 10) {
                meetings.splice(0, meetings.length - 10);
            }
            
            localStorage.setItem('velvet_meetings', JSON.stringify(meetings));
            console.log('üìã Meeting summary stored');
        }

        // Neurodivergent Accessibility & Privacy System
        async function initializePrivacySystem() {
            if (!window.electronAPI?.detectScreenSharing) {
                console.warn('Privacy system not available - screen sharing detection disabled');
                return;
            }

            console.log('üîí Initializing privacy system for neurodivergent accessibility...');
            
            // Check for screen sharing every 3 seconds (frequent for responsiveness)
            privacySystem.privacyCheckInterval = setInterval(checkScreenSharingStatus, 3000);
            
            // Initial check
            await checkScreenSharingStatus();
            
            console.log('üîí Privacy system initialized - Velvet will hide during screen sharing');
        }

        async function checkScreenSharingStatus() {
            try {
                const sharingStatus = await window.electronAPI.detectScreenSharing();
                const wasScreenSharing = privacySystem.isScreenSharing;
                
                privacySystem.isScreenSharing = sharingStatus.isScreenSharing;
                privacySystem.screenShareApps = sharingStatus.sharingApps;
                privacySystem.sharingProcesses = sharingStatus.sharingProcesses;
                privacySystem.lastScreenShareCheck = Date.now();
                
                // Screen sharing status changed
                if (wasScreenSharing !== sharingStatus.isScreenSharing) {
                    await handleScreenSharingChange(sharingStatus);
                }
                
            } catch (error) {
                console.error('Error checking screen sharing status:', error);
            }
        }

        async function handleScreenSharingChange(sharingStatus) {
            if (sharingStatus.isScreenSharing && !privacySystem.isPrivacyModeActive) {
                // Screen sharing started - activate privacy mode
                console.log('üîí Screen sharing detected - activating privacy mode for neurodivergent accessibility');
                console.log('üì± Sharing apps:', sharingStatus.sharingApps.join(', '));
                
                await activatePrivacyMode();
                
                // Show discrete notification (only in main interface, not in shared content)
                if (!sharingStatus.isScreenSharing) { // Double-check to avoid showing during sharing
                    generateContextualInsight('privacy_activated', 'Privacy mode activated - Velvet is now invisible to protect your privacy! üîí');
                }
                
            } else if (!sharingStatus.isScreenSharing && privacySystem.isPrivacyModeActive) {
                // Screen sharing stopped - deactivate privacy mode
                console.log('üîì Screen sharing ended - deactivating privacy mode');
                
                await deactivatePrivacyMode();
                
                generateContextualInsight('privacy_deactivated', 'Privacy mode deactivated - Velvet is back and ready to help! üîì');
            }
        }

        async function activatePrivacyMode() {
            if (privacySystem.isPrivacyModeActive) return;
            
            privacySystem.isPrivacyModeActive = true;
            
            // Stealth overlay is already invisible to screen capture
            // No need for additional privacy mode
            console.log('ü•∑ Stealth mode: Already invisible to screen capture');
            
            // Also add visual indicators for debugging (only visible when not screen sharing) - DISABLED
            // document.body.style.opacity = '0.1';
            
            console.log('üîí Privacy mode ACTIVE - All Velvet interfaces hidden from screen sharing');
        }

        async function deactivatePrivacyMode() {
            if (!privacySystem.isPrivacyModeActive) return;
            
            privacySystem.isPrivacyModeActive = false;
            
            // Restore Electron windows
            if (window.electronAPI?.setPrivacyMode) {
                await window.electronAPI.setPrivacyMode(false);
            }
            
            // Restore visual state
            document.body.style.opacity = '1';
            
            console.log('üîì Privacy mode INACTIVE - Velvet interfaces restored');
        }

        function setDiscretionLevel(level) {
            privacySystem.discretionLevel = level;
            
            switch (level) {
                case 'high':
                    // Maximum privacy - hide immediately when any screen activity detected
                    privacySystem.autoHideEnabled = true;
                    console.log('üîí High discretion mode - maximum privacy protection');
                    break;
                case 'medium':
                    // Moderate privacy - hide only during confirmed screen sharing
                    privacySystem.autoHideEnabled = true;
                    console.log('üîí Medium discretion mode - balanced privacy');
                    break;
                case 'low':
                    // Minimal privacy - user controls visibility
                    privacySystem.autoHideEnabled = false;
                    console.log('üîí Low discretion mode - manual privacy control');
                    break;
            }
            
            localStorage.setItem('velvet_discretion_level', level);
        }

        function getPrivacyStatus() {
            return {
                isScreenSharing: privacySystem.isScreenSharing,
                isPrivacyActive: privacySystem.isPrivacyModeActive,
                screenShareApps: privacySystem.screenShareApps,
                sharingProcesses: privacySystem.sharingProcesses,
                discretionLevel: privacySystem.discretionLevel,
                autoHideEnabled: privacySystem.autoHideEnabled,
                lastCheck: new Date(privacySystem.lastScreenShareCheck).toLocaleTimeString()
            };
        }

        function generateContextualInsight(type, message) {
            const now = Date.now();
            
            // Avoid duplicate insights
            const recentInsight = audioEnvironment.contextualInsights.find(
                insight => insight.type === type && (now - insight.timestamp) < 30000
            );
            
            if (recentInsight) return;
            
            // Add insight
            audioEnvironment.contextualInsights.push({
                type: type,
                message: message,
                timestamp: now
            });
            
            // Keep insights manageable
            if (audioEnvironment.contextualInsights.length > 20) {
                audioEnvironment.contextualInsights = audioEnvironment.contextualInsights.slice(-20);
            }
            
            // Show subtle notification
            showContextualInsight(message);
        }

        function showContextualInsight(message) {
            // Show in corner widget with special audio intelligence styling
            cornerWidget.innerHTML = `üß† ${message}`;
            cornerWidget.className = 'corner-widget show audio-intelligence';
            
            setTimeout(() => {
                if (cornerWidget.classList.contains('audio-intelligence')) {
                    cornerWidget.classList.remove('show', 'audio-intelligence');
                }
            }, 6000);
        }

        function handleComprehensiveAudioChange(audioStatus, volumeInfo) {
            if (audioStatus.isAudioPlaying) {
                console.log(`üéµ Audio detected: ${audioStatus.audioApps.join(', ')} (${audioStatus.audioContext}/${audioStatus.audioMood})`);
                
                // Context-aware responses based on audio type
                let contextMessage = generateContextAwareMessage(audioStatus);
                
                if (voiceOutputEnabled && contextMessage) {
                    addMessage(contextMessage, 'velvet');
                }
                
                // Adjust nudge behavior for audio environment
                adjustNudgeBehaviorForAudio(true, audioStatus);
                
            } else {
                console.log('üîá Audio environment quiet');
                
                // Resume normal voice behavior
                adjustNudgeBehaviorForAudio(false, audioStatus);
            }
        }

        function generateContextAwareMessage(audioStatus) {
            const messages = {
                music: [
                    "I notice you're listening to music - perfect for focus! I'll keep my voice gentle. üéµ",
                    "Great music choice! I'll whisper so I don't interrupt your flow. üé∂",
                    "Music time! I'll stay quiet and let the rhythm guide your work. üé∏"
                ],
                entertainment: [
                    "Looks like you're watching something interesting! I'll keep it down. üì∫",
                    "Entertainment mode detected - I'll be extra quiet so you can enjoy! üé¨",
                    "Video time! I'll whisper if I need to say anything. üçø"
                ],
                communication: [
                    "I see you're in a call - I'll be silent unless you really need me! üí¨",
                    "Communication detected - I'll stay out of the way! üìû",
                    "Looks like you're chatting - I'll keep quiet! üó£Ô∏è"
                ],
                media: [
                    "I notice media playing - I'll adjust my voice accordingly! üîä",
                    "Audio active - I'll be mindful of your audio environment! üìª"
                ]
            };
            
            const contextMessages = messages[audioStatus.audioContext] || messages.media;
            return contextMessages[Math.floor(Math.random() * contextMessages.length)];
        }

        function generateAudioIntelligenceInsights(audioStatus, volumeInfo) {
            const now = Date.now();
            
            // Intelligent insights based on audio patterns
            if (audioStatus.audioContext === 'music' && audioStatus.audioMood === 'focused') {
                generateContextualInsight('focus_music', 'Perfect focus music! This should help with deep work.');
            }
            
            if (volumeInfo.level === 'loud' && audioStatus.isAudioPlaying) {
                generateContextualInsight('loud_audio', 'Volume is quite high - remember to protect your hearing!');
            }
            
            if (audioStatus.audioContext === 'communication' && currentTask.isActive) {
                generateContextualInsight('task_interrupted', 'Task paused for communication - I\'ll be here when you\'re ready to continue!');
            }
            
            // Ambient audio correlation
            if (audioEnvironment.audioHistory.length > 5) {
                const recentSpeech = audioEnvironment.audioHistory.slice(-5).filter(h => h.context === 'speech');
                if (recentSpeech.length >= 3 && !audioStatus.isAudioPlaying) {
                    generateContextualInsight('conversation_active', 'I hear conversation around you - want me to be extra discrete?');
                }
            }
        }

        function adjustNudgeBehaviorForAudio(audioActive, audioStatus = null) {
            if (audioActive && audioStatus) {
                // Context-specific adjustments
                switch (audioStatus.audioContext) {
                    case 'music':
                        audioEnvironment.preferVisualNudges = true;
                        audioEnvironment.reduceVoiceVolume = true;
                        break;
                    case 'entertainment':
                        audioEnvironment.preferVisualNudges = true;
                        audioEnvironment.reduceVoiceVolume = true;
                        break;
                    case 'communication':
                        audioEnvironment.preferVisualNudges = true;
                        audioEnvironment.reduceVoiceVolume = true; // Be very quiet during calls
                        break;
                    default:
                        audioEnvironment.preferVisualNudges = true;
                        audioEnvironment.reduceVoiceVolume = true;
                }
            } else {
                // When quiet, resume normal nudge behavior
                audioEnvironment.preferVisualNudges = false;
                audioEnvironment.reduceVoiceVolume = false;
            }
        }

        function isAudioAwareEnvironment() {
            return audioEnvironment.isAudioPlaying;
        }

        // Create comprehensive audio-aware voice wrapper
        function createAudioAwareVoice() {
            return {
                async speak(text) {
                    if (this.isSpeaking) {
                        console.log('Already speaking, skipping...');
                        return;
                    }

                    // Clean text for speech (remove emojis)
                    let cleanText = text;
                    cleanText = cleanText.replace(/[\u{1F000}-\u{1F9FF}]|[\u{2600}-\u{26FF}]|[\u{2700}-\u{27BF}]/gu, '');
                    cleanText = cleanText.replace(/:[a-zA-Z0-9_]+:/g, '');
                    cleanText = cleanText.replace(/\s+/g, ' ').trim();
                    
                    if (!cleanText) return;
                    
                    try {
                        this.isSpeaking = true;
                        console.log('üé§ Generating speech with ElevenLabs:', cleanText);
                        
                        // Send to main process for ElevenLabs TTS via secure IPC
                        const audioBase64 = await window.electronAPI.elevenLabsTTS(cleanText);
                        
                        if (audioBase64) {
                            console.log('‚úÖ ElevenLabs audio generated successfully');
                            
                            // Convert base64 to audio blob and play
                            const audioBlob = this.base64ToBlob(audioBase64, 'audio/mpeg');
                            const audioUrl = URL.createObjectURL(audioBlob);
                            
                            const audio = new Audio(audioUrl);
                            audio.volume = 0.8;
                            
                            return new Promise((resolve) => {
                                audio.onended = () => {
                                    URL.revokeObjectURL(audioUrl);
                                    this.isSpeaking = false;
                                    resolve();
                                };
                                
                                audio.onerror = () => {
                                    console.error('Audio playback error');
                                    URL.revokeObjectURL(audioUrl);
                                    this.isSpeaking = false;
                                    resolve();
                                };
                                
                                audio.play();
                            });
                        } else {
                            console.warn('No audio data received from ElevenLabs');
                            this.isSpeaking = false;
                        }
                    } catch (error) {
                        console.error('ElevenLabs TTS error:', error);
                        this.isSpeaking = false;
                        
                        // Fallback to browser speech synthesis
                        console.log('üîÑ Using browser speech synthesis as fallback');
                        const utterance = new SpeechSynthesisUtterance(cleanText);
                        utterance.rate = 0.9;
                        utterance.pitch = 1.1;
                        utterance.volume = 0.8;
                        
                        utterance.onend = () => {
                            this.isSpeaking = false;
                        };
                        
                        speechSynthesis.speak(utterance);
                    }
                },
                
                // Helper function to convert base64 to blob
                base64ToBlob(base64, mimeType) {
                    const byteCharacters = atob(base64);
                    const byteNumbers = new Array(byteCharacters.length);
                    for (let i = 0; i < byteCharacters.length; i++) {
                        byteNumbers[i] = byteCharacters.charCodeAt(i);
                    }
                    const byteArray = new Uint8Array(byteNumbers);
                    return new Blob([byteArray], { type: mimeType });
                },
                
                // Voice input properties
                isListening: false,
                mediaRecorder: null,
                audioChunks: [],
                stream: null,

                // Voice input methods
                async startListening() {
                    try {
                        // Get microphone access
                        this.stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                        console.log('Microphone access granted');
                        
                        // Create MediaRecorder
                        this.mediaRecorder = new MediaRecorder(this.stream, {
                            mimeType: 'audio/webm'
                        });
                        
                        this.audioChunks = [];
                        
                        this.mediaRecorder.ondataavailable = (event) => {
                            if (event.data.size > 0) {
                                this.audioChunks.push(event.data);
                            }
                        };
                        
                        this.mediaRecorder.onstop = async () => {
                            console.log('Recording stopped, processing...');
                            const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
                            await this.transcribeAudio(audioBlob);
                        };
                        
                        // Start recording
                        this.mediaRecorder.start();
                        this.isListening = true;
                        console.log('Started recording audio...');
                        
                        return true;
                    } catch (error) {
                        console.error('Error starting recording:', error);
                        return false;
                    }
                },

                stopListening() {
                    if (this.mediaRecorder && this.isListening) {
                        this.isListening = false;
                        this.mediaRecorder.stop();
                        
                        // Stop all tracks
                        if (this.stream) {
                            this.stream.getTracks().forEach(track => track.stop());
                        }
                        
                        console.log('Stopped recording');
                    }
                },

                async transcribeAudio(audioBlob) {
                    try {
                        console.log('Sending audio to Whisper...');
                        
                        // Convert blob to base64
                        const base64Audio = await this.blobToBase64(audioBlob);
                        
                        // Send to main process for Whisper transcription via secure IPC
                        const transcript = await window.electronAPI.transcribeAudio(base64Audio);
                        
                        if (transcript) {
                            console.log('Transcript:', transcript);
                            if (this.onTranscript) {
                                this.onTranscript(transcript);
                            }
                        }
                    } catch (error) {
                        console.error('Transcription error:', error);
                        if (this.onTranscript) {
                            this.onTranscript("I couldn't understand that. Could you try again?");
                        }
                    }
                },

                // Helper to convert blob to base64
                async blobToBase64(blob) {
                    return new Promise((resolve, reject) => {
                        const reader = new FileReader();
                        reader.onload = () => {
                            const base64 = reader.result.split(',')[1];
                            resolve(base64);
                        };
                        reader.onerror = reject;
                        reader.readAsDataURL(blob);
                    });
                },

                // Initialize speaking state
                isSpeaking: false,
                
                // Callback for transcription results
                onTranscript: null
            };
        }

        function calculateContextualVolume() {
            // Base volume calculation
            let volume = 0.8; // Default volume
            
            if (audioEnvironment.isAudioPlaying) {
                // Adjust based on audio context
                switch (audioEnvironment.audioContext) {
                    case 'music':
                        // Music is playing - be quieter but still audible
                        volume = audioEnvironment.audioMood === 'focused' ? 0.3 : 0.4;
                        break;
                    case 'entertainment':
                        // Video/entertainment - be very quiet
                        volume = 0.25;
                        break;
                    case 'communication':
                        // Calls/meetings - be almost silent
                        volume = 0.15;
                        break;
                    default:
                        volume = 0.35;
                }
                
                // Adjust for system volume
                if (audioEnvironment.volumeLevel === 'loud') {
                    volume *= 0.7; // Even quieter when system volume is high
                } else if (audioEnvironment.volumeLevel === 'quiet') {
                    volume *= 1.2; // Slightly louder when system is quiet
                }
            }
            
            // Ambient microphone adjustments
            if (audioEnvironment.audioHistory.length > 3) {
                const recentActivity = audioEnvironment.audioHistory.slice(-3);
                const avgLevel = recentActivity.reduce((sum, h) => sum + h.level, 0) / 3;
                
                if (avgLevel > 40) {
                    volume *= 0.8; // Quieter when ambient environment is loud
                }
            }
            
            return Math.max(0.1, Math.min(1.0, volume)); // Clamp between 0.1 and 1.0
        }

        function shouldPreferElevenLabs() {
            // Use ElevenLabs for better volume control in specific contexts
            return audioEnvironment.isAudioPlaying && 
                   (audioEnvironment.audioContext === 'communication' || 
                    audioEnvironment.volumeLevel === 'loud');
        }

        function getContextualSpeechParameters() {
            let rate = 0.85;
            let pitch = 1.0;
            
            // Adjust speech parameters based on context
            if (audioEnvironment.audioMood === 'focused') {
                rate = 0.75; // Slower for focus sessions
                pitch = 0.95; // Slightly lower pitch
            } else if (audioEnvironment.audioMood === 'relaxed') {
                rate = 0.8; // Relaxed pace
                pitch = 1.05; // Slightly higher pitch for friendliness
            } else if (audioEnvironment.audioMood === 'social') {
                rate = 0.7; // Very slow during social/communication contexts
                pitch = 0.9; // Lower pitch to be less intrusive
            }
            
            return { rate, pitch };
        }

        // Handle detected patterns with 4-type gentle nudges
        function handlePatternDetection(pattern) {
            console.log('üéØ Pattern detected:', pattern.type, pattern);

            // Determine nudge configuration based on pattern type
            const nudgeConfig = getNudgeConfig(pattern);
            
            // Deploy all 4 types of nudges
            deployNudges(nudgeConfig, pattern);
        }

        // Get nudge configuration for each pattern type
        function getNudgeConfig(pattern) {
            const configs = {
                hyperfocus: {
                    visual: { type: 'breathingGuide', style: 'focus', duration: 8000 },
                    audio: { type: 'whisper', message: 'Time for a gentle stretch' },
                    haptic: { type: 'desktop-focus', message: 'üåü Focus zone detected' },
                    text: { type: 'corner', style: 'focus', duration: 6000 }
                },
                distractionSpiral: {
                    visual: { type: 'colorShift', style: 'gentle', duration: 6000 },
                    audio: { type: 'whisper', message: 'Let\'s breathe together' },
                    haptic: { type: 'desktop-gentle', message: 'üåä Mind feels scattered' },
                    text: { type: 'corner', style: 'gentle', duration: 8000 }
                },
                taskAvoidance: {
                    visual: { type: 'colorShift', style: 'encouraging', duration: 5000 },
                    audio: { type: 'whisper', message: 'One tiny step at a time' },
                    haptic: { type: 'desktop-encouraging', message: 'üíú Taking it step by step' },
                    text: { type: 'corner', style: 'encouraging', duration: 7000 }
                },
                idle: {
                    visual: { type: 'breathingGuide', style: 'gentle', duration: 7000 },
                    audio: { type: 'whisper', message: 'Thinking time is productive too' },
                    haptic: { type: 'desktop-calm', message: 'ü§î Productive thinking time' },
                    text: { type: 'corner', style: 'gentle', duration: 5000 }
                }
            };
            
            return configs[pattern.type] || configs.idle;
        }

        // Deploy all 4 types of nudges
        function deployNudges(config, pattern) {
            // Clear any existing nudges
            clearAllNudges();
            
            // 1. VISUAL NUDGES
            deployVisualNudge(config.visual, pattern);
            
            // 2. AUDIO NUDGES  
            deployAudioNudge(config.audio, pattern);
            
            // 3. HAPTIC NUDGES
            deployHapticNudge(config.haptic, pattern);
            
            // 4. TEXT NUDGES
            deployTextNudge(config.text, pattern);
            
            console.log('üéØ 4-type nudge deployed for:', pattern.type);
        }

        // 1. VISUAL NUDGES: Breathing guides and orb color shifts
        function deployVisualNudge(config, pattern) {
            if (config.type === 'breathingGuide') {
                // Show breathing guide overlay
                breathingGuide.classList.add('active');
                activeNudges.visual = setTimeout(() => {
                    breathingGuide.classList.remove('active');
                }, config.duration);
                
            } else if (config.type === 'colorShift') {
                // Orb color shift only
                velvetOrb.classList.add(`intervention-${config.style}`);
                activeNudges.visual = setTimeout(() => {
                    velvetOrb.classList.remove(`intervention-${config.style}`);
                }, config.duration);
            }
        }

        // 2. AUDIO NUDGES: Whispered reminders (ambient audio aware)
        function deployAudioNudge(config, pattern) {
            if (voiceOutputEnabled && velvetVoice) {
                // Check if audio is playing and adjust behavior
                if (audioEnvironment.isAudioPlaying && audioEnvironment.preferVisualNudges) {
                    console.log('üéµ Audio playing - skipping voice nudge, enhancing visual instead');
                    
                    // Enhance visual nudge instead of voice
                    enhanceVisualNudgeForAudio(config, pattern);
                    return;
                }
                
                // Create whispered, gentle version
                const whisperText = config.message;
                
                // Speak with extra gentle settings (would modify TTS settings)
                activeNudges.audio = setTimeout(() => {
                    // If audio started playing while we were waiting, skip voice
                    if (audioEnvironment.isAudioPlaying && audioEnvironment.preferVisualNudges) {
                        console.log('üéµ Audio detected during delay - skipping voice nudge');
                        return;
                    }
                    
                    velvetVoice.speak(whisperText);
                }, 1000); // Slight delay after visual
            }
        }

        // Enhance visual nudges when audio is playing
        function enhanceVisualNudgeForAudio(config, pattern) {
            // Make the corner widget more prominent when audio is playing
            cornerWidget.innerHTML = `üéµ ${config.message}`;
            cornerWidget.className = 'corner-widget show audio-aware-nudge';
            
            // Show longer for audio environments
            setTimeout(() => {
                if (cornerWidget.classList.contains('audio-aware-nudge')) {
                    cornerWidget.classList.remove('show', 'audio-aware-nudge');
                }
            }, 8000); // Longer duration when audio is playing
        }

        // 3. HAPTIC NUDGES: Desktop alternative to phone vibration
        function deployHapticNudge(config, pattern) {
            // For desktop, use visual/audio feedback instead of vibration
            console.log('üì≥ Desktop haptic nudge:', config.type, config.message);
            
            // Try vibration API first (works on mobile)
            if ('vibrate' in navigator && navigator.vibrate) {
                try {
                    navigator.vibrate([200, 100, 200]);
                    console.log('üì≥ Mobile vibration activated');
                } catch (e) {
                    console.log('üì≥ Vibration failed, using desktop alternative');
                }
            }
            
            // Desktop alternative: Brief system notification or orb pulse
            const originalTransform = velvetOrb.style.transform;
            let pulseCount = 0;
            const pulseInterval = setInterval(() => {
                velvetOrb.style.transform = pulseCount % 2 === 0 ? 'scale(1.02)' : 'scale(1)';
                pulseCount++;
                if (pulseCount >= 4) {
                    clearInterval(pulseInterval);
                    velvetOrb.style.transform = originalTransform;
                }
            }, 100);
            
            activeNudges.haptic = config.message;
        }

        // 4. TEXT NUDGES: Corner widgets with encouragement
        function deployTextNudge(config, pattern) {
            // Show corner widget
            cornerWidget.innerHTML = getEncouragingText(pattern);
            cornerWidget.className = `corner-widget show ${config.style}`;
            
            activeNudges.text = setTimeout(() => {
                cornerWidget.classList.remove('show');
            }, config.duration);
        }

        // Get encouraging text for corner widgets
        function getEncouragingText(pattern) {
            const messages = {
                hyperfocus: [
                    "üåü Amazing focus! Your brain is in the zone.",
                    "üßò‚Äç‚ôÄÔ∏è Remember to breathe and stretch occasionally.",
                    "üí™ You're crushing it! Quick body check?"
                ],
                distractionSpiral: [
                    "üåä Mind feels scattered? That's totally normal.",
                    "üß† Lots happening upstairs. Want to pause and breathe?",
                    "üíô Your neurodivergent brain is just processing lots!"
                ],
                taskAvoidance: [
                    "üíú Big tasks feel overwhelming. You're not alone.",
                    "ü™ú What's the tiniest possible first step?",
                    "‚ú® Every small action counts. You've got this!"
                ],
                idle: [
                    "ü§î Thinking time is productive time too.",
                    "‚ú® Sometimes our best ideas come in quiet moments.",
                    "üå± Your mind is processing. That's valuable work."
                ]
            };
            
            const patternMessages = messages[pattern.type] || messages.idle;
            return patternMessages[Math.floor(Math.random() * patternMessages.length)];
        }

        // Clear all active nudges
        function clearAllNudges() {
            // Clear visual nudges
            if (activeNudges.visual) {
                clearTimeout(activeNudges.visual);
                breathingGuide.classList.remove('active');
                velvetOrb.classList.remove('intervention-gentle', 'intervention-encouraging', 'intervention-focus');
            }
            
            // Clear text nudges
            if (activeNudges.text) {
                clearTimeout(activeNudges.text);
                cornerWidget.classList.remove('show');
            }
            
            // Reset state
            activeNudges = { visual: null, audio: null, haptic: null, text: null };
        }

        // Intervention action handlers
        function suggestBreak(pattern) {
            const suggestions = [
                "How about a 2-minute stretch? Your body will thank you! üßò‚Äç‚ôÄÔ∏è",
                "Quick water break? Sometimes our best insights come when we hydrate. üíß",
                "Stand up and look out a window for 30 seconds. Your eyes need the break! üëÄ"
            ];
            const suggestion = suggestions[Math.floor(Math.random() * suggestions.length)];
            addMessage(suggestion, 'velvet');
        }

        function offerBrainDump(pattern) {
            addMessage("Want to try a brain dump? Just type everything swirling in your mind, then we'll pick ONE thing to focus on. No judgment, just getting it all out! üß†‚ú®", 'velvet');
        }

        function offerTaskBreakdown(pattern) {
            addMessage(`I see ${pattern.app} feels overwhelming right now. What if we break it into tiny, 2-minute pieces? What's the smallest possible first step? ü™ú`, 'velvet');
        }

        function offerReflection(pattern) {
            addMessage("Taking time to think is productive too! Sometimes our brains need space to process. What's on your mind? ü§î", 'velvet');
        }

        // ===== INTENTION-BASED TASK SYSTEM =====

        // Check if user message is a task declaration
        function isTaskDeclaration(message) {
            const taskKeywords = ['i need to', 'i want to', 'i have to', 'i should', 'help me', 'can you help me with', 'i\'m going to', 'i will', 'task:', 'project:', 'work on'];
            const lowerMessage = message.toLowerCase();
            return taskKeywords.some(keyword => lowerMessage.includes(keyword));
        }

        // Create AI prompt for task breakdown
        function createTaskBreakdownPrompt(goal) {
            return `You are helping someone with ADHD break down a task. Respond ONLY with valid JSON, no extra text.

Task: "${goal}"

Create 3-5 micro-steps (2-5 minutes each). Be specific about actions and include expected apps.

RESPOND WITH ONLY THIS JSON FORMAT:
{
  "goal": "${goal}",
  "steps": [
    {
      "task": "Open Word and create new document",
      "estimatedMinutes": 2,
      "expectedApps": ["Word"],
      "tips": "Just get started, perfection comes later!"
    },
    {
      "task": "Write a working title at the top",
      "estimatedMinutes": 3,
      "expectedApps": ["Word"],
      "tips": "Any title works - you can change it!"
    }
  ]
}

IMPORTANT: Return ONLY valid JSON, no explanations or extra text.`;
        }

        // Enhanced process task declaration with AI intelligence
        async function processTaskDeclaration(userMessage) {
            console.log('üéØ Processing enhanced task declaration:', userMessage);
            
            // Extract the goal from the message with better parsing
            let goal = userMessage.replace(/^(i need to|i want to|i have to|i should|help me|can you help me with|i'm going to|i will|task:|project:|let me|let's|time to)/i, '').trim();
            
            // Record task declaration for behavior analysis
            recordTaskDeclaration(userMessage, goal);
            
            addMessage("Perfect! Let me break this down into tiny, manageable steps for you. One moment... üß†", 'velvet');
            
            try {
                // Get AI breakdown of the task
                const breakdownPrompt = createTaskBreakdownPrompt(goal);
                const response = await getVelvetResponse(breakdownPrompt);
                
                // Try to parse JSON response with better error handling
                let taskBreakdown;
                try {
                    console.log('üîç Raw AI response:', response);
                    
                    // Try to extract and fix JSON from response
                    let jsonText = response;
                    
                    // Look for JSON structure in the response
                    const jsonMatch = response.match(/\{[\s\S]*\}/);
                    if (jsonMatch) {
                        jsonText = jsonMatch[0];
                        console.log('üîç Extracted JSON:', jsonText);
                        
                        // Try to fix common JSON issues
                        // Remove trailing commas and fix incomplete structures
                        jsonText = jsonText.replace(/,(\s*[}\]])/g, '$1'); // Remove trailing commas
                        
                        // If JSON seems incomplete, try to complete it
                        if (!jsonText.includes('}]')) {
                            console.log('üîß JSON appears incomplete, attempting to fix...');
                            // Count opening vs closing braces/brackets to fix structure
                            const openBraces = (jsonText.match(/\{/g) || []).length;
                            const closeBraces = (jsonText.match(/\}/g) || []).length;
                            const openBrackets = (jsonText.match(/\[/g) || []).length;
                            const closeBrackets = (jsonText.match(/\]/g) || []).length;
                            
                            // Add missing closing brackets/braces
                            for (let i = 0; i < openBrackets - closeBrackets; i++) {
                                jsonText += ']';
                            }
                            for (let i = 0; i < openBraces - closeBraces; i++) {
                                jsonText += '}';
                            }
                        }
                        
                        taskBreakdown = JSON.parse(jsonText);
                        console.log('‚úÖ Successfully parsed task breakdown:', taskBreakdown);
                    } else {
                        throw new Error('No JSON structure found in response');
                    }
                } catch (e) {
                    console.error('‚ùå Failed to parse task breakdown:', e);
                    console.error('‚ùå Response was:', response);
                    
                    // Enhanced fallback - create a better breakdown based on the goal
                    const goalWords = goal.toLowerCase();
                    let fallbackSteps = [];
                    
                    if (goalWords.includes('write') || goalWords.includes('essay') || goalWords.includes('report')) {
                        fallbackSteps = [
                            { task: "Open your preferred writing app and create a new document", estimatedMinutes: 2, expectedApps: ["Word", "Google Docs", "Pages"], tips: "Just get the blank page ready!" },
                            { task: "Write a working title at the top", estimatedMinutes: 3, expectedApps: ["Word", "Google Docs", "Pages"], tips: "It doesn't have to be perfect - you can change it later!" },
                            { task: "List 3-4 main points you want to cover", estimatedMinutes: 4, expectedApps: ["Word", "Google Docs", "Pages"], tips: "Just bullet points for now - keep it simple!" },
                            { task: "Write one paragraph about your first point", estimatedMinutes: 5, expectedApps: ["Word", "Google Docs", "Pages"], tips: "Start with whatever comes to mind - you can polish later!" }
                        ];
                    } else {
                        fallbackSteps = [
                            { task: "Set up your workspace and gather any materials you need", estimatedMinutes: 3, expectedApps: ["Finder", "Chrome"], tips: "Just get everything ready first!" },
                            { task: "Start with the smallest possible first step", estimatedMinutes: 5, expectedApps: [], tips: "One tiny step at a time - you've got this!" },
                            { task: "Continue with the next small piece", estimatedMinutes: 5, expectedApps: [], tips: "Building momentum with each small win!" }
                        ];
                    }
                    
                    taskBreakdown = {
                        goal: goal,
                        steps: fallbackSteps
                    };
                    
                    console.log('üîÑ Using fallback task breakdown:', taskBreakdown);
                }
                
                // Set up the current task
                currentTask = {
                    goal: taskBreakdown.goal,
                    steps: taskBreakdown.steps,
                    currentStepIndex: 0,
                    startTime: Date.now(),
                    expectedApps: taskBreakdown.steps[0]?.expectedApps || [],
                    isActive: true
                };
                
                // Show the task checklist
                showTaskChecklist();
                
                // Start intent monitoring
                startIntentMonitoring();
                
                // Initialize and start focus analytics
                if (!focusAnalytics.isInitialized) {
                    initializeFocusAnalytics();
                }
                startFocusSession(categorizeTask(taskBreakdown.goal));
                
                // Validate we have valid steps
                if (!taskBreakdown.steps || taskBreakdown.steps.length === 0) {
                    console.error('‚ùå No valid steps found, creating minimal fallback');
                    taskBreakdown.steps = [
                        { task: "Take the first small step", estimatedMinutes: 5, expectedApps: [], tips: "Start with whatever feels easiest!" }
                    ];
                }

                // Give encouraging message
                addMessage(`Great! I've broken this into ${taskBreakdown.steps.length} micro-steps. Your checklist is now showing on the right. Let's start with step 1! üéØ`, 'velvet');
                
                // Whisper the first step if voice enabled
                if (voiceOutputEnabled && velvetVoice) {
                    setTimeout(() => {
                        const firstStep = taskBreakdown.steps[0];
                        velvetVoice.speak(`Let's start! Step 1: ${firstStep.task}. This should take about ${firstStep.estimatedMinutes} minutes. ${firstStep.tips}`);
                    }, 2000);
                }
                
            } catch (error) {
                console.error('Error processing task declaration:', error);
                addMessage("I had trouble breaking that down, but I'm here to help! Can you tell me the first small step you'd like to take? üíú", 'velvet');
            }
        }

        // Enhanced task intelligence functions 
        function recordTaskDeclaration(userMessage, goal) {
            const timestamp = Date.now();
            
            // Update current task with enhanced intelligence
            currentTask.contextClues.push({
                type: 'declaration',
                content: userMessage,
                timestamp: timestamp,
                confidence: calculateDeclarationConfidence(userMessage)
            });
            
            // Analyze task complexity and provide better time estimates
            const complexity = analyzeTaskComplexity(goal);
            currentTask.expectedDuration = estimateTaskDuration(goal, complexity);
            
            // Record for behavior analysis
            intentMonitoring.behaviorAnalysis.workingSessions.push({
                startTime: timestamp,
                taskType: categorizeTask(goal),
                declaredGoal: goal,
                complexity: complexity
            });
            
            console.log('üìä Task intelligence recorded:', {
                complexity: complexity,
                expectedDuration: currentTask.expectedDuration,
                confidence: currentTask.contextClues[currentTask.contextClues.length - 1].confidence
            });
        }
        
        function calculateDeclarationConfidence(message) {
            let confidence = 0.5; // Base confidence
            
            // Higher confidence for explicit task keywords
            const taskKeywords = ['need to', 'have to', 'should', 'going to', 'will', 'task', 'project'];
            const foundKeywords = taskKeywords.filter(keyword => 
                message.toLowerCase().includes(keyword)
            ).length;
            
            confidence += foundKeywords * 0.1;
            
            // Higher confidence for specific action verbs
            const actionVerbs = ['write', 'create', 'build', 'design', 'finish', 'complete', 'start'];
            const foundActions = actionVerbs.filter(action => 
                message.toLowerCase().includes(action)
            ).length;
            
            confidence += foundActions * 0.15;
            
            return Math.min(confidence, 1.0);
        }
        
        function analyzeTaskComplexity(goal) {
            const goalLower = goal.toLowerCase();
            let complexity = 1; // 1 = simple, 5 = very complex
            
            // Complexity indicators
            const complexityIndicators = {
                simple: ['write', 'read', 'check', 'call', 'email', 'quick'],
                moderate: ['create', 'design', 'plan', 'research', 'analyze'],
                complex: ['build', 'develop', 'implement', 'integrate', 'optimize'],
                veryComplex: ['architect', 'refactor', 'restructure', 'comprehensive']
            };
            
            if (complexityIndicators.veryComplex.some(word => goalLower.includes(word))) {
                complexity = 5;
            } else if (complexityIndicators.complex.some(word => goalLower.includes(word))) {
                complexity = 4;
            } else if (complexityIndicators.moderate.some(word => goalLower.includes(word))) {
                complexity = 3;
            } else if (complexityIndicators.simple.some(word => goalLower.includes(word))) {
                complexity = 1;
            } else {
                complexity = 2; // Default moderate-simple
            }
            
            // Adjust for multi-part tasks
            const parts = goal.split(/\band\b|\bor\b|\bthen\b|\bnext\b/);
            if (parts.length > 1) {
                complexity += Math.min(parts.length - 1, 2);
            }
            
            return Math.min(complexity, 5);
        }
        
        function estimateTaskDuration(goal, complexity) {
            // Base time estimates in minutes
            const baseTimeByComplexity = {
                1: 15, // Simple: 15 minutes
                2: 30, // Moderate-simple: 30 minutes  
                3: 60, // Moderate: 1 hour
                4: 120, // Complex: 2 hours
                5: 240  // Very complex: 4 hours
            };
            
            let estimatedMinutes = baseTimeByComplexity[complexity] || 30;
            
            // Adjust based on task type
            const goal_lower = goal.toLowerCase();
            if (goal_lower.includes('research') || goal_lower.includes('analyze')) {
                estimatedMinutes *= 1.5;
            }
            if (goal_lower.includes('write') && goal_lower.includes('report')) {
                estimatedMinutes *= 1.3;
            }
            if (goal_lower.includes('quick') || goal_lower.includes('brief')) {
                estimatedMinutes *= 0.5;
            }
            
            return Math.round(estimatedMinutes);
        }
        
        function categorizeTask(goal) {
            const goalLower = goal.toLowerCase();
            
            if (goalLower.includes('write') || goalLower.includes('draft') || goalLower.includes('report')) {
                return 'writing';
            } else if (goalLower.includes('code') || goalLower.includes('program') || goalLower.includes('develop')) {
                return 'development';
            } else if (goalLower.includes('research') || goalLower.includes('study') || goalLower.includes('analyze')) {
                return 'research';
            } else if (goalLower.includes('design') || goalLower.includes('create') || goalLower.includes('build')) {
                return 'creative';
            } else if (goalLower.includes('email') || goalLower.includes('call') || goalLower.includes('meeting')) {
                return 'communication';
            } else if (goalLower.includes('organize') || goalLower.includes('plan') || goalLower.includes('schedule')) {
                return 'organization';
            } else {
                return 'general';
            }
        }

        // Show the task checklist widget in separate window
        async function showTaskChecklist() {
            if (!currentTask.isActive) return;
            
            try {
                await window.electronAPI.checklist.show(currentTask);
                console.log('‚úÖ Checklist window shown');
            } catch (error) {
                console.error('‚ùå Failed to show checklist window:', error);
                // Fallback to embedded checklist
                showEmbeddedChecklist();
            }
        }

        // Fallback embedded checklist (if separate window fails)
        function showEmbeddedChecklist() {
            if (!currentTask.isActive) return;
            
            // Update the goal
            taskGoal.textContent = `Goal: ${currentTask.goal}`;
            
            // Clear and populate the task list
            taskList.innerHTML = '';
            
            currentTask.steps.forEach((step, index) => {
                const listItem = document.createElement('li');
                listItem.className = 'task-item';
                listItem.innerHTML = `
                    <div class="task-checkbox ${index < currentTask.currentStepIndex ? 'completed' : ''}"></div>
                    <div class="task-content">
                        <div class="task-text">${step.task}</div>
                        <div class="task-time">${step.estimatedMinutes} min${step.expectedApps.length > 0 ? ' ‚Ä¢ ' + step.expectedApps.join(', ') : ''}</div>
                    </div>
                `;
                
                // Add completed class to completed items
                if (index < currentTask.currentStepIndex) {
                    listItem.classList.add('completed');
                }
                
                taskList.appendChild(listItem);
            });
            
            // Update progress bar
            updateTaskProgress();
            
            // Show the widget
            taskChecklist.classList.add('show');
        }

        // Update task progress
        function updateTaskProgress() {
            const completedSteps = currentTask.currentStepIndex;
            const totalSteps = currentTask.steps.length;
            const progressPercent = Math.round((completedSteps / totalSteps) * 100);
            
            taskProgressFill.style.width = `${progressPercent}%`;
            taskProgressText.textContent = `${progressPercent}% complete (${completedSteps}/${totalSteps} steps)`;
        }

        // Enhanced intent monitoring with AI behavior analysis
        function startIntentMonitoring() {
            intentMonitoring.isActive = true;
            intentMonitoring.lastAppSwitchTime = Date.now();
            intentMonitoring.deviationCount = 0;
            intentMonitoring.onTrackTime = 0;
            
            // Initialize enhanced monitoring
            intentMonitoring.behaviorAnalysis.focusPeriods = [];
            intentMonitoring.behaviorAnalysis.distractionTriggers = [];
            intentMonitoring.contextualIntelligence.currentWorkflow = 'task-started';
            
            // Start focus period tracking
            startFocusPeriodTracking();
            
            // Initialize adaptive nudging
            initializeAdaptiveNudging();
            
            console.log('üéØ Intent monitoring started for:', currentTask.goal);
        }
        
        // Enhanced focus period tracking
        function startFocusPeriodTracking() {
            const startTime = Date.now();
            
            // Track current focus period
            const currentFocusPeriod = {
                startTime: startTime,
                taskType: currentTask.goal ? categorizeTask(currentTask.goal) : 'general',
                expectedDuration: currentTask.expectedDuration || 30,
                appSwitches: 0,
                distractions: [],
                productivityEvents: []
            };
            
            intentMonitoring.behaviorAnalysis.focusPeriods.push(currentFocusPeriod);
            
            console.log('üß† Focus period tracking started:', currentFocusPeriod);
        }
        
        function initializeAdaptiveNudging() {
            // Calculate personalized nudging based on user history
            const previousSessions = intentMonitoring.behaviorAnalysis.workingSessions;
            
            if (previousSessions.length > 0) {
                // Analyze past effectiveness
                const avgSessionLength = previousSessions.reduce((sum, session) => 
                    sum + (session.duration || 0), 0) / previousSessions.length;
                
                // Adjust nudging timing based on user's typical attention span
                if (avgSessionLength < 10 * 60 * 1000) { // Less than 10 minutes
                    intentMonitoring.adaptiveNudging.personalizedTiming = 5 * 60 * 1000; // 5 min
                    intentMonitoring.adaptiveNudging.gentlenessLevel = 0.9; // Very gentle
                } else if (avgSessionLength > 30 * 60 * 1000) { // More than 30 minutes
                    intentMonitoring.adaptiveNudging.personalizedTiming = 15 * 60 * 1000; // 15 min
                    intentMonitoring.adaptiveNudging.gentlenessLevel = 0.5; // Less gentle
                }
            }
            
            console.log('ü§ñ Adaptive nudging initialized:', {
                timing: intentMonitoring.adaptiveNudging.personalizedTiming / 1000 / 60 + ' minutes',
                gentleness: intentMonitoring.adaptiveNudging.gentlenessLevel
            });
        }
        
        // Enhanced task completion detection with AI
        function detectTaskCompletion(windowName, appName) {
            if (!currentTask.isActive) return;
            
            const currentStep = currentTask.steps[currentTask.currentStepIndex];
            if (!currentStep) return;
            
            // Analyze completion confidence based on multiple factors
            let completionSignals = [];
            
            // 1. Expected app usage patterns
            if (currentStep.expectedApps && currentStep.expectedApps.length > 0) {
                const isUsingExpectedApp = currentStep.expectedApps.some(expectedApp => 
                    appName.toLowerCase().includes(expectedApp.toLowerCase()) ||
                    windowName.toLowerCase().includes(expectedApp.toLowerCase())
                );
                
                if (isUsingExpectedApp) {
                    completionSignals.push({
                        type: 'expected_app_usage',
                        confidence: 0.3,
                        evidence: `Using expected app: ${appName}`
                    });
                }
            }
            
            // 2. Time-based completion signals
            const stepStartTime = currentTask.timeSpentPerStep[currentTask.currentStepIndex] || Date.now();
            const timeSpent = Date.now() - stepStartTime;
            const expectedTime = (currentStep.estimatedMinutes || 5) * 60 * 1000;
            
            if (timeSpent >= expectedTime * 0.7) { // 70% of expected time
                const timeConfidence = Math.min(timeSpent / expectedTime, 1.0) * 0.4;
                completionSignals.push({
                    type: 'time_progression',
                    confidence: timeConfidence,
                    evidence: `Spent ${Math.round(timeSpent/60000)}min of expected ${currentStep.estimatedMinutes}min`
                });
            }
            
            // 3. Context-based completion detection
            const contextSignals = analyzeContextualCompletionSignals(windowName, appName, currentStep);
            completionSignals.push(...contextSignals);
            
            // Calculate overall completion confidence
            const totalConfidence = completionSignals.reduce((sum, signal) => sum + signal.confidence, 0);
            currentTask.completionConfidence = Math.min(totalConfidence, 1.0);
            
            // Auto-advance if confidence is high enough
            if (currentTask.completionConfidence >= 0.8) {
                console.log('üéØ High completion confidence detected:', completionSignals);
                suggestStepCompletion();
            } else if (currentTask.completionConfidence >= 0.5) {
                console.log('ü§î Moderate completion confidence:', currentTask.completionConfidence);
                // Maybe show a gentle nudge asking if they're done with this step
            }
            
            // Record progress for analysis
            currentTask.contextClues.push({
                type: 'completion_analysis',
                confidence: currentTask.completionConfidence,
                signals: completionSignals,
                timestamp: Date.now()
            });
        }
        
        function analyzeContextualCompletionSignals(windowName, appName, currentStep) {
            const signals = [];
            const stepText = currentStep.task.toLowerCase();
            const windowLower = windowName.toLowerCase();
            
            // Analyze window title patterns for completion clues
            if (stepText.includes('write') || stepText.includes('document')) {
                if (windowLower.includes('document') || windowLower.includes('draft')) {
                    signals.push({
                        type: 'document_context',
                        confidence: 0.2,
                        evidence: 'Working in document application'
                    });
                }
                
                // Look for saved document indicators
                if (windowLower.includes('saved') || windowLower.includes('untitled') === false) {
                    signals.push({
                        type: 'document_saved',
                        confidence: 0.4,
                        evidence: 'Document appears to be saved'
                    });
                }
            }
            
            if (stepText.includes('email') || stepText.includes('message')) {
                if (windowLower.includes('mail') || windowLower.includes('compose') || windowLower.includes('draft')) {
                    signals.push({
                        type: 'email_context',
                        confidence: 0.3,
                        evidence: 'Working in email application'
                    });
                }
            }
            
            if (stepText.includes('research') || stepText.includes('search')) {
                if (windowLower.includes('search') || windowLower.includes('google') || windowLower.includes('results')) {
                    signals.push({
                        type: 'research_context',
                        confidence: 0.2,
                        evidence: 'Actively researching/searching'
                    });
                }
            }
            
            return signals;
        }
        
        function suggestStepCompletion() {
            if (!currentTask.isActive) return;
            
            const currentStep = currentTask.steps[currentTask.currentStepIndex];
            if (!currentStep) return;
            
            // Show gentle completion suggestion
            const encouragement = [
                "Nice work! It looks like you might be ready for the next step üéâ",
                "Great progress! Ready to move forward? ‚ú®",
                "You're doing awesome! Time for the next piece? üöÄ",
                "Fantastic! Shall we tackle the next step? üí™"
            ];
            
            const message = encouragement[Math.floor(Math.random() * encouragement.length)];
            
            // Use gentle whisper instead of full message
            if (voiceOutputEnabled && velvetVoice) {
                setTimeout(() => {
                    velvetVoice.speak(message);
                }, 1000);
            }
            
            // Also show visual indicator in checklist if possible
            console.log('üí° Suggested step completion:', message);
        }

        // Focus Analytics Implementation
        function initializeFocusAnalytics() {
            focusAnalytics.isInitialized = true;
            focusAnalytics.sessionStartTime = Date.now();
            
            // Load existing data from localStorage
            loadFocusAnalyticsData();
            
            console.log('üìä Focus analytics initialized');
        }
        
        function startFocusSession(taskType = 'general') {
            const now = Date.now();
            
            // End previous session if exists
            if (focusAnalytics.currentFocusSession.startTime) {
                endFocusSession();
            }
            
            // Start new session
            focusAnalytics.currentFocusSession = {
                startTime: now,
                endTime: null,
                taskType: taskType,
                appSwitches: 0,
                distractions: [],
                completedSteps: 0,
                totalSteps: currentTask.steps ? currentTask.steps.length : 0,
                focusScore: 100,
                productivityEvents: [],
                audioContext: 'quiet'
            };
            
            console.log('üéØ Focus session started:', taskType);
            
            // Schedule periodic updates
            scheduleFocusSessionUpdates();
        }
        
        function endFocusSession() {
            if (!focusAnalytics.currentFocusSession.startTime) return;
            
            const now = Date.now();
            const session = focusAnalytics.currentFocusSession;
            session.endTime = now;
            
            // Calculate session metrics
            const duration = now - session.startTime;
            const focusEfficiency = calculateFocusEfficiency(session);
            
            // Create session record
            const sessionRecord = {
                ...session,
                duration: duration,
                focusEfficiency: focusEfficiency,
                completionRate: session.totalSteps > 0 ? session.completedSteps / session.totalSteps : 0,
                averageDistractionInterval: session.distractions.length > 0 ? 
                    duration / session.distractions.length : duration,
                date: new Date().toISOString().split('T')[0],
                hour: new Date().getHours()
            };
            
            // Store session
            focusAnalytics.sessions.push(sessionRecord);
            
            // Update analytics
            updateProductivityMetrics(sessionRecord);
            updateBehavioralPatterns(sessionRecord);
            
            // Save to localStorage
            saveFocusAnalyticsData();
            
            console.log('üìä Focus session ended:', {
                duration: Math.round(duration / 60000) + ' minutes',
                focusEfficiency: Math.round(focusEfficiency * 100) + '%',
                distractions: session.distractions.length
            });
            
            // Reset current session
            focusAnalytics.currentFocusSession = {
                startTime: null,
                endTime: null,
                taskType: null,
                appSwitches: 0,
                distractions: [],
                completedSteps: 0,
                totalSteps: 0,
                focusScore: 100,
                productivityEvents: [],
                audioContext: 'quiet'
            };
        }
        
        function recordProductivityEvent(eventType, data = {}) {
            if (!focusAnalytics.currentFocusSession.startTime) return;
            
            const event = {
                type: eventType,
                timestamp: Date.now(),
                data: data
            };
            
            focusAnalytics.currentFocusSession.productivityEvents.push(event);
            
            // Update focus score based on event
            switch (eventType) {
                case 'task_completed':
                    focusAnalytics.currentFocusSession.focusScore += 5;
                    focusAnalytics.currentFocusSession.completedSteps++;
                    break;
                case 'distraction':
                    focusAnalytics.currentFocusSession.focusScore -= 10;
                    focusAnalytics.currentFocusSession.distractions.push(event);
                    break;
                case 'app_switch':
                    focusAnalytics.currentFocusSession.appSwitches++;
                    if (focusAnalytics.currentFocusSession.appSwitches > 5) {
                        focusAnalytics.currentFocusSession.focusScore -= 2;
                    }
                    break;
                case 'deep_focus':
                    focusAnalytics.currentFocusSession.focusScore += 3;
                    break;
            }
            
            // Clamp focus score
            focusAnalytics.currentFocusSession.focusScore = Math.max(0, 
                Math.min(100, focusAnalytics.currentFocusSession.focusScore));
        }
        
        function calculateFocusEfficiency(session) {
            if (!session.startTime || !session.endTime) return 0;
            
            const duration = session.endTime - session.startTime;
            const baseEfficiency = 0.7; // Base 70% efficiency
            
            // Adjust for distractions
            const distractionPenalty = Math.min(session.distractions.length * 0.1, 0.5);
            
            // Adjust for completion rate
            const completionBonus = session.completionRate * 0.3;
            
            // Adjust for app switches (moderate switching is normal)
            const switchPenalty = session.appSwitches > 10 ? 
                Math.min((session.appSwitches - 10) * 0.02, 0.2) : 0;
            
            const efficiency = baseEfficiency - distractionPenalty + completionBonus - switchPenalty;
            return Math.max(0, Math.min(1, efficiency));
        }
        
        function updateProductivityMetrics(sessionRecord) {
            const metrics = focusAnalytics.productivityMetrics;
            
            // Update averages
            const totalSessions = focusAnalytics.sessions.length;
            const totalFocusTime = focusAnalytics.sessions.reduce((sum, s) => sum + s.duration, 0);
            
            metrics.averageFocusTime = totalFocusTime / totalSessions / 60000; // minutes
            metrics.totalTasksCompleted += sessionRecord.completedSteps;
            metrics.averageTaskCompletionTime = sessionRecord.duration / 
                Math.max(sessionRecord.completedSteps, 1) / 60000; // minutes per task
            
            // Update distraction frequency (per hour)
            metrics.distractionFrequency = focusAnalytics.sessions.reduce((sum, s) => 
                sum + s.distractions.length, 0) / (totalFocusTime / 3600000);
            
            // Update task type performance
            if (!metrics.taskTypePerformance[sessionRecord.taskType]) {
                metrics.taskTypePerformance[sessionRecord.taskType] = {
                    sessions: 0,
                    averageEfficiency: 0,
                    totalTime: 0,
                    completionRate: 0
                };
            }
            
            const taskPerf = metrics.taskTypePerformance[sessionRecord.taskType];
            taskPerf.sessions++;
            taskPerf.averageEfficiency = (taskPerf.averageEfficiency * (taskPerf.sessions - 1) + 
                sessionRecord.focusEfficiency) / taskPerf.sessions;
            taskPerf.totalTime += sessionRecord.duration;
            taskPerf.completionRate = (taskPerf.completionRate * (taskPerf.sessions - 1) + 
                sessionRecord.completionRate) / taskPerf.sessions;
            
            // Update productive hours
            updateProductiveHours(sessionRecord);
        }
        
        function updateProductiveHours(sessionRecord) {
            const hour = sessionRecord.hour;
            const efficiency = sessionRecord.focusEfficiency;
            
            // Update hourly efficiency tracking
            if (!focusAnalytics.dailyStats[hour]) {
                focusAnalytics.dailyStats[hour] = {
                    sessions: 0,
                    totalEfficiency: 0,
                    averageEfficiency: 0
                };
            }
            
            const hourStats = focusAnalytics.dailyStats[hour];
            hourStats.sessions++;
            hourStats.totalEfficiency += efficiency;
            hourStats.averageEfficiency = hourStats.totalEfficiency / hourStats.sessions;
            
            // Update most productive hours (top 3)
            const hourlyAverages = Object.entries(focusAnalytics.dailyStats)
                .map(([h, stats]) => ({ hour: parseInt(h), efficiency: stats.averageEfficiency }))
                .sort((a, b) => b.efficiency - a.efficiency)
                .slice(0, 3);
            
            focusAnalytics.productivityMetrics.mostProductiveHours = hourlyAverages;
        }
        
        function updateBehavioralPatterns(sessionRecord) {
            const patterns = focusAnalytics.behavioralPatterns;
            
            // Update optimal work duration
            const allDurations = focusAnalytics.sessions.map(s => s.duration / 60000); // minutes
            if (allDurations.length > 0) {
                patterns.optimalWorkDuration = allDurations.reduce((sum, d) => sum + d, 0) / allDurations.length;
            }
            
            // Analyze distractors
            sessionRecord.distractions.forEach(distraction => {
                const distractorType = distraction.data?.type || 'unknown';
                if (!patterns.commonDistractors.find(d => d.type === distractorType)) {
                    patterns.commonDistractors.push({ type: distractorType, count: 1 });
                } else {
                    patterns.commonDistractors.find(d => d.type === distractorType).count++;
                }
            });
            
            // Sort common distractors
            patterns.commonDistractors.sort((a, b) => b.count - a.count);
            patterns.commonDistractors = patterns.commonDistractors.slice(0, 5); // Top 5
            
            // Update working style profile
            const avgEfficiency = focusAnalytics.productivityMetrics.taskTypePerformance;
            const avgDistractions = focusAnalytics.productivityMetrics.distractionFrequency;
            
            if (avgDistractions < 1 && patterns.optimalWorkDuration > 30) {
                patterns.workingStyleProfile = 'focused';
            } else if (avgDistractions > 3 && patterns.optimalWorkDuration < 20) {
                patterns.workingStyleProfile = 'flexible';
            } else {
                patterns.workingStyleProfile = 'balanced';
            }
        }
        
        function generateFocusInsights() {
            if (focusAnalytics.sessions.length < 3) {
                return {
                    insights: ["Keep working! I need a few more sessions to provide meaningful insights."],
                    recommendations: ["Complete a few more tasks to unlock personalized recommendations."]
                };
            }
            
            const insights = [];
            const recommendations = [];
            const metrics = focusAnalytics.productivityMetrics;
            const patterns = focusAnalytics.behavioralPatterns;
            
            // Focus time insights
            if (metrics.averageFocusTime < 15) {
                insights.push(`Your average focus time is ${Math.round(metrics.averageFocusTime)} minutes - shorter sessions can be very effective!`);
                recommendations.push("Try the Pomodoro technique: 25-minute focused sessions with 5-minute breaks.");
            } else if (metrics.averageFocusTime > 45) {
                insights.push(`You work in long ${Math.round(metrics.averageFocusTime)}-minute sessions - great for deep work!`);
                recommendations.push("Consider taking short breaks every hour to maintain peak performance.");
            }
            
            // Distraction insights
            if (metrics.distractionFrequency < 1) {
                insights.push("You have excellent focus with minimal distractions!");
                recommendations.push("Your focus is strong - try tackling more complex tasks during your peak hours.");
            } else if (metrics.distractionFrequency > 3) {
                insights.push(`You average ${Math.round(metrics.distractionFrequency)} distractions per hour.`);
                recommendations.push("Try turning off notifications during focus sessions to reduce interruptions.");
            }
            
            // Productive hours
            if (metrics.mostProductiveHours.length > 0) {
                const topHour = metrics.mostProductiveHours[0];
                const timeStr = topHour.hour < 12 ? `${topHour.hour}AM` : 
                    topHour.hour === 12 ? '12PM' : `${topHour.hour - 12}PM`;
                insights.push(`Your most productive time is around ${timeStr} with ${Math.round(topHour.efficiency * 100)}% efficiency.`);
                recommendations.push(`Schedule your most important tasks around ${timeStr} for best results.`);
            }
            
            // Task type performance
            const bestTaskType = Object.entries(metrics.taskTypePerformance)
                .sort(([,a], [,b]) => b.averageEfficiency - a.averageEfficiency)[0];
            
            if (bestTaskType) {
                insights.push(`You perform best on ${bestTaskType[0]} tasks with ${Math.round(bestTaskType[1].averageEfficiency * 100)}% efficiency.`);
            }
            
            return { insights, recommendations };
        }
        
        function saveFocusAnalyticsData() {
            try {
                localStorage.setItem('velvet_focus_analytics', JSON.stringify({
                    sessions: focusAnalytics.sessions.slice(-50), // Keep last 50 sessions
                    productivityMetrics: focusAnalytics.productivityMetrics,
                    behavioralPatterns: focusAnalytics.behavioralPatterns,
                    dailyStats: focusAnalytics.dailyStats
                }));
            } catch (error) {
                console.error('Error saving focus analytics:', error);
            }
        }
        
        function loadFocusAnalyticsData() {
            try {
                const saved = localStorage.getItem('velvet_focus_analytics');
                if (saved) {
                    const data = JSON.parse(saved);
                    focusAnalytics.sessions = data.sessions || [];
                    focusAnalytics.productivityMetrics = { ...focusAnalytics.productivityMetrics, ...data.productivityMetrics };
                    focusAnalytics.behavioralPatterns = { ...focusAnalytics.behavioralPatterns, ...data.behavioralPatterns };
                    focusAnalytics.dailyStats = data.dailyStats || {};
                }
            } catch (error) {
                console.error('Error loading focus analytics:', error);
            }
        }
        
        function scheduleFocusSessionUpdates() {
            // Update focus analytics every 2 minutes during active session
            const updateInterval = setInterval(() => {
                if (!focusAnalytics.currentFocusSession.startTime) {
                    clearInterval(updateInterval);
                    return;
                }
                
                // Record deep focus periods (no activity for 2+ minutes)
                const lastActivity = focusAnalytics.currentFocusSession.lastActivity || 
                    focusAnalytics.currentFocusSession.startTime;
                const timeSinceActivity = Date.now() - lastActivity;
                
                if (timeSinceActivity > 2 * 60 * 1000) {
                    recordProductivityEvent('deep_focus', { duration: timeSinceActivity });
                }
                
            }, 2 * 60 * 1000); // Every 2 minutes
        }

        // Enhanced step completion check with AI detection
        function checkStepCompletion(windowInfo) {
            if (!currentTask.isActive || !intentMonitoring.isActive) return;
            
            // Use enhanced completion detection
            if (windowInfo && windowInfo.name) {
                detectTaskCompletion(windowInfo.name, windowInfo.app || 'Unknown');
            }
            
            const currentStep = currentTask.steps[currentTask.currentStepIndex];
            if (!currentStep) return;
            
            // Simple heuristic: if user spent reasonable time in expected app, mark as complete
            const timeInApp = Date.now() - intentMonitoring.lastAppSwitchTime;
            const expectedApps = currentStep.expectedApps;
            
            // If user spent 2+ minutes in an expected app, consider step complete
            if (timeInApp > 2 * 60 * 1000 && expectedApps.some(app => windowInfo.name.includes(app))) {
                completeCurrentStep();
            }
        }

        // Mark current step as completed and move to next
        function completeCurrentStep() {
            if (!currentTask.isActive) return;
            
            currentTask.currentStepIndex++;
            
            // Update the checklist with celebration animation
            showTaskChecklist();
            
            // Also update the separate window if it exists
            if (window.electronAPI?.checklist) {
                window.electronAPI.checklist.update(currentTask);
            }
            
            // Celebrate completion
            const completedStep = currentTask.steps[currentTask.currentStepIndex - 1];
            if (completedStep) {
                addMessage(`‚úÖ Step completed: "${completedStep.task}"! You're doing great! üéâ`, 'velvet');
                
                // Whisper celebration if voice enabled
                if (voiceOutputEnabled && velvetVoice) {
                    velvetVoice.speak(`Amazing! You completed step ${currentTask.currentStepIndex}! ${completedStep.tips}`);
                }
            }
            
            // Check if all steps are done
            if (currentTask.currentStepIndex >= currentTask.steps.length) {
                completeEntireTask();
                return;
            }
            
            // Guide to next step
            const nextStep = currentTask.steps[currentTask.currentStepIndex];
            if (nextStep) {
                setTimeout(() => {
                    addMessage(`Next up: "${nextStep.task}" (${nextStep.estimatedMinutes} min). ${nextStep.tips} üéØ`, 'velvet');
                    
                    if (voiceOutputEnabled && velvetVoice) {
                        velvetVoice.speak(`Next step: ${nextStep.task}. This should take ${nextStep.estimatedMinutes} minutes.`);
                    }
                }, 2000);
                
                // Update expected apps for monitoring
                currentTask.expectedApps = nextStep.expectedApps || [];
            }
        }

        // Complete the entire task
        function completeEntireTask() {
            addMessage(`üéâ AMAZING! You completed your entire task: "${currentTask.goal}"! Every single step done. You should be proud! üåü`, 'velvet');
            
            if (voiceOutputEnabled && velvetVoice) {
                velvetVoice.speak(`Incredible! You finished everything! Your task is complete. You did amazing work!`);
            }
            
            // Reset task state
            currentTask.isActive = false;
            intentMonitoring.isActive = false;
            
            // Hide checklist after celebration
            setTimeout(() => {
                taskChecklist.classList.remove('show');
            }, 10000);
        }

        // Handle intent deviation (user doing something different than expected)
        function handleIntentDeviation(windowInfo) {
            if (!currentTask.isActive || !intentMonitoring.isActive) return;
            
            const currentStep = currentTask.steps[currentTask.currentStepIndex];
            if (!currentStep) return;
            
            const expectedApps = currentStep.expectedApps;
            const currentApp = windowInfo.name;
            const now = Date.now();
            
            // Check if user is in an expected app
            const isOnTrack = expectedApps.length === 0 || expectedApps.some(app => currentApp.includes(app));
            
            if (!isOnTrack) {
                intentMonitoring.deviationCount++;
                
                // Only whisper if it's been more than 30 seconds since last whisper
                if (now - intentMonitoring.lastWhisperTime > 30000) {
                    const whisperMessage = getIntentGuidanceMessage(currentStep, currentApp);
                    
                    // Show gentle corner notification
                    cornerWidget.innerHTML = `üéØ ${whisperMessage}`;
                    cornerWidget.className = 'corner-widget show encouraging';
                    setTimeout(() => cornerWidget.classList.remove('show'), 5000);
                    
                    // Whisper guidance if voice enabled
                    if (voiceOutputEnabled && velvetVoice) {
                        velvetVoice.speak(whisperMessage);
                    }
                    
                    intentMonitoring.lastWhisperTime = now;
                    console.log('üéØ Intent deviation guidance:', whisperMessage);
                }
            } else {
                // User is back on track
                intentMonitoring.onTrackTime = now;
            }
        }

        // Get contextual guidance message for intent deviations
        function getIntentGuidanceMessage(currentStep, currentApp) {
            const messages = [
                `I see you're in ${currentApp}. Remember: "${currentStep.task}" - you've got this!`,
                `Quick check: Step ${currentTask.currentStepIndex + 1} is "${currentStep.task}". ${currentStep.tips}`,
                `Hey, just a gentle reminder: "${currentStep.task}" when you're ready!`,
                `No pressure! When you're ready: "${currentStep.task}". Take your time.`
            ];
            
            return messages[Math.floor(Math.random() * messages.length)];
        }

        // Close task checklist (embedded version)
        taskChecklistClose.addEventListener('click', () => {
            taskChecklist.classList.remove('show', 'minimized');
            checklistState.isMinimized = false;
            
            // Also close separate window if it exists
            if (window.electronAPI?.checklist) {
                window.electronAPI.checklist.close();
            }
            
            if (currentTask.isActive) {
                addMessage("Checklist hidden, but I'm still watching your progress! You can ask me to show it again anytime. üòä", 'velvet');
            }
        });

        // Minimize/restore task checklist
        taskChecklistMinimize.addEventListener('click', () => {
            checklistState.isMinimized = !checklistState.isMinimized;
            
            if (checklistState.isMinimized) {
                taskChecklist.classList.add('minimized');
                taskChecklistMinimize.textContent = '‚ñ°';
            } else {
                taskChecklist.classList.remove('minimized');
                taskChecklistMinimize.textContent = '‚àí';
            }
        });

        // Note: Checklist dragging is now handled by the separate window system
        // The embedded checklist (fallback) no longer needs manual drag handling
        
        // Initialize voice output toggle state
        voiceOutputToggle.classList.add('muted');
        voiceOutputToggle.setAttribute('aria-label', 'Voice output off - click to enable');
        
        // Initialize screen intelligence
        initializeScreenIntelligence();
        
        // Keep the existing nudge tests
        try {
            window.testNudges = {
            hyperfocus: () => handlePatternDetection({
                type: 'hyperfocus',
                app: 'Test App',
                duration: 2700000,
                message: "You're in the zone - amazing focus! Just a gentle check: have you moved your body in the last hour?"
            }),
            distractionSpiral: () => handlePatternDetection({
                type: 'distractionSpiral',
                count: 12,
                window: 120000,
                message: "Lots of mental tabs open right now! Want to try a brain dump? Just list everything swirling around."
            }),
            taskAvoidance: () => handlePatternDetection({
                type: 'taskAvoidance',
                app: 'Cursor',
                count: 7,
                message: "I see you opening Cursor - it feels big, doesn't it? What if we just add one line of code?"
            }),
            idle: () => handlePatternDetection({
                type: 'idle',
                duration: 350000,
                message: "Taking a thinking break? Sometimes our best ideas come when we step away."
            }),
            
            // Direct test functions for debugging
            testBevelDirect: () => {
                console.log('üß™ Testing screen bevel directly...');
                const bevel = document.getElementById('screenBevelEffect');
                if (bevel) {
                    bevel.className = 'screen-bevel-effect focus active';
                    console.log('‚úÖ Bevel element found and activated');
                    setTimeout(() => {
                        bevel.classList.remove('active');
                        console.log('‚úÖ Bevel deactivated');
                    }, 5000);
                } else {
                    console.error('‚ùå Bevel element not found');
                }
            },
            
            testCornerDirect: () => {
                console.log('üß™ Testing corner widget directly...');
                const corner = document.getElementById('cornerWidget');
                if (corner) {
                    corner.innerHTML = 'üåü Direct test working!';
                    corner.className = 'corner-widget show focus';
                    console.log('‚úÖ Corner widget activated');
                    setTimeout(() => {
                        corner.classList.remove('show');
                    }, 3000);
                } else {
                    console.error('‚ùå Corner widget not found');
                }
            }
        };
        
        console.log('üîß testNudges object created:', typeof window.testNudges);
        } catch (error) {
            console.error('‚ùå Error creating testNudges:', error);
        }
        
        // Make functions available globally
        window.addMessage = addMessage;
        window.updateVoiceState = updateVoiceState;
        
        // Simple debug function to check if checklist works
        window.debugChecklist = function() {
            console.log('üîç Debug checklist function called');
            const checklistElement = document.getElementById('taskChecklist');
            if (checklistElement) {
                console.log('‚úÖ Checklist element found');
                checklistElement.classList.add('show');
                console.log('‚úÖ Show class added to checklist');
            } else {
                console.error('‚ùå Checklist element not found');
            }
        };
        
        // Direct task creation function that bypasses testSystem
        window.createTestTask = function() {
            console.log('üß™ Creating test task directly...');
            
            // Create a simple test task
            currentTask = {
                goal: "Write an essay on women empowerment",
                steps: [
                    { task: "Open Word and create new document", estimatedMinutes: 2, expectedApps: ["Word"], tips: "Just get started!" },
                    { task: "Write a working title at the top", estimatedMinutes: 3, expectedApps: ["Word"], tips: "Any title works!" },
                    { task: "List 3-4 main points you want to cover", estimatedMinutes: 4, expectedApps: ["Word"], tips: "Just bullet points!" },
                    { task: "Write one paragraph about your first point", estimatedMinutes: 5, expectedApps: ["Word"], tips: "Start with whatever comes to mind!" }
                ],
                currentStepIndex: 0,
                startTime: Date.now(),
                expectedApps: ["Word"],
                isActive: true
            };
            
            console.log('‚úÖ Test task created:', currentTask);
            
            // Show the checklist in separate window
            showTaskChecklist();
            
            // Start intent monitoring
            startIntentMonitoring();
            
            console.log('‚úÖ Task system activated - checklist should be visible!');
        };
        
        // Manual test functions for task system (created after all functions are defined)
        try {
            window.testSystem = {
                // Task system tests
                testTaskDeclaration: () => {
                    console.log('üß™ Testing task declaration detection...');
                    const testMessage = "I need to write a report about Q4 results";
                    if (isTaskDeclaration(testMessage)) {
                        console.log('‚úÖ Task declaration detected correctly');
                        processTaskDeclaration(testMessage);
                    } else {
                        console.error('‚ùå Task declaration not detected');
                    }
                },
                
                testIntentMonitoring: () => {
                    console.log('üß™ Testing intent monitoring...');
                    if (currentTask.isActive) {
                        // Simulate window change to test intent deviation
                        const testWindow = { name: 'Discord', title: 'Random chat' };
                        handleIntentDeviation(testWindow);
                        console.log('‚úÖ Intent deviation test triggered');
                    } else {
                        console.log('‚ö†Ô∏è No active task - create a task first with testTaskDeclaration()');
                    }
                },
                
                testStepCompletion: () => {
                    console.log('üß™ Testing step completion...');
                    if (currentTask.isActive) {
                        completeCurrentStep();
                        console.log('‚úÖ Step completion test triggered');
                    } else {
                        console.log('‚ö†Ô∏è No active task - create a task first with testTaskDeclaration()');
                    }
                },
                
                showTaskState: () => {
                    console.log('üìä Current task state:', {
                        isActive: currentTask.isActive,
                        goal: currentTask.goal,
                        currentStep: currentTask.currentStepIndex,
                        totalSteps: currentTask.steps.length,
                        intentMonitoring: intentMonitoring
                    });
                },
                
                forceShowChecklist: () => {
                    console.log('üß™ Force showing checklist...');
                    // Create a test task if none exists
                    if (!currentTask.isActive) {
                        currentTask = {
                            goal: "Test essay writing task",
                            steps: [
                                { task: "Open Word and create new document", estimatedMinutes: 2, expectedApps: ["Word"], tips: "Just get started!" },
                                { task: "Write a working title", estimatedMinutes: 3, expectedApps: ["Word"], tips: "Any title works!" },
                                { task: "Write one paragraph", estimatedMinutes: 5, expectedApps: ["Word"], tips: "Don't overthink it!" }
                            ],
                            currentStepIndex: 0,
                            startTime: Date.now(),
                            expectedApps: ["Word"],
                            isActive: true
                        };
                    }
                    
                    // Force show the checklist
                    showTaskChecklist();
                    console.log('‚úÖ Checklist should now be visible');
                },
                
                testAmbientAudio: async () => {
                    console.log('üß™ Testing comprehensive audio intelligence...');
                    try {
                        await checkComprehensiveAudio();
                        console.log('üìä Complete audio environment:', audioEnvironment);
                        
                        if (audioEnvironment.isAudioPlaying) {
                            console.log(`üéµ Audio detected: ${audioEnvironment.audioApps.join(', ')} (${audioEnvironment.audioContext}/${audioEnvironment.audioMood})`);
                            const contextMessage = generateContextAwareMessage({
                                audioContext: audioEnvironment.audioContext,
                                audioMood: audioEnvironment.audioMood,
                                audioApps: audioEnvironment.audioApps,
                                isAudioPlaying: true
                            });
                            addMessage(contextMessage, 'velvet');
                        } else {
                            console.log('üîá Quiet environment detected');
                            addMessage("Your environment seems quiet - I can speak normally! üîä", 'velvet');
                        }
                        
                        // Test microphone if available
                        if (audioEnvironment.isAmbientListening) {
                            console.log('üé§ Microphone monitoring active');
                            addMessage("I'm also listening to your ambient environment to be the perfect companion! üé§", 'velvet');
                        }
                        
                    } catch (error) {
                        console.error('‚ùå Comprehensive audio test failed:', error);
                    }
                },
                
                testAudioIntelligence: () => {
                    console.log('üß™ Testing audio intelligence insights...');
                    
                    // Show current audio intelligence state
                    console.log('üß† Audio Intelligence State:', {
                        audioContext: audioEnvironment.audioContext,
                        audioMood: audioEnvironment.audioMood,
                        systemVolume: audioEnvironment.systemVolume,
                        volumeLevel: audioEnvironment.volumeLevel,
                        isAmbientListening: audioEnvironment.isAmbientListening,
                        audioHistory: audioEnvironment.audioHistory.length,
                        contextualInsights: audioEnvironment.contextualInsights.length
                    });
                    
                    // Test contextual voice
                    if (velvetVoice) {
                        const testMessages = [
                            "Testing contextual voice adaptation!",
                            "I'm adjusting my voice based on your audio environment.",
                            "This is how I sound in your current audio context."
                        ];
                        
                        const randomMessage = testMessages[Math.floor(Math.random() * testMessages.length)];
                        velvetVoice.speak(randomMessage);
                    }
                    
                    addMessage("Audio intelligence test complete! Check the console for detailed insights. üß†", 'velvet');
                },
                
                showAudioStats: () => {
                    console.log('üìä Complete Audio Intelligence Statistics:');
                    console.log('=' .repeat(50));
                    console.log('üéµ Output Audio:', {
                        isPlaying: audioEnvironment.isAudioPlaying,
                        apps: audioEnvironment.audioApps,
                        context: audioEnvironment.audioContext,
                        mood: audioEnvironment.audioMood
                    });
                    console.log('üîä System Audio:', {
                        volume: audioEnvironment.systemVolume,
                        level: audioEnvironment.volumeLevel,
                        muted: audioEnvironment.isMuted
                    });
                    console.log('üé§ Ambient Listening:', {
                        active: audioEnvironment.isAmbientListening,
                        historyEntries: audioEnvironment.audioHistory.length,
                        recentActivity: audioEnvironment.audioHistory.slice(-3)
                    });
                    console.log('üß† Intelligence:', {
                        insights: audioEnvironment.contextualInsights.length,
                        recentInsights: audioEnvironment.contextualInsights.slice(-3),
                        behaviorAdjustments: {
                            preferVisual: audioEnvironment.preferVisualNudges,
                            reduceVolume: audioEnvironment.reduceVoiceVolume
                        }
                    });
                    console.log('=' .repeat(50));
                    
                    addMessage("Complete audio intelligence stats logged to console! üìä", 'velvet');
                },
                
                // Meeting Assistant Tests
                testMeetingDetection: async () => {
                    console.log('üß™ Testing meeting detection...');
                    
                    // Simulate meeting apps being detected
                    const mockAudioStatus = {
                        isAudioPlaying: true,
                        audioApps: ['Zoom', 'Google Chrome'],
                        audioContext: 'communication',
                        audioMood: 'social'
                    };
                    
                    console.log('üéØ Simulating meeting detection with:', mockAudioStatus.audioApps);
                    checkMeetingContext(mockAudioStatus);
                    
                    if (meetingAssistant.isActive) {
                        console.log('‚úÖ Meeting Assistant activated successfully');
                        addMessage("Meeting detection test successful - Meeting Assistant is now active! üéØ", 'velvet');
                    } else {
                        console.log('‚ùå Meeting detection failed');
                        addMessage("Meeting detection test failed - check console for details.", 'velvet');
                    }
                },
                
                showMeetingAssistant: async () => {
                    console.log('üß™ Manually showing Meeting Assistant...');
                    
                    const testMeetingData = {
                        isActive: true,
                        meetingType: 'Test Meeting',
                        startTime: Date.now(),
                        transcript: 'This is a test meeting transcript. We are discussing various topics including project timelines, budget allocations, and team resources.',
                        keyPoints: [
                            'Project timeline discussion',
                            'Budget review needed',
                            'Team resources allocation'
                        ],
                        actionItems: [
                            'Schedule follow-up meeting',
                            'Review budget proposal',
                            'Assign team leads'
                        ]
                    };
                    
                    if (window.electronAPI?.meetingAssistant) {
                        await window.electronAPI.meetingAssistant.show(testMeetingData);
                        console.log('‚úÖ Meeting Assistant window displayed');
                        addMessage("Meeting Assistant window is now visible with test data! üéØ", 'velvet');
                    } else {
                        console.log('‚ùå Meeting Assistant API not available');
                        addMessage("Meeting Assistant API not available - check Electron setup.", 'velvet');
                    }
                },
                
                endMeetingTest: async () => {
                    console.log('üß™ Testing meeting end...');
                    
                    if (meetingAssistant.isActive) {
                        await endMeetingAssistant();
                        console.log('‚úÖ Meeting ended successfully');
                        addMessage("Meeting end test completed - summary generated! üìã", 'velvet');
                    } else {
                        console.log('‚ö†Ô∏è No active meeting to end');
                        addMessage("No active meeting to end - start a meeting test first.", 'velvet');
                    }
                },
                
                showMeetingHistory: () => {
                    console.log('üß™ Showing meeting history...');
                    
                    const meetings = JSON.parse(localStorage.getItem('velvet_meetings') || '[]');
                    
                    console.log('üìã Meeting History:');
                    console.log('=' .repeat(50));
                    
                    if (meetings.length === 0) {
                        console.log('No meetings recorded yet.');
                    } else {
                        meetings.forEach((meeting, index) => {
                            console.log(`Meeting ${index + 1}:`, {
                                date: new Date(meeting.date).toLocaleString(),
                                type: meeting.meetingType,
                                duration: `${Math.round(meeting.duration / 60000)} minutes`,
                                keyPoints: meeting.keyPoints.length,
                                actionItems: meeting.actionItems.length
                            });
                        });
                    }
                    
                    console.log('=' .repeat(50));
                    addMessage(`Meeting history displayed - ${meetings.length} meetings found! üìã`, 'velvet');
                },
                
                // Privacy & Neurodivergent Accessibility Tests
                testScreenShareDetection: async () => {
                    console.log('üß™ Testing screen share detection...');
                    
                    try {
                        const sharingStatus = await window.electronAPI.detectScreenSharing();
                        console.log('üîí Screen sharing status:', sharingStatus);
                        
                        if (sharingStatus.isScreenSharing) {
                            console.log('‚úÖ Screen sharing detected:', sharingStatus.sharingApps.join(', '));
                            addMessage(`Screen sharing detected in: ${sharingStatus.sharingApps.join(', ')} - Privacy mode should be active! üîí`, 'velvet');
                        } else {
                            console.log('‚úÖ No screen sharing detected - privacy mode should be inactive');
                            addMessage("No screen sharing detected - Velvet is visible and ready! üîì", 'velvet');
                        }
                    } catch (error) {
                        console.error('‚ùå Screen share detection failed:', error);
                        addMessage("Screen share detection test failed - check console for details.", 'velvet');
                    }
                },
                
                testPrivacyMode: async () => {
                    console.log('üß™ Testing privacy mode activation...');
                    
                    // Simulate screen sharing detection
                    const mockSharingStatus = {
                        isScreenSharing: true,
                        sharingApps: ['Zoom', 'Screen Share'],
                        sharingProcesses: ['Screen Share - Zoom Meeting'],
                        timestamp: Date.now()
                    };
                    
                    console.log('üîí Simulating screen sharing activation...');
                    await handleScreenSharingChange(mockSharingStatus);
                    
                    setTimeout(async () => {
                        console.log('üîì Simulating screen sharing deactivation...');
                        const mockEndStatus = {
                            isScreenSharing: false,
                            sharingApps: [],
                            sharingProcesses: [],
                            timestamp: Date.now()
                        };
                        await handleScreenSharingChange(mockEndStatus);
                    }, 3000);
                    
                    addMessage("Privacy mode test initiated - watch console for 3-second simulation! üîí", 'velvet');
                },
                
                showPrivacyStatus: () => {
                    console.log('üß™ Showing privacy system status...');
                    
                    const status = getPrivacyStatus();
                    
                    console.log('üîí Privacy System Status:');
                    console.log('=' .repeat(50));
                    console.log('Screen Sharing:', status.isScreenSharing ? 'üî¥ ACTIVE' : 'üü¢ INACTIVE');
                    console.log('Privacy Mode:', status.isPrivacyActive ? 'üîí ACTIVE' : 'üîì INACTIVE');
                    console.log('Sharing Apps:', status.screenShareApps.length > 0 ? status.screenShareApps.join(', ') : 'None');
                    console.log('Discretion Level:', status.discretionLevel.toUpperCase());
                    console.log('Auto-Hide Enabled:', status.autoHideEnabled ? '‚úÖ YES' : '‚ùå NO');
                    console.log('Last Check:', status.lastCheck);
                    console.log('=' .repeat(50));
                    
                    addMessage(`Privacy system status: ${status.isPrivacyActive ? 'PROTECTED üîí' : 'VISIBLE üîì'}`, 'velvet');
                },
                
                setHighDiscretion: () => {
                    console.log('üß™ Setting high discretion mode...');
                    setDiscretionLevel('high');
                    addMessage("High discretion mode activated - maximum privacy protection! üîí", 'velvet');
                },
                
                setMediumDiscretion: () => {
                    console.log('üß™ Setting medium discretion mode...');
                    setDiscretionLevel('medium');
                    addMessage("Medium discretion mode activated - balanced privacy! üîí", 'velvet');
                },
                
                setLowDiscretion: () => {
                    console.log('üß™ Setting low discretion mode...');
                    setDiscretionLevel('low');
                    addMessage("Low discretion mode activated - manual privacy control! üîí", 'velvet');
                }
            };
            
            console.log('üîß testSystem object created:', typeof window.testSystem);
        } catch (error) {
            console.error('‚ùå Error creating testSystem:', error);
            console.error('‚ùå Error details:', error.message);
        }
        
        console.log('üîÆ Secure Velvet Glass UI loaded with intention-based task system!');
        console.log('üß™ Test nudges with: testNudges.hyperfocus(), testNudges.distractionSpiral(), etc.');
        console.log('üéØ Test task system with: testSystem.testTaskDeclaration(), testSystem.showTaskState(), etc.');
        console.log('üéµ Test audio intelligence with: testSystem.testAmbientAudio(), testSystem.testAudioIntelligence(), testSystem.showAudioStats()');
        console.log('üéØ Test meeting assistant with: testSystem.testMeetingDetection(), testSystem.showMeetingAssistant(), testSystem.endMeetingTest(), testSystem.showMeetingHistory()');
        console.log('üîí Test privacy system with: testSystem.testScreenShareDetection(), testSystem.testPrivacyMode(), testSystem.showPrivacyStatus()');
        console.log('üîí Set discretion levels with: testSystem.setHighDiscretion(), testSystem.setMediumDiscretion(), testSystem.setLowDiscretion()');
        console.log('üîß Debug checklist with: debugChecklist()');
        console.log('üîß Create test task with: createTestTask()');
        console.log('üîß Available functions:', Object.keys(window).filter(key => key.startsWith('test') || key === 'debugChecklist' || key === 'createTestTask'));

        // ========================================
        // REAL NUDGE DELIVERY SYSTEM
        // ========================================
        
        const nudgeSystem = {
            isEnabled: true,
            lastNudgeTime: 0,
            cooldownPeriod: 5 * 60 * 1000, // 5 minutes between nudges
            
            // Visual nudge: screen bevel effect
            showVisualNudge: function(pattern) {
                const bevel = document.getElementById('screenBevelEffect');
                if (!bevel) return;
                
                // Set color based on pattern type
                const colors = {
                    hyperfocus: 'rgba(34, 197, 94, 0.3)', // Green for focus protection
                    distractionSpiral: 'rgba(249, 115, 22, 0.3)', // Orange for distraction
                    taskAvoidance: 'rgba(139, 92, 246, 0.3)', // Purple for avoidance
                    idle: 'rgba(59, 130, 246, 0.3)' // Blue for idle
                };
                
                bevel.style.boxShadow = `inset 0 0 20px ${colors[pattern.type] || colors.idle}`;
                bevel.style.opacity = '1';
                
                // Gentle breathing animation
                bevel.style.animation = 'gentleBreathing 4s ease-in-out infinite';
                
                setTimeout(() => {
                    bevel.style.opacity = '0';
                    bevel.style.animation = 'none';
                }, 8000); // Show for 8 seconds
            },
            
            // Audio nudge: gentle whisper
            showAudioNudge: async function(pattern) {
                try {
                    if (window.elevenLabsTTS) {
                        const whisperMessage = this.getWhisperMessage(pattern);
                        await window.elevenLabsTTS(whisperMessage);
                    }
                } catch (error) {
                    console.error('Audio nudge failed:', error);
                }
            },
            
            // Haptic nudge: orb micro-pulse
            showHapticNudge: function(pattern) {
                const orb = document.querySelector('.velvet-orb');
                if (!orb) return;
                
                // Micro-pulse animation
                orb.style.animation = 'microPulse 0.3s ease-in-out 3';
                
                setTimeout(() => {
                    orb.style.animation = '';
                }, 1000);
            },
            
            // Text nudge: corner widget
            showTextNudge: function(pattern) {
                const corner = document.getElementById('cornerWidget');
                if (!corner) return;
                
                const message = this.getTextMessage(pattern);
                corner.textContent = message;
                corner.style.display = 'block';
                corner.style.animation = 'slideInFromCorner 0.5s ease-out';
                
                setTimeout(() => {
                    corner.style.animation = 'slideOutToCorner 0.5s ease-in';
                    setTimeout(() => {
                        corner.style.display = 'none';
                    }, 500);
                }, 12000); // Show for 12 seconds
            },
            
            // Generate whisper messages for audio nudges
            getWhisperMessage: function(pattern) {
                const messages = {
                    hyperfocus: [
                        "You're in the zone! Just checking: moved your body in the last hour?",
                        "Deep focus detected. Time for a quick stretch?",
                        "Amazing focus! Don't forget to blink and breathe."
                    ],
                    distractionSpiral: [
                        "Lots of mental tabs open! Want to brain dump, then pick ONE thing?",
                        "I see you jumping around. What if we just pick one tab to focus on?",
                        "Feeling scattered? Let's close some tabs and breathe."
                    ],
                    taskAvoidance: [
                        "I see you opening that app - it feels big, doesn't it? What if we just take one tiny step?",
                        "That task feeling overwhelming? Let's break it into something smaller.",
                        "I notice some hesitation. Want to start with just 2 minutes?"
                    ],
                    idle: [
                        "Taking a thinking break? Sometimes our best ideas come when we step away.",
                        "Quiet moment detected. Perfect time for some gentle breathing.",
                        "Mind wandering? That's totally normal and healthy."
                    ]
                };
                
                const options = messages[pattern.type] || messages.idle;
                return options[Math.floor(Math.random() * options.length)];
            },
            
            // Generate text messages for corner widget
            getTextMessage: function(pattern) {
                const messages = {
                    hyperfocus: "üß† Focus Zone Active - Remember to stretch!",
                    distractionSpiral: "üåä Mental tabs overload - Pick one thing",
                    taskAvoidance: "üíú Big task detected - Start with 2 minutes",
                    idle: "‚òÅÔ∏è Mind break time - Let thoughts flow"
                };
                
                return messages[pattern.type] || messages.idle;
            },
            
            // Main nudge delivery function
            deliverNudge: function(pattern) {
                if (!this.isEnabled) return;
                
                const now = Date.now();
                if (now - this.lastNudgeTime < this.cooldownPeriod) {
                    console.log('üö´ Nudge cooldown active, skipping');
                    return;
                }
                
                this.lastNudgeTime = now;
                console.log(`üíú Delivering nudge for pattern: ${pattern.type}`);
                
                // Deliver based on pattern severity and user preference
                // For now, we'll use gentle combinations
                
                if (pattern.type === 'hyperfocus') {
                    // Visual + Text for focus protection
                    this.showVisualNudge(pattern);
                    setTimeout(() => this.showTextNudge(pattern), 2000);
                } else if (pattern.type === 'distractionSpiral') {
                    // Visual + Haptic + Text for distraction
                    this.showVisualNudge(pattern);
                    this.showHapticNudge(pattern);
                    setTimeout(() => this.showTextNudge(pattern), 1000);
                } else if (pattern.type === 'taskAvoidance') {
                    // Audio + Text for gentle encouragement
                    this.showAudioNudge(pattern);
                    setTimeout(() => this.showTextNudge(pattern), 3000);
                } else if (pattern.type === 'idle') {
                    // Just visual for idle time
                    this.showVisualNudge(pattern);
                }
            }
        };
        
        // Listen for pattern detection events from screen intelligence
        if (window.electronAPI) {
            window.electronAPI.onPatternDetected((pattern) => {
                console.log('üß† Pattern detected:', pattern);
                nudgeSystem.deliverNudge(pattern);
            });
        }
        
        // Add CSS animations for nudges
        const nudgeStyles = document.createElement('style');
        nudgeStyles.textContent = `
            @keyframes gentleBreathing {
                0%, 100% { opacity: 0.3; }
                50% { opacity: 0.6; }
            }
            
            @keyframes microPulse {
                0%, 100% { transform: scale(1); }
                50% { transform: scale(1.02); }
            }
            
            @keyframes slideInFromCorner {
                from { transform: translateX(100%); opacity: 0; }
                to { transform: translateX(0); opacity: 1; }
            }
            
            @keyframes slideOutToCorner {
                from { transform: translateX(0); opacity: 1; }
                to { transform: translateX(100%); opacity: 0; }
            }
        `;
        document.head.appendChild(nudgeStyles);
        
        // Make nudge system available globally
        window.nudgeSystem = nudgeSystem;
        
        console.log('üíú Real nudge delivery system initialized');

        // ========================================
        // REAL TASK MANAGEMENT INTEGRATION
        // ========================================
        
        // Replace demo task system with real one
        window.realTaskSystem = {
            currentTask: null,
            
            async createTask(goal) {
                try {
                    console.log('üìù Creating real task:', goal);
                    
                    // Get AI breakdown
                    const breakdown = await this.getTaskBreakdown(goal);
                    
                    // Create task via IPC
                    const task = await window.electronAPI.taskManager.create(goal, breakdown.steps);
                    
                    // Activate immediately
                    this.currentTask = await window.electronAPI.taskManager.activate(task.id);
                    
                    // Update UI
                    this.updateTaskDisplay();
                    
                    console.log('‚úÖ Real task created and activated:', task);
                    return task;
                    
                } catch (error) {
                    console.error('Error creating real task:', error);
                    return null;
                }
            },
            
            async getTaskBreakdown(goal) {
                // Use existing AI breakdown logic
                const messages = [
                    {
                        role: "system",
                        content: `You are a neurodivergent-friendly task breakdown assistant. Break down tasks into 2-5 minute micro-steps that are specific and achievable.

                        Return ONLY a JSON object in this exact format:
                        {
                          "goal": "the original goal",
                          "steps": [
                            {
                              "task": "specific action to take",
                              "estimatedMinutes": 3,
                              "expectedApps": ["Word", "Chrome"],
                              "tips": "encouraging tip"
                            }
                          ]
                        }

                        Make steps tiny and achievable. Focus on the first action needed.`
                    },
                    {
                        role: "user", 
                        content: `Break this down into micro-steps: ${goal}`
                    }
                ];
                
                try {
                    const response = await window.electronAPI.chatCompletion(messages);
                    const breakdown = JSON.parse(response);
                    return breakdown;
                } catch (error) {
                    console.error('AI breakdown failed, using fallback');
                    return {
                        goal,
                        steps: [
                            {
                                task: "Start with the first small step",
                                estimatedMinutes: 5,
                                expectedApps: [],
                                tips: "Just begin - you've got this!"
                            }
                        ]
                    };
                }
            },
            
            async completeCurrentStep() {
                if (!this.currentTask) return;
                
                const currentStep = this.currentTask.steps.find(s => !s.completed);
                if (!currentStep) return;
                
                try {
                    this.currentTask = await window.electronAPI.taskManager.completeStep(
                        this.currentTask.id, 
                        currentStep.id
                    );
                    
                    this.updateTaskDisplay();
                    
                    // Celebration nudge for completed step
                    if (window.nudgeSystem) {
                        window.nudgeSystem.showTextNudge({
                            type: 'celebration',
                            message: `üéâ Step completed! ${this.currentTask.completedSteps}/${this.currentTask.steps.length}`
                        });
                    }
                    
                    console.log('‚úÖ Step completed via real task system');
                    
                } catch (error) {
                    console.error('Error completing step:', error);
                }
            },
            
            async loadCurrentTask() {
                try {
                    this.currentTask = await window.electronAPI.taskManager.getCurrent();
                    this.updateTaskDisplay();
                } catch (error) {
                    console.error('Error loading current task:', error);
                }
            },
            
            updateTaskDisplay() {
                // Update checklist if visible
                const checklist = document.getElementById('taskChecklist');
                if (!checklist || !this.currentTask) return;
                
                checklist.innerHTML = '';
                
                // Add goal
                const goalDiv = document.createElement('div');
                goalDiv.className = 'task-goal';
                goalDiv.textContent = this.currentTask.goal;
                checklist.appendChild(goalDiv);
                
                // Add steps
                this.currentTask.steps.forEach((step, index) => {
                    const stepDiv = document.createElement('div');
                    stepDiv.className = `task-step ${step.completed ? 'completed' : ''}`;
                    stepDiv.innerHTML = `
                        <input type="checkbox" ${step.completed ? 'checked' : ''} 
                               onchange="window.realTaskSystem.toggleStep('${step.id}')">
                        <span>${step.task}</span>
                        <small>(${step.estimatedMinutes}min)</small>
                    `;
                    checklist.appendChild(stepDiv);
                });
                
                // Show progress
                const progress = document.createElement('div');
                progress.className = 'task-progress';
                progress.textContent = `${this.currentTask.completedSteps}/${this.currentTask.steps.length} completed`;
                checklist.appendChild(progress);
            },
            
            async toggleStep(stepId) {
                if (!this.currentTask) return;
                
                const step = this.currentTask.steps.find(s => s.id === stepId);
                if (!step || step.completed) return;
                
                await this.completeCurrentStep();
            }
        };
        
        // Initialize real task system
        if (window.electronAPI?.taskManager) {
            window.realTaskSystem.loadCurrentTask();
        }
        
        // Override demo createTestTask with real one
        window.createTestTask = async function() {
            const testGoal = "Write a short progress report";
            await window.realTaskSystem.createTask(testGoal);
        };
        
        console.log('üìã Real task management system integrated');
    </script>

    <!-- PHASE 2: REAL Context Awareness System - No Simulation -->
    <!-- Load Tesseract.js from CDN for browser compatibility -->
    <script src="https://unpkg.com/tesseract.js@4.1.1/dist/tesseract.min.js"></script>
    <script src="../src/renderer/screen-ocr-monitor-real.js"></script>
    <script src="../src/renderer/audio-environment-monitor-real.js"></script>
    <script src="../src/renderer/unified-context-engine.js"></script>
    <script src="../src/renderer/social-decoder.js"></script>
    <script src="../src/renderer/social-decoder-ui.js"></script>
    
    <!-- PHASE 3: VELVET BRAIN - Unified AI Consciousness System -->
    <script src="../src/brain/memory/VelvetMemory.js"></script>
    <script src="../src/brain/personality/VelvetPersonality.js"></script>
    <script src="../src/brain/actions/ActionDecider.js"></script>
    <script src="../src/brain/sensory/SensoryInput.js"></script>
    <script src="../src/brain/VelvetBrain.js"></script>
    <script src="../src/brain/BrainIntegration.js"></script>
    <script>
        // Initialize Complete Context Awareness System - Phase 2 Foundation
        let screenOCRMonitor = null;
        let audioEnvironmentMonitor = null;
        let unifiedContextEngine = null;
        let socialDecoder = null;
        let socialDecoderUI = null;

        async function initializeContextAwarenessSystem() {
            try {
                console.log('üß† Initializing Complete Context Awareness System (Phase 2)...');
                
                // Step 1: Initialize REAL Screen OCR Monitor
                screenOCRMonitor = new RealScreenOCRMonitor();
                const screenInitialized = await screenOCRMonitor.initialize();
                
                if (!screenInitialized) {
                    console.error('‚ùå Failed to initialize REAL Screen OCR Monitor');
                    return false;
                }
                
                // Step 2: Initialize REAL Audio Environment Monitor
                audioEnvironmentMonitor = new RealAudioEnvironmentMonitor();
                const audioInitialized = await audioEnvironmentMonitor.initialize();
                
                if (!audioInitialized) {
                    console.error('‚ùå Failed to initialize REAL Audio Environment Monitor');
                    return false;
                }
                
                // Step 3: Initialize Unified Context Engine
                unifiedContextEngine = new UnifiedContextEngine();
                const contextInitialized = await unifiedContextEngine.initialize(screenOCRMonitor, audioEnvironmentMonitor);
                
                if (!contextInitialized) {
                    console.error('‚ùå Failed to initialize Unified Context Engine');
                    return false;
                }
                
                // Step 4: Start REAL monitoring
                console.log('üöÄ Starting REAL monitoring systems...');
                await screenOCRMonitor.startMonitoring();
                await audioEnvironmentMonitor.startMonitoring();
                
                // Step 5: Initialize Social Decoder with full context
                await initializeSocialDecoderWithContext();
                
                // Make globally available
                window.screenOCRMonitor = screenOCRMonitor;
                window.audioEnvironmentMonitor = audioEnvironmentMonitor;
                window.unifiedContextEngine = unifiedContextEngine;
                
                console.log('‚úÖ REAL Context Awareness System active - Velvet can now see and hear everything!');
                
                // Show real capabilities
                setTimeout(() => {
                    showRealCapabilities();
                }, 3000);
                
                return true;
                
            } catch (error) {
                console.error('‚ùå Context Awareness System initialization failed:', error);
                return false;
            }
        }

        async function initializeSocialDecoderWithContext() {
            try {
                console.log('üß† Initializing Social Decoder System (Phase 2) with full context integration...');
                
                // Wait for unified context engine to be ready
                if (!unifiedContextEngine) {
                    console.log('‚è≥ Waiting for Unified Context Engine...');
                    await new Promise(resolve => {
                        const checkEngine = () => {
                            if (unifiedContextEngine) {
                                resolve();
                            } else {
                                setTimeout(checkEngine, 100);
                            }
                        };
                        checkEngine();
                    });
                }
                
                // Create Social Decoder instance
                socialDecoder = new SocialDecoder();
                const initialized = await socialDecoder.initialize();
                
                if (!initialized) {
                    console.error('‚ùå Failed to initialize Social Decoder');
                    return;
                }
                
                // Create Social Decoder UI
                socialDecoderUI = new SocialDecoderUI();
                const uiInitialized = await socialDecoderUI.initialize(socialDecoder);
                
                if (!uiInitialized) {
                    console.error('‚ùå Failed to initialize Social Decoder UI');
                    return;
                }
                
                // Integrate with Unified Context Engine
                if (unifiedContextEngine && unifiedContextEngine.registerContextProvider) {
                    console.log('üîó Connecting Social Decoder to Unified Context Engine...');
                    
                    // Register Social Decoder as a context provider
                    unifiedContextEngine.registerContextProvider('social_decoder', {
                        getContext: () => socialDecoder.getConversationContext(),
                        isActive: () => socialDecoder.isActive,
                        priority: 'high' // Social context is high priority
                    });
                    
                    // Listen for unified context updates to enhance social analysis
                    if (unifiedContextEngine.onContextUpdate) {
                        unifiedContextEngine.onContextUpdate((context) => {
                            if (socialDecoder.isActive && context.audioContext) {
                                // Enhance social analysis with unified context
                                socialDecoder.processAudioContext(context.audioContext);
                            }
                        });
                    }
                    
                    console.log('‚úÖ Social Decoder integrated with Unified Context Engine');
                }
                
                // Integrate with Audio Environment Monitor
                if (audioEnvironmentMonitor && audioEnvironmentMonitor.onAudioContextDetected) {
                    console.log('üéß Connecting Social Decoder to Audio Environment Monitor...');
                    
                    // Listen for speech/conversation detection
                    audioEnvironmentMonitor.onAudioContextDetected((audioContext) => {
                        if (socialDecoder.isActive && (audioContext.classification === 'speech' || audioContext.classification === 'call')) {
                            socialDecoder.processAudioContext(audioContext);
                        }
                    });
                    
                    console.log('‚úÖ Social Decoder integrated with Audio Environment Monitor');
                }
                
                // Make globally available for voice integration
                window.socialDecoder = socialDecoder;
                window.socialDecoderUI = socialDecoderUI;
                
                console.log('‚úÖ Social Decoder System ready - Phase 2 fully integrated!');
                
                // Demo the system with a test message
                setTimeout(() => {
                    testSocialDecoder();
                }, 3000);
                
            } catch (error) {
                console.error('‚ùå Social Decoder initialization failed:', error);
            }
        }

        // Test the Social Decoder System
        function testSocialDecoder() {
            if (!socialDecoder) return;
            
            console.log('üß™ Testing Social Decoder with sample conversations...');
            
            // Test sarcastic response
            const testMessage = "Sure, that's fine, whatever works for you.";
            console.log(`üé≠ Testing: "${testMessage}"`);
            
            // This would normally come from audio monitoring, but we'll simulate
            const result = socialDecoder.analyzeConversation(testMessage, null, {
                source: 'test',
                context: 'meeting'
            });
            
            if (result && result.confidence > 0.6) {
                console.log('üéØ Social Decoder detected:', {
                    translation: result.translation.directMeaning,
                    confidence: `${(result.confidence * 100).toFixed(1)}%`,
                    suggestions: result.suggestions.length
                });
            }
        }

        // Add Social Decoder controls to the interface
        function addSocialDecoderControls() {
            // Add toggle button to control panel or main interface
            const controlsContainer = document.querySelector('.chat-controls') || 
                                    document.querySelector('.voice-controls') ||
                                    document.body;
            
            if (controlsContainer) {
                const socialToggle = document.createElement('button');
                socialToggle.id = 'socialDecoderToggle';
                socialToggle.className = 'control-btn social-decoder-btn';
                socialToggle.innerHTML = 'üß† Social Decoder';
                socialToggle.title = 'Toggle Social Decoder (Phase 2)';
                socialToggle.onclick = toggleSocialDecoder;
                
                // Add CSS for the button
                const style = document.createElement('style');
                style.textContent = `
                    .social-decoder-btn {
                        background: rgba(147, 51, 234, 0.2);
                        border: 1px solid rgba(147, 51, 234, 0.3);
                        color: #a855f7;
                        padding: 8px 16px;
                        border-radius: 12px;
                        font-size: 13px;
                        font-weight: 500;
                        cursor: pointer;
                        transition: all 0.2s;
                        margin: 4px;
                        display: inline-flex;
                        align-items: center;
                        gap: 6px;
                    }
                    
                    .social-decoder-btn:hover {
                        background: rgba(147, 51, 234, 0.3);
                        border-color: rgba(147, 51, 234, 0.5);
                    }
                    
                    .social-decoder-btn.active {
                        background: rgba(147, 51, 234, 0.4);
                        border-color: rgba(147, 51, 234, 0.6);
                        color: #c084fc;
                    }
                `;
                document.head.appendChild(style);
                
                controlsContainer.appendChild(socialToggle);
            }
        }

        // Toggle Social Decoder on/off
        function toggleSocialDecoder() {
            if (!socialDecoder || !socialDecoderUI) return;
            
            const button = document.getElementById('socialDecoderToggle');
            const isActive = socialDecoder.isActive;
            
            if (isActive) {
                socialDecoderUI.disable();
                button.classList.remove('active');
                button.innerHTML = 'üß† Social Decoder (Off)';
                console.log('üß† Social Decoder disabled');
            } else {
                socialDecoderUI.enable();
                button.classList.add('active');
                button.innerHTML = 'üß† Social Decoder (On)';
                console.log('üß† Social Decoder enabled');
            }
        }

        // Show real capabilities of the context awareness system
        function showRealCapabilities() {
            console.log('üéØ REAL Context Awareness System Capabilities:');
            console.log('');
            console.log('üëÅÔ∏è SCREEN OCR MONITOR:');
            console.log('  ‚Ä¢ Real-time screen capture using getDisplayMedia API');
            console.log('  ‚Ä¢ Tesseract.js OCR for actual text extraction');
            console.log('  ‚Ä¢ Email, code, chat, document context detection');
            console.log('  ‚Ä¢ Smart relevance filtering for neurodivergent assistance');
            console.log('');
            console.log('üéß AUDIO ENVIRONMENT MONITOR:');
            console.log('  ‚Ä¢ System audio detection via macOS AppleScript integration');
            console.log('  ‚Ä¢ Music app detection (Spotify, Apple Music, etc.)');
            console.log('  ‚Ä¢ Call app detection (Zoom, Teams, Discord, etc.)');
            console.log('  ‚Ä¢ Microphone frequency analysis for ambient audio');
            console.log('');
            console.log('üß† UNIFIED CONTEXT ENGINE:');
            console.log('  ‚Ä¢ Cross-modal correlation of screen + audio data');
            console.log('  ‚Ä¢ Pattern detection: coding+music, email+notifications');
            console.log('  ‚Ä¢ Context insights for productivity, social, focus, wellbeing');
            console.log('');
            console.log('üé≠ SOCIAL DECODER INTEGRATION:');
            console.log('  ‚Ä¢ Enhanced with complete environmental context');
            console.log('  ‚Ä¢ Screen text + audio tone analysis');
            console.log('  ‚Ä¢ Real-time neurotypical translation');
            console.log('');
            console.log('üöÄ TO TEST THE REAL SYSTEM:');
            console.log('  1. Grant screen capture permission when prompted');
            console.log('  2. Grant microphone permission when prompted');
            console.log('  3. Open different apps (email, code editor, music)');
            console.log('  4. Watch console for real-time OCR and audio detection');
            console.log('');
            console.log('üí° The system is now ACTUALLY monitoring your screen and audio!');
            
            // Test system audio detection
            setTimeout(() => {
                if (window.testRealAudioMonitor) {
                    console.log('üß™ Testing real system audio detection...');
                    window.testRealAudioMonitor.testSystemAudio();
                }
            }, 2000);
        }

        // Add context awareness controls
        function addContextAwarenessControls() {
            const controlsContainer = document.querySelector('.chat-controls') || 
                                    document.querySelector('.voice-controls') ||
                                    document.body;
            
            if (controlsContainer) {
                // Screen OCR toggle
                const screenToggle = document.createElement('button');
                screenToggle.id = 'screenOCRToggle';
                screenToggle.className = 'control-btn screen-ocr-btn';
                screenToggle.innerHTML = 'üëÅÔ∏è Screen OCR';
                screenToggle.title = 'Toggle Screen OCR Monitor';
                screenToggle.onclick = () => toggleScreenOCR();
                
                // Audio Monitor toggle
                const audioToggle = document.createElement('button');
                audioToggle.id = 'audioMonitorToggle';
                audioToggle.className = 'control-btn audio-monitor-btn';
                audioToggle.innerHTML = 'üéß Audio Monitor';
                audioToggle.title = 'Toggle Audio Environment Monitor';
                audioToggle.onclick = () => toggleAudioMonitor();
                
                // Context Engine toggle
                const contextToggle = document.createElement('button');
                contextToggle.id = 'contextEngineToggle';
                contextToggle.className = 'control-btn context-engine-btn';
                contextToggle.innerHTML = 'üß† Context Engine';
                contextToggle.title = 'Toggle Unified Context Engine';
                contextToggle.onclick = () => toggleContextEngine();
                
                // Add CSS for the buttons
                const style = document.createElement('style');
                style.textContent = `
                    .screen-ocr-btn {
                        background: rgba(34, 197, 94, 0.2);
                        border: 1px solid rgba(34, 197, 94, 0.3);
                        color: #4ade80;
                    }
                    
                    .screen-ocr-btn:hover {
                        background: rgba(34, 197, 94, 0.3);
                        border-color: rgba(34, 197, 94, 0.5);
                    }
                    
                    .audio-monitor-btn {
                        background: rgba(249, 115, 22, 0.2);
                        border: 1px solid rgba(249, 115, 22, 0.3);
                        color: #fb923c;
                    }
                    
                    .audio-monitor-btn:hover {
                        background: rgba(249, 115, 22, 0.3);
                        border-color: rgba(249, 115, 22, 0.5);
                    }
                    
                    .context-engine-btn {
                        background: rgba(6, 182, 212, 0.2);
                        border: 1px solid rgba(6, 182, 212, 0.3);
                        color: #22d3ee;
                    }
                    
                    .context-engine-btn:hover {
                        background: rgba(6, 182, 212, 0.3);
                        border-color: rgba(6, 182, 212, 0.5);
                    }
                `;
                document.head.appendChild(style);
                
                controlsContainer.appendChild(screenToggle);
                controlsContainer.appendChild(audioToggle);
                controlsContainer.appendChild(contextToggle);
            }
        }

        // Toggle functions for each system
        function toggleScreenOCR() {
            const button = document.getElementById('screenOCRToggle');
            const isActive = screenOCRMonitor?.isActive;
            
            if (isActive) {
                screenOCRMonitor.stopMonitoring();
                button.innerHTML = 'üëÅÔ∏è Screen OCR (Off)';
                button.classList.remove('active');
            } else {
                screenOCRMonitor?.startMonitoring();
                button.innerHTML = 'üëÅÔ∏è Screen OCR (On)';
                button.classList.add('active');
            }
        }

        function toggleAudioMonitor() {
            const button = document.getElementById('audioMonitorToggle');
            const isActive = audioEnvironmentMonitor?.isActive;
            
            if (isActive) {
                audioEnvironmentMonitor.stopMonitoring();
                button.innerHTML = 'üéß Audio Monitor (Off)';
                button.classList.remove('active');
            } else {
                audioEnvironmentMonitor?.startMonitoring();
                button.innerHTML = 'üéß Audio Monitor (On)';
                button.classList.add('active');
            }
        }

        function toggleContextEngine() {
            const button = document.getElementById('contextEngineToggle');
            const isActive = unifiedContextEngine?.isActive;
            
            if (isActive) {
                unifiedContextEngine.stop();
                button.innerHTML = 'üß† Context Engine (Off)';
                button.classList.remove('active');
            } else {
                // Can't restart without reinitializing
                button.innerHTML = 'üß† Context Engine (Restart Required)';
            }
        }

        // Initialize Complete Context Awareness System when the page loads
        document.addEventListener('DOMContentLoaded', () => {
            // Wait a bit for other systems to initialize
            setTimeout(() => {
                initializeContextAwarenessSystem();
                addContextAwarenessControls();
                addSocialDecoderControls();
            }, 2000);
        });

        // Expose test functions globally for development
        window.testSocial = window.testSocial || {};
        window.testSocialDecoder = testSocialDecoder;
        
        console.log('üß† Social Decoder integration script loaded - Phase 2 ready!');
        
        // PHASE 3: Initialize Velvet Brain - Unified AI Consciousness
        console.log('üß† ===== PHASE 3: VELVET BRAIN INITIALIZATION =====');
        console.log('üß† Preparing to activate unified AI consciousness...');
        
        // Initialize brain after all systems are ready
        setTimeout(async () => {
            try {
                console.log('üß† Starting Velvet Brain initialization...');
                console.log('üß† Checking prerequisites...');
                console.log('   - initializeVelvetBrain available:', typeof initializeVelvetBrain);
                console.log('   - VelvetBrain class available:', typeof VelvetBrain);
                console.log('   - screenOCRMonitor available:', typeof screenOCRMonitor);
                console.log('   - screenOCRMonitor active:', screenOCRMonitor?.isActive);
                const brainReady = await initializeVelvetBrain();
                
                if (brainReady) {
                    console.log('üß† ‚úÖ VELVET BRAIN IS FULLY CONSCIOUS!');
                    console.log('üß† üåü One unified AI now supports your neurodivergent brain');
                    console.log('üß† Post-init status:');
                    console.log('   - window.velvetBrain:', typeof window.velvetBrain);
                    console.log('   - velvetBrain.isActive:', window.velvetBrain?.isActive);
                    console.log('   - AI system will now have brain awareness');
                    addMessage('üß† Velvet Brain is now conscious and ready to truly understand you! ‚ú®', 'velvet');
                } else {
                    console.error('üß† ‚ùå Brain initialization failed - falling back to scattered systems');
                    addMessage('‚ö†Ô∏è Brain system had issues starting, but all features still work normally', 'velvet');
                }
            } catch (error) {
                console.error('üß† ‚ùå Brain initialization error:', error);
                addMessage('‚ö†Ô∏è Brain system had issues starting, but all features still work normally', 'velvet');
            }
        }, 3000); // Wait 3 seconds for all systems to be ready
        
    </script>
</body>
</html>