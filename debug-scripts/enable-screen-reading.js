/**
 * ENABLE SCREEN READING - Paste in Console
 * This will make Velvet actually read your screen content
 */

console.log('ğŸ‘ï¸ ENABLING VELVET SCREEN READING');
console.log('==================================');

async function enableScreenReading() {
    try {
        console.log('ğŸ”§ Setting up screen reading system...');
        
        // Check if we have the necessary APIs
        if (!window.electronAPI?.desktopCapturer) {
            console.error('âŒ Desktop capturer API not available');
            return false;
        }
        
        // Load Tesseract for OCR if not already loaded
        if (typeof Tesseract === 'undefined') {
            console.log('ğŸ“¥ Loading Tesseract.js for OCR...');
            const script = document.createElement('script');
            script.src = 'https://unpkg.com/tesseract.js@v2.1.0/dist/tesseract.min.js';
            document.head.appendChild(script);
            
            await new Promise((resolve, reject) => {
                script.onload = resolve;
                script.onerror = reject;
                setTimeout(reject, 10000); // 10 second timeout
            });
            
            console.log('âœ… Tesseract.js loaded successfully');
        }
        
        // Initialize OCR worker
        console.log('ğŸ”§ Initializing OCR worker...');
        const worker = Tesseract.createWorker();
        await worker.load();
        await worker.loadLanguage('eng');
        await worker.initialize('eng');
        console.log('âœ… OCR worker ready');
        
        // Create screen reading function
        async function captureAndReadScreen() {
            try {
                console.log('ğŸ“¸ Capturing screen for reading...');
                
                // Get screen sources
                const sourcesResult = await window.electronAPI.desktopCapturer.getSources({ types: ['screen'] });
                if (!sourcesResult.success || sourcesResult.sources.length === 0) {
                    console.error('âŒ No screen sources available');
                    return null;
                }
                
                // Use the first (primary) screen
                const primarySource = sourcesResult.sources[0];
                console.log('ğŸ“º Using screen:', primarySource.name);
                
                // Capture screen
                const captureResult = await window.electronAPI.desktopCapturer.captureScreenForOCR(primarySource.id);
                if (!captureResult.imageDataUrl) {
                    console.error('âŒ Screen capture failed');
                    return null;
                }
                
                console.log('ğŸ“· Screen captured, processing with OCR...');
                
                // Process with OCR
                const { data: { text, confidence } } = await worker.recognize(captureResult.imageDataUrl);
                
                // Clean up the text
                const cleanText = text.replace(/\s+/g, ' ').trim();
                const wordCount = cleanText.split(' ').length;
                
                console.log(`âœ… Screen read successfully!`);
                console.log(`ğŸ“Š OCR Confidence: ${Math.round(confidence)}%`);
                console.log(`ğŸ“ Extracted ${cleanText.length} characters, ${wordCount} words`);
                
                if (cleanText.length > 100) {
                    console.log(`ğŸ“„ Screen content preview: "${cleanText.substring(0, 150)}..."`);
                } else if (cleanText.length > 0) {
                    console.log(`ğŸ“„ Screen content: "${cleanText}"`);
                } else {
                    console.log('ğŸ“„ No readable text found on screen');
                }
                
                // Store in global for Velvet AI to access
                window.currentScreenText = cleanText;
                window.lastScreenCapture = {
                    text: cleanText,
                    confidence: confidence,
                    timestamp: Date.now(),
                    wordCount: wordCount
                };
                
                // Notify Velvet brain if available
                if (window.velvetBrain && typeof window.velvetBrain.updateScreenContext === 'function') {
                    window.velvetBrain.updateScreenContext({
                        text: cleanText,
                        confidence: confidence,
                        timestamp: Date.now()
                    });
                    console.log('ğŸ§  Updated Velvet brain with screen context');
                }
                
                return {
                    text: cleanText,
                    confidence: confidence,
                    timestamp: Date.now()
                };
                
            } catch (error) {
                console.error('âŒ Screen capture/OCR failed:', error);
                return null;
            }
        }
        
        // Start periodic screen reading
        console.log('â° Starting periodic screen reading (every 10 seconds)...');
        
        // Initial capture
        await captureAndReadScreen();
        
        // Set up interval
        const screenReadingInterval = setInterval(async () => {
            await captureAndReadScreen();
        }, 10000); // Every 10 seconds
        
        // Store controls globally
        window.stopScreenReading = () => {
            clearInterval(screenReadingInterval);
            worker.terminate();
            console.log('ğŸ›‘ Screen reading stopped');
        };
        
        window.readScreenNow = () => {
            return captureAndReadScreen();
        };
        
        window.getLastScreenText = () => {
            return window.lastScreenCapture;
        };
        
        console.log('âœ… SCREEN READING IS NOW ACTIVE!');
        console.log('ğŸ“– Velvet will read your screen every 10 seconds');
        console.log('');
        console.log('ğŸ® Controls:');
        console.log('  â€¢ Read now: window.readScreenNow()');
        console.log('  â€¢ Stop reading: window.stopScreenReading()');
        console.log('  â€¢ Get last text: window.getLastScreenText()');
        console.log('');
        console.log('ğŸ’¡ Try saying "Velvet, what\'s on my screen?" after a few seconds!');
        
        return true;
        
    } catch (error) {
        console.error('âŒ Failed to enable screen reading:', error);
        return false;
    }
}

// Auto-start screen reading
console.log('ğŸš€ Starting screen reading in 2 seconds...');
setTimeout(async () => {
    const success = await enableScreenReading();
    if (success) {
        console.log('');
        console.log('ğŸ‰ SUCCESS! Velvet can now read your screen!');
        console.log('ğŸ¯ Perfect for your YC demo - Velvet will now understand screen context');
    } else {
        console.log('');
        console.log('âŒ Screen reading setup failed');
        console.log('ğŸ’¡ Try refreshing and running the script again');
    }
}, 2000);