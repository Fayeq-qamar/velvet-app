// Test script to see exactly what Velvet is reading and hearing
// Run this in the browser console to see real monitoring logs

console.log('ğŸ§ª REAL MONITORING TEST SCRIPT');
console.log('===============================');

// Test 1: Check if systems are loaded
function checkSystemStatus() {
    console.log('\nğŸ“‹ SYSTEM STATUS CHECK:');
    console.log('â€¢ Tesseract loaded:', typeof Tesseract !== 'undefined' ? 'âœ…' : 'âŒ');
    console.log('â€¢ Electron API loaded:', typeof window.electronAPI !== 'undefined' ? 'âœ…' : 'âŒ');
    console.log('â€¢ Screen OCR class loaded:', typeof RealScreenOCRMonitor !== 'undefined' ? 'âœ…' : 'âŒ');
    console.log('â€¢ Audio Monitor class loaded:', typeof RealAudioEnvironmentMonitor !== 'undefined' ? 'âœ…' : 'âŒ');
    console.log('â€¢ Context Engine loaded:', typeof UnifiedContextEngine !== 'undefined' ? 'âœ…' : 'âŒ');
}

// Test 2: Initialize and test REAL screen OCR
async function testRealScreenOCR() {
    console.log('\nğŸ‘ï¸ TESTING REAL SCREEN OCR:');
    console.log('=============================');
    
    try {
        const monitor = new RealScreenOCRMonitor();
        console.log('ğŸ”§ Initializing OCR monitor...');
        
        const initialized = await monitor.initialize();
        if (!initialized) {
            console.error('âŒ Failed to initialize screen OCR');
            return;
        }
        
        console.log('âœ… Screen OCR initialized successfully');
        console.log('ğŸ¯ Starting screen capture (permission will be requested)...');
        
        const started = await monitor.startMonitoring();
        if (started) {
            console.log('âœ… REAL screen monitoring started!');
            console.log('ğŸ“¸ Velvet will now capture your screen every 5 seconds');
            console.log('ğŸ” OCR will extract text from everything visible');
            console.log('');
            console.log('ğŸ’¡ WATCH FOR THESE LOGS:');
            console.log('  ğŸ“¸ Capturing real screen...');
            console.log('  ğŸ” Running Tesseract OCR...');
            console.log('  âœ… Real screen OCR completed in Xms - confidence: Y');
            console.log('  ğŸ“ REAL screen text processed: [actual text from your screen]');
            console.log('  ğŸ¯ Relevance: X.XX, Context: [detected context]');
            
            // Store reference for later use
            window.testScreenMonitor = monitor;
            
            return true;
        }
    } catch (error) {
        console.error('âŒ Screen OCR test failed:', error);
        return false;
    }
}

// Test 3: Initialize and test REAL audio monitoring
async function testRealAudioMonitoring() {
    console.log('\nğŸ§ TESTING REAL AUDIO MONITORING:');
    console.log('=================================');
    
    try {
        const monitor = new RealAudioEnvironmentMonitor();
        console.log('ğŸ”§ Initializing audio monitor...');
        
        const initialized = await monitor.initialize();
        if (!initialized) {
            console.error('âŒ Failed to initialize audio monitor');
            return;
        }
        
        console.log('âœ… Audio monitor initialized successfully');
        console.log('ğŸµ Starting system audio detection...');
        
        const started = await monitor.startMonitoring();
        if (started) {
            console.log('âœ… REAL audio monitoring started!');
            console.log('ğŸ”Š Velvet will now detect your system audio every 3 seconds');
            console.log('ğŸµ Detects: Spotify, Apple Music, YouTube, calls, etc.');
            console.log('ğŸ¤ Also analyzes microphone for ambient audio');
            console.log('');
            console.log('ğŸ’¡ WATCH FOR THESE LOGS:');
            console.log('  ğŸµ Getting current audio context...');
            console.log('  âœ… Audio context: music (Spotify)');
            console.log('  ğŸ”Š Audio source: music from Spotify');
            console.log('  ğŸ§ Audio analysis completed in Xms - detected: music');
            console.log('  ğŸµ REAL audio context: music from system_detection - enhancing');
            
            // Store reference for later use
            window.testAudioMonitor = monitor;
            
            return true;
        }
    } catch (error) {
        console.error('âŒ Audio monitoring test failed:', error);
        return false;
    }
}

// Test 4: Test system audio detection directly
async function testSystemAudioDirect() {
    console.log('\nğŸ”Š TESTING SYSTEM AUDIO DETECTION DIRECTLY:');
    console.log('==========================================');
    
    if (!window.electronAPI || !window.electronAPI.audioEnvironment) {
        console.error('âŒ Electron Audio API not available');
        return;
    }
    
    try {
        console.log('ğŸµ Getting current audio context...');
        const audioContext = await window.electronAPI.audioEnvironment.getCurrentAudioContext();
        console.log('ğŸ“Š Audio Context Result:', audioContext);
        
        console.log('ğŸ”Š Capturing system audio sources...');
        const audioSources = await window.electronAPI.audioEnvironment.captureSystemAudio();
        console.log('ğŸ“Š Audio Sources Result:', audioSources);
        
        console.log('ğŸ§ Getting system audio devices...');
        const audioDevices = await window.electronAPI.audioEnvironment.getSystemAudioDevices();
        console.log('ğŸ“Š Audio Devices Result:', audioDevices);
        
        // Interpret results
        console.log('\nğŸ¯ INTERPRETATION:');
        if (audioContext.context === 'music') {
            console.log(`ğŸµ Music detected: ${audioContext.currentTrack} (${audioContext.app})`);
        } else if (audioContext.context === 'silence') {
            console.log('ğŸ”‡ No music currently playing');
        }
        
        if (audioSources.audioSources && audioSources.audioSources.length > 0) {
            console.log(`ğŸ”Š Active audio apps: ${audioSources.audioSources.map(s => s.name).join(', ')}`);
        } else {
            console.log('ğŸ”‡ No active audio applications detected');
        }
        
        return true;
    } catch (error) {
        console.error('âŒ System audio detection failed:', error);
        return false;
    }
}

// Test 5: View monitoring history
function viewMonitoringHistory() {
    console.log('\nğŸ“š VIEWING MONITORING HISTORY:');
    console.log('==============================');
    
    if (window.testScreenMonitor && window.testScreenMonitor.textHistory) {
        console.log(`ğŸ‘ï¸ Screen OCR History (${window.testScreenMonitor.textHistory.length} entries):`);
        window.testScreenMonitor.textHistory.slice(-3).forEach((entry, i) => {
            console.log(`  ${i + 1}. [${new Date(entry.timestamp).toLocaleTimeString()}] "${entry.text.substring(0, 100)}..."`);
            console.log(`     Context: ${entry.context.type}, Relevance: ${entry.relevance.score.toFixed(2)}`);
        });
    } else {
        console.log('ğŸ‘ï¸ No screen OCR history available yet');
    }
    
    if (window.testAudioMonitor && window.testAudioMonitor.audioHistory) {
        console.log(`ğŸ§ Audio History (${window.testAudioMonitor.audioHistory.length} entries):`);
        window.testAudioMonitor.audioHistory.slice(-3).forEach((entry, i) => {
            console.log(`  ${i + 1}. [${new Date(entry.timestamp).toLocaleTimeString()}] ${entry.primaryType} from ${entry.source}`);
            console.log(`     Focus Impact: ${entry.insights.focusImpact}, Relevance: ${entry.relevance.score.toFixed(2)}`);
        });
    } else {
        console.log('ğŸ§ No audio history available yet');
    }
}

// Main test runner
async function runCompleteTest() {
    console.clear();
    console.log('ğŸš€ COMPLETE REAL MONITORING TEST');
    console.log('================================');
    console.log('This will test ALL real monitoring capabilities');
    console.log('');
    
    // Check system status
    checkSystemStatus();
    
    // Test system audio first (doesn't require permissions)
    await testSystemAudioDirect();
    
    // Wait a moment
    await new Promise(resolve => setTimeout(resolve, 2000));
    
    // Test audio monitoring (may require microphone permission)
    const audioWorking = await testRealAudioMonitoring();
    
    // Wait a moment
    await new Promise(resolve => setTimeout(resolve, 2000));
    
    // Test screen OCR (will require screen capture permission)
    const screenWorking = await testRealScreenOCR();
    
    // Final instructions
    console.log('\nğŸ¯ TEST COMPLETE - MONITORING IS NOW ACTIVE!');
    console.log('============================================');
    console.log('');
    if (screenWorking) {
        console.log('ğŸ‘ï¸ SCREEN MONITORING: Active - capturing every 5 seconds');
        console.log('  â€¢ Open different apps (email, code editor, browser)');
        console.log('  â€¢ Type some text');
        console.log('  â€¢ Watch console for OCR results');
    }
    if (audioWorking) {
        console.log('ğŸ§ AUDIO MONITORING: Active - detecting every 3 seconds');
        console.log('  â€¢ Play music on Spotify or Apple Music');
        console.log('  â€¢ Join a video call');
        console.log('  â€¢ Watch console for audio detection');
    }
    console.log('');
    console.log('ğŸ“š TO VIEW HISTORY: run viewMonitoringHistory()');
    console.log('ğŸ”„ TO CHECK STATUS AGAIN: run checkSystemStatus()');
    console.log('');
    console.log('ğŸ’¡ Velvet is now ACTUALLY monitoring your screen and audio!');
}

// Export functions for manual use
window.testRealMonitoring = {
    runComplete: runCompleteTest,
    checkStatus: checkSystemStatus,
    testScreen: testRealScreenOCR,
    testAudio: testRealAudioMonitoring,
    testSystemAudio: testSystemAudioDirect,
    viewHistory: viewMonitoringHistory
};

console.log('ğŸ§ª Real Monitoring Test Script Loaded');
console.log('ğŸ’¡ Run: testRealMonitoring.runComplete() to start complete test');
console.log('ğŸ’¡ Or run individual tests: testRealMonitoring.testScreen(), etc.');